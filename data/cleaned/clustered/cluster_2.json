[
    {
        "article": "Spyware is the most complex, obfuscated, and targeted class of malware, which has grown dramatically in recent years. Spyware is designed for secret, long-term, and persistent missions. This paper provides a novel method for detection, tracking, and confronting the stealth and obfuscated spyware and ransomware, including keyloggers, screen recorders, and blockers. The proposed method of this paper is based on a dynamic behavioral analysis through deep and transparent hooking of kernel-level routines. We used linear regression, JRIP, and J48 decision tree algorithms as a classifier to recognize three classes of malware. This paper presents the main architectural plan of an anti-spyware application to track spyware footprints in order to detect and force terminate running processes, eliminate executable files, and restrict network communications. The efficiency of the proposed method was evaluated from the viewpoint of accuracy in detecting real-world samples of spyware by ROC curve analysis and from the viewpoint of success rate to confront effectively with active spyware. Our proposed method was able to recognize spyware with an accuracy of about 93% and an error rate near 7%. In addition, the proposed system can disinfect an operating system from infection by spyware with a hit rate of about 82%.",
        "abstract": "Spyware is the most complex, obfuscated, and targeted class of malware, which has grown dramatically in recent years. Spyware is designed for secret, long-term, and persistent missions. This paper provides a novel method for detection, tracking, and confronting the stealth and obfuscated spyware and ransomware, including keyloggers, screen recorders, and blockers. The proposed method of this paper is based on a dynamic behavioral analysis through deep and transparent hooking of kernel-level routines. We used linear regression, JRIP, and J48 decision tree algorithms as a classifier to recognize three classes of malware. This paper presents the main architectural plan of an anti-spyware application to track spyware footprints in order to detect and force terminate running processes, eliminate executable files, and restrict network communications. The efficiency of the proposed method was evaluated from the viewpoint of accuracy in detecting real-world samples of spyware by ROC curve analysis and from the viewpoint of success rate to confront effectively with active spyware. Our proposed method was able to recognize spyware with an accuracy of about 93% and an error rate near 7%. In addition, the proposed system can disinfect an operating system from infection by spyware with a hit rate of about 82%.",
        "authors": "Amir Masoud Rahmani, Danial Javaheri, Mehdi Hosseinzadeh",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884964",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A majority of the existing research on HTTP focuses on improving the quality of experience for single-source videos. Recently, there has been a focus on multi-view streaming applications, where multiple videos are streamed simultaneously. The challenge is to improve the viewing experience of concurrently streamed videos over the bandwidth-constrained network. In this paper, we propose an adaptive scheme for multi-view streaming over HTTP that requests segments of multiple videos concurrently. First, we propose a unified throughput estimation method to calculate the throughput, solving unfair and inefficient throughput utilization problems of existing estimation methods. Second, we design a unified segment scheduling method that guarantees synchronicity among segments of concurrently downloaded video streams. Third, we present a multi-path streaming method that leverages path diversity to improve the user experience of the video streams. Finally, we present a rate adaptation algorithm that dynamically downloads high-quality segments of multiple video streams, while minimizing video quality changes and the risk of playback interruption in single- and multi-path environments.",
        "abstract": "A majority of the existing research on HTTP focuses on improving the quality of experience for single-source videos. Recently, there has been a focus on multi-view streaming applications, where multiple videos are streamed simultaneously. The challenge is to improve the viewing experience of concurrently streamed videos over the bandwidth-constrained network. In this paper, we propose an adaptive scheme for multi-view streaming over HTTP that requests segments of multiple videos concurrently. First, we propose a unified throughput estimation method to calculate the throughput, solving unfair and inefficient throughput utilization problems of existing estimation methods. Second, we design a unified segment scheduling method that guarantees synchronicity among segments of concurrently downloaded video streams. Third, we present a multi-path streaming method that leverages path diversity to improve the user experience of the video streams. Finally, we present a rate adaptation algorithm that dynamically downloads high-quality segments of multiple video streams, while minimizing video quality changes and the risk of playback interruption in single- and multi-path environments.",
        "authors": "Kwangsue Chung, Waqas Ur Rahman",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884248",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Drilling systems are widely used today for various purposes, but in harsh environments, such as the Arctic or space, it is hard to utilize these conventional systems because they require large amounts of equipment and labor. Various research projects have attempted to resolve this issue, but they are still in development stages owing to poor drilling performance. To solve this problem, we exploit the digging habits of the mole that is one of the representative animals living underground. In this paper, we propose the excavation mechanism for an embedded drilling robot, including cutting removal and balancing in the hole, which is inspired by a type of a mole known as the African mole-rat. To find the rate of penetration (ROP) trends of the proposed drilling system, simple drilling experiments are performed on autoclaved lightweight concrete blocks with a drill bit that imitates the polycrystalline diamond compact bit. The maximum ROP is obtained with the experiments and finite-element modeling simulations.",
        "abstract": "Drilling systems are widely used today for various purposes, but in harsh environments, such as the Arctic or space, it is hard to utilize these conventional systems because they require large amounts of equipment and labor. Various research projects have attempted to resolve this issue, but they are still in development stages owing to poor drilling performance. To solve this problem, we exploit the digging habits of the mole that is one of the representative animals living underground. In this paper, we propose the excavation mechanism for an embedded drilling robot, including cutting removal and balancing in the hole, which is inspired by a type of a mole known as the African mole-rat. To find the rate of penetration (ROP) trends of the proposed drilling system, simple drilling experiments are performed on autoclaved lightweight concrete blocks with a drill bit that imitates the polycrystalline diamond compact bit. The maximum ROP is obtained with the experiments and finite-element modeling simulations.",
        "authors": "Heung Woon Jang, Hyun Myung, Jae-Uk Shin, Jongheon Kim, Jung-Wuk Hong",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884495",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Land use and land cover (LULC) change is frequent in mountainous terrain of southern China. Although remote sensing technology has become an important tool for gathering and monitoring LULC dynamics, image pairs can occur scale changes, noises, geometrical distortions, and illuminated variations if these are acquired from different types of sensors (e.g., satellites). Meanwhile, how to design an efficient land cover change detection algorithm that ensures a high detection rate remains a critical and challenging step. To address these problems, we propose a robust multi-temporal change detection framework for land cover change in mountainous terrain which contains the following contributions. i) To transform multi-temporal remote sensing image pairs acquired by different type of sensors into the same coordinate system by image registration, a multi-scale feature description is generated using layers formed via a pretrained VGG network. ii) A gradually increasing selection of inliers is defined for improving the robustness of feature points registration, and L2-minimizing estimate (L2E)-based energy optimization is formulated to calculate a reasonable position in a reproducing kernel Hilbert space. iii) Fuzzy C-Means classifier is adopted to generate a similarity matrix between image pair of geometric correction, and a robust and contractive change map is built through feature similarity analysis. Extensive experiments on multi-temporal image pairs taken by different type of satellites (e.g., Chinese GF and Landsat) or small unmanned aerial vehicles are conducted. Experimental results show that our method provides better performances in most cases after comparing with the five state-of-the-art image registration methods and the four state-of-the-art change detection methods.",
        "abstract": "Land use and land cover (LULC) change is frequent in mountainous terrain of southern China. Although remote sensing technology has become an important tool for gathering and monitoring LULC dynamics, image pairs can occur scale changes, noises, geometrical distortions, and illuminated variations if these are acquired from different types of sensors (e.g., satellites). Meanwhile, how to design an efficient land cover change detection algorithm that ensures a high detection rate remains a critical and challenging step. To address these problems, we propose a robust multi-temporal change detection framework for land cover change in mountainous terrain which contains the following contributions. i) To transform multi-temporal remote sensing image pairs acquired by different type of sensors into the same coordinate system by image registration, a multi-scale feature description is generated using layers formed via a pretrained VGG network. ii) A gradually increasing selection of inliers is defined for improving the robustness of feature points registration, and L2-minimizing estimate (L2E)-based energy optimization is formulated to calculate a reasonable position in a reproducing kernel Hilbert space. iii) Fuzzy C-Means classifier is adopted to generate a similarity matrix between image pair of geometric correction, and a robust and contractive change map is built through feature similarity analysis. Extensive experiments on multi-temporal image pairs taken by different type of satellites (e.g., Chinese GF and Landsat) or small unmanned aerial vehicles are conducted. Experimental results show that our method provides better performances in most cases after comparing with the five state-of-the-art image registration methods and the four state-of-the-art change detection methods.",
        "authors": "Fei Song, Tingting Dan, Wanjing Zhao, Xueyan Gao, Yang Yang, Zhuoqian Yang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883254",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, a novel double image encryption scheme based on amplitude-phase encoding and discrete complex random transform (DCRT) is proposed. First, two images are merged into a plural matrix by precoding, and the synthetic signal is modulated by the phase mask which is calculated from Chen chaotic sequences generated by using the self-adapting parameter as initial values. Then, the modulated signal is encrypted by amplitude-phase encoding and the DCRT. The final cipher-text is decomposed into amplitude and phase to obtain two encrypted images. Experimental results and performance analysis show that the proposed encryption scheme can effectively resist different attacks such as the differential attack, statistical attack, and so on, and it can be used for the secure communications.",
        "abstract": "In this paper, a novel double image encryption scheme based on amplitude-phase encoding and discrete complex random transform (DCRT) is proposed. First, two images are merged into a plural matrix by precoding, and the synthetic signal is modulated by the phase mask which is calculated from Chen chaotic sequences generated by using the self-adapting parameter as initial values. Then, the modulated signal is encrypted by amplitude-phase encoding and the DCRT. The final cipher-text is decomposed into amplitude and phase to obtain two encrypted images. Experimental results and performance analysis show that the proposed encryption scheme can effectively resist different attacks such as the differential attack, statistical attack, and so on, and it can be used for the secure communications.",
        "authors": "Frank Jiang, Junxiu Liu, Lvchen Cao, Shunbin Tang, Xingsheng Qin, Yuling Luo",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884013",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Accurate forecasting of high-risk clinical symptoms, like epileptic seizures, has the potential to transform clinical epilepsy care and to create new therapeutic strategies for individuals in clinical decision support systems. With the development of pervasive sensor technologies, physiological signals can be captured continuously to prevent the serious outcomes caused by epilepsy. However, the progress on seizure prediction has been hindered by the lack of automatic early warning system. The existing research is classifying electroencephalograph (EEG) clips and is distinguishing the clips of onset epileptic seizures. Deep learning is a promising method to analyze the large-scale unlabeled data and to widely spread the clinical treatment and risk prediction. In this paper, we outline a patient-specific method for extracting the frequency domain and time-series data features based on the two-layer convolutional neural networks (CNNs). A data preprocessing method based on the discrete Fourier transform is proposed to convert the time-domain signal of the EEG data to the frequency-domain signal. Long short-term memory networks are introduced in seizure prediction using pre-seizure clips of the EEG dataset, expanding the use of deep learning algorithms with recurrent neural networks (RNNs). Furthermore, the proposed CNN and RNN are compared with the traditional machine learning algorithms, such as linear discriminant analysis and logistic regression, and the evaluation criteria are on the area under the curve. The extensive experimental results demonstrate that our method can effectively extract the latent features with meaningful interpretation and exhibits excellent performance for predicting epileptic preictal state changes, and hence is an effective method in detecting the epileptic seizure.",
        "abstract": "Accurate forecasting of high-risk clinical symptoms, like epileptic seizures, has the potential to transform clinical epilepsy care and to create new therapeutic strategies for individuals in clinical decision support systems. With the development of pervasive sensor technologies, physiological signals can be captured continuously to prevent the serious outcomes caused by epilepsy. However, the progress on seizure prediction has been hindered by the lack of automatic early warning system. The existing research is classifying electroencephalograph (EEG) clips and is distinguishing the clips of onset epileptic seizures. Deep learning is a promising method to analyze the large-scale unlabeled data and to widely spread the clinical treatment and risk prediction. In this paper, we outline a patient-specific method for extracting the frequency domain and time-series data features based on the two-layer convolutional neural networks (CNNs). A data preprocessing method based on the discrete Fourier transform is proposed to convert the time-domain signal of the EEG data to the frequency-domain signal. Long short-term memory networks are introduced in seizure prediction using pre-seizure clips of the EEG dataset, expanding the use of deep learning algorithms with recurrent neural networks (RNNs). Furthermore, the proposed CNN and RNN are compared with the traditional machine learning algorithms, such as linear discriminant analysis and logistic regression, and the evaluation criteria are on the area under the curve. The extensive experimental results demonstrate that our method can effectively extract the latent features with meaningful interpretation and exhibits excellent performance for predicting epileptic preictal state changes, and hence is an effective method in detecting the epileptic seizure.",
        "authors": "Fuxu Wang, Mingrui Sun, Tengfei Min, Tianyi Zang, Yadong Wang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883562",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper suggests a real-valued sparse representation method using a unitary transformation that can convert complex-valued manifold matrices from uniform circular array into real ones. Because of this transformation, the computational complexity is modified. Simulation results confirmed the effectiveness of the proposed method with a circular array radar.",
        "abstract": "This paper suggests a real-valued sparse representation method using a unitary transformation that can convert complex-valued manifold matrices from uniform circular array into real ones. Because of this transformation, the computational complexity is modified. Simulation results confirmed the effectiveness of the proposed method with a circular array radar.",
        "authors": "Iman Kazemi, Mohammad Reza Moniri, Ramin Shaghaghi Kandovan",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2270173",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Particle swarm optimization (PSO) is an evolutionary algorithm that is well known for its simplicity and effectiveness. It usually has strong global search capability but has the drawback of being easily trapped by local optima. A scaling mutation strategy and an elitist learning strategy are presented in this paper. Based on these strategies, an improved PSO variant (LSERPSO) is developed through a local search and ring topology strategy. The new scaling mutation strategy involved an exploration and exploitation balance focusing on mutation operation. A collection of elite individuals is maintained such that an array of current particles can learn from them. A ring topology-based neighborhood structure is adopted to maintain the population diversity and to reduce the possibility of particles being trapped in local optima. Finally, a quasi-Newton-based local search is incorporated to enhance the fine-grained capability. The effects of these proposed strategies and their cooperation are verified step by step. The performance of LSERPSO is comprehensively studied using IEEE CEC2015 benchmark functions.",
        "abstract": "Particle swarm optimization (PSO) is an evolutionary algorithm that is well known for its simplicity and effectiveness. It usually has strong global search capability but has the drawback of being easily trapped by local optima. A scaling mutation strategy and an elitist learning strategy are presented in this paper. Based on these strategies, an improved PSO variant (LSERPSO) is developed through a local search and ring topology strategy. The new scaling mutation strategy involved an exploration and exploitation balance focusing on mutation operation. A collection of elite individuals is maintained such that an array of current particles can learn from them. A ring topology-based neighborhood structure is adopted to maintain the population diversity and to reduce the possibility of particles being trapped in local optima. Finally, a quasi-Newton-based local search is incorporated to enhance the fine-grained capability. The effects of these proposed strategies and their cooperation are verified step by step. The performance of LSERPSO is comprehensively studied using IEEE CEC2015 benchmark functions.",
        "authors": "Guangzhi Xu, Rui Li, Tong Wu, Xinchao Zhao, Xingmei Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885036",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, we study the metric dimension of barycentric subdivision of Möbius ladders and the metric dimension of generalized Petersen multigraphs. We prove that the generalized Petersen multigraphs denoted byP(2n,n)have metric dimension 3 whennis even and 4 otherwise. We also study the exchange property for resolving sets of barycentric subdivisions of Möbius ladders and generalized Petersen multigraphs and prove that the exchange property of the bases in a vector space does not hold for minimal resolving sets of these graphs.",
        "abstract": "In this paper, we study the metric dimension of barycentric subdivision of Möbius ladders and the metric dimension of generalized Petersen multigraphs. We prove that the generalized Petersen multigraphs denoted byP(2n,n)have metric dimension 3 whennis even and 4 otherwise. We also study the exchange property for resolving sets of barycentric subdivisions of Möbius ladders and generalized Petersen multigraphs and prove that the exchange property of the bases in a vector space does not hold for minimal resolving sets of these graphs.",
        "authors": "Muhammad Imran, Muhammad Kamran Siddiqui, Rishi Naeem",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883556",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In order to reduce the frequency deviation resulting from renewable energy fluctuation and load variance, the coordination control strategy for isolated wind-diesel hybrid micro-grid is proposed by taking advantage of smart neural network observer and sliding mode method. For diesel generator system side, the sliding mode load frequency control including load variance is designed to regulate the output power. For the wind turbine generator system side, the sliding mode pitch angle control considering load variance is constructed to smooth the wind turbine generator output power fluctuation. Furthermore, the different coordinated strategies are proposed to realize the plug and play for the hybrid micro-grid, it is easy to see that the control accuracy can be improved by the designed neural network adaptive observer and considering the load variation. The effectiveness of the proposed control strategy is validated through real time digital simulator platform under different operation condition.",
        "abstract": "In order to reduce the frequency deviation resulting from renewable energy fluctuation and load variance, the coordination control strategy for isolated wind-diesel hybrid micro-grid is proposed by taking advantage of smart neural network observer and sliding mode method. For diesel generator system side, the sliding mode load frequency control including load variance is designed to regulate the output power. For the wind turbine generator system side, the sliding mode pitch angle control considering load variance is constructed to smooth the wind turbine generator output power fluctuation. Furthermore, the different coordinated strategies are proposed to realize the plug and play for the hybrid micro-grid, it is easy to see that the control accuracy can be improved by the designed neural network adaptive observer and considering the load variation. The effectiveness of the proposed control strategy is validated through real time digital simulator platform under different operation condition.",
        "authors": "Chengshan Wang, Minghan Yuan, Yang Fu, Yang Mi, Zhenkun Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883492",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "An accurate and efficient physical-optics (PO) near-field integral representation is proposed and introduced into shooting and bouncing ray (SBR) to evaluate the electric and magnetic near-fields scattered from electrically large complex objects illuminated by dipole sources. The PO near-field integral expressions can be reduced to closed-form expressions by applying locally expanded phase approximations and surface partitioning. By introducing the proposed PO near-field integral expressions, SBR can be used to solve near-field scattering problems from electrically large complex objects. Simple and complex numerical examples are presented to demonstrate the efficiency and accuracy of the proposed method.",
        "abstract": "An accurate and efficient physical-optics (PO) near-field integral representation is proposed and introduced into shooting and bouncing ray (SBR) to evaluate the electric and magnetic near-fields scattered from electrically large complex objects illuminated by dipole sources. The PO near-field integral expressions can be reduced to closed-form expressions by applying locally expanded phase approximations and surface partitioning. By introducing the proposed PO near-field integral expressions, SBR can be used to solve near-field scattering problems from electrically large complex objects. Simple and complex numerical examples are presented to demonstrate the efficiency and accuracy of the proposed method.",
        "authors": "Guangbin Guo, Lixin Guo",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885148",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In a massive MIMO-enabled cognitive HetNet, a multi-user massive MIMO macrocell is underlaid with dense cognitive small cells. Then, a plenty of exclusion zones (EZs) are formed around co-scheduled macrocell users. Applying enhanced inter-cell interference coordination (eICIC) in each EZ separately, we propose an EZ-assisted eICIC scheme which arranges a customized eICIC configuration for each EZ independently to others. The proposed scheme can overcome potential weaknesses of current eICIC and that of cognitive HetNets. Then, the EZ-assisted eICIC configuration problem is formulated as a general optimization problem, which maximizes overall weighted sum rate of the network. Since the problem is intractable, a cluster segmentation approach is, therefore, proposed to convert the problem into a feasible version. Furthermore, we design an iterative multi-user scheduling-based algorithm to solve the problem. Numerical results demonstrate that the proposed EZ-assisted eICIC scheme significantly outperforms eICIC and further eICIC in dense HetNet.",
        "abstract": "In a massive MIMO-enabled cognitive HetNet, a multi-user massive MIMO macrocell is underlaid with dense cognitive small cells. Then, a plenty of exclusion zones (EZs) are formed around co-scheduled macrocell users. Applying enhanced inter-cell interference coordination (eICIC) in each EZ separately, we propose an EZ-assisted eICIC scheme which arranges a customized eICIC configuration for each EZ independently to others. The proposed scheme can overcome potential weaknesses of current eICIC and that of cognitive HetNets. Then, the EZ-assisted eICIC configuration problem is formulated as a general optimization problem, which maximizes overall weighted sum rate of the network. Since the problem is intractable, a cluster segmentation approach is, therefore, proposed to convert the problem into a feasible version. Furthermore, we design an iterative multi-user scheduling-based algorithm to solve the problem. Numerical results demonstrate that the proposed EZ-assisted eICIC scheme significantly outperforms eICIC and further eICIC in dense HetNet.",
        "authors": "Jiandong Li, Jinjing Huang, Zichen Chen",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883592",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In organized healthcare quality improvement collaboratives (QICs), teams of practitioners from different hospitals exchange information on clinical practices with the aim of improving health outcomes at their own institutions. However, what works in one hospital may not work in others with different local contexts because of nonlinear interactions among various demographics, treatments, and practices. In previous studies of collaborations where the goal is a collective problem solving, teams of diverse individuals have been shown to outperform teams of similar individuals. However, when the purpose of collaboration is knowledge diffusion in complex environments, it is not clear whether team diversity will help or hinder effective learning. In this paper, we first use an agent-based model of QICs to show that teams comprising similar individuals outperform those with more diverse individuals under nearly all conditions, and that this advantage increases with the complexity of the landscape and level of noise in assessing performance. Examination of data from a network of real hospitals provides encouraging evidence of a high degree of similarity in clinical practices, especially within teams of hospitals engaging in QIC teams. However, our model also suggests that groups of similar hospitals could benefit from larger teams and more open sharing of details on clinical outcomes than is currently the norm. To facilitate this, we propose a secure virtual collaboration system that would allow hospitals to efficiently identify potentially better practices in use at other institutions similar to theirs without any institutions having to sacrifice the privacy of their own data. Our results may also have implications for other types of data-driven diffusive learning such as in personalized medicine and evolutionary search in noisy, complex combinatorial optimization problems.",
        "abstract": "In organized healthcare quality improvement collaboratives (QICs), teams of practitioners from different hospitals exchange information on clinical practices with the aim of improving health outcomes at their own institutions. However, what works in one hospital may not work in others with different local contexts because of nonlinear interactions among various demographics, treatments, and practices. In previous studies of collaborations where the goal is a collective problem solving, teams of diverse individuals have been shown to outperform teams of similar individuals. However, when the purpose of collaboration is knowledge diffusion in complex environments, it is not clear whether team diversity will help or hinder effective learning. In this paper, we first use an agent-based model of QICs to show that teams comprising similar individuals outperform those with more diverse individuals under nearly all conditions, and that this advantage increases with the complexity of the landscape and level of noise in assessing performance. Examination of data from a network of real hospitals provides encouraging evidence of a high degree of similarity in clinical practices, especially within teams of hospitals engaging in QIC teams. However, our model also suggests that groups of similar hospitals could benefit from larger teams and more open sharing of details on clinical outcomes than is currently the norm. To facilitate this, we propose a secure virtual collaboration system that would allow hospitals to efficiently identify potentially better practices in use at other institutions similar to theirs without any institutions having to sacrifice the privacy of their own data. Our results may also have implications for other types of data-driven diffusive learning such as in personalized medicine and evolutionary search in noisy, complex combinatorial optimization problems.",
        "authors": "Jeffrey D. Horbar, Margaret J. Eppstein, Narine Manukyan",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2280086",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper provides an overview of the main features of several bibliometric indicators which were proposed in the last few decades. Their pros and cons are highlighted and compared with the features of the well-known impact factor (IF) to show how alternative metrics are specifically designed to address the flaws that the IF was shown to have, especially in the last few years. We also report the results of recent studies in the bibliometric literature showing how the scientific impact of journals as evaluated by bibliometrics is a very complicated matter and it is completely unrealistic to try to capture it by any single indicator, such as the IF or any other. As such, we conclude that the adoption of more metrics, with complementary features, to assess journal quality would be very beneficial as it would both offer a more comprehensive and balanced view of each journal in the space of scholarly publications, as well as eliminate the pressure on individuals and their incentive to do metric manipulation which is an unintended result of the current (mis)use of the IF as the gold standard for publication quality.",
        "abstract": "This paper provides an overview of the main features of several bibliometric indicators which were proposed in the last few decades. Their pros and cons are highlighted and compared with the features of the well-known impact factor (IF) to show how alternative metrics are specifically designed to address the flaws that the IF was shown to have, especially in the last few years. We also report the results of recent studies in the bibliometric literature showing how the scientific impact of journals as evaluated by bibliometrics is a very complicated matter and it is completely unrealistic to try to capture it by any single indicator, such as the IF or any other. As such, we conclude that the adoption of more metrics, with complementary features, to assess journal quality would be very beneficial as it would both offer a more comprehensive and balanced view of each journal in the space of scholarly publications, as well as eliminate the pressure on individuals and their incentive to do metric manipulation which is an unintended result of the current (mis)use of the IF as the gold standard for publication quality.",
        "authors": "Gianluca Setti",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2261115",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper is designed as a comparative study between technical and further education institutes in Australia and community colleges in Malaysia in order to improve and develop the quality and quantity of technicians in the automotive industry. The descriptive survey research design was utilized in carrying out the study. The 113 students studying the Diploma Certificate in automotive engineering responded to a structured questionnaire, which addressed the research questions. Cronbach's alpha coefficient was used in determining the reliability of the instrument. The reliability coefficient of the instrument used for data collection stood at 0.89. Based on the findings, it was recommended that the training institutions should upgrade their training facilities with new equipment similar to that in the industry and Technical Vocational Education and Training department in the Ministry of Higher Education, and should advocate a broad range of industry participation opportunities to higher institutions in the preparation of highly skilled workforce for the industry.",
        "abstract": "This paper is designed as a comparative study between technical and further education institutes in Australia and community colleges in Malaysia in order to improve and develop the quality and quantity of technicians in the automotive industry. The descriptive survey research design was utilized in carrying out the study. The 113 students studying the Diploma Certificate in automotive engineering responded to a structured questionnaire, which addressed the research questions. Cronbach's alpha coefficient was used in determining the reliability of the instrument. The reliability coefficient of the instrument used for data collection stood at 0.89. Based on the findings, it was recommended that the training institutions should upgrade their training facilities with new equipment similar to that in the industry and Technical Vocational Education and Training department in the Ministry of Higher Education, and should advocate a broad range of industry participation opportunities to higher institutions in the preparation of highly skilled workforce for the industry.",
        "authors": "Adnan Ahmad, Azlan Abdul Latib, Muhammad Sukri Bin Saud, Noraffandy Yahaya, Waleed Mugahed Al-Rahmi, Yusri Bin Kamin",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883694",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper presents a cloud theory-based iterated greedy (CTIG) algorithm for solving the no-wait flowshop scheduling problem (NWFSP) with the objective of minimizing the sum of makespan and total weighted tardiness. The performance of the proposed CTIG algorithm is evaluated by comparing its computational results to those of the best-to-date meta-heuristic algorithm, particle swarm optimization (PSO), as presented in this paper. The experimental results concerning two sets of benchmark problem instances in this paper demonstrate that the CTIG algorithm obtains more (near) optimal solution in less computational time than the PSO algorithm. The computational results in this paper fill the research gap in the development of a novel algorithm to improve the solution quality in the case of the NWFSP with the objective of minimizing the sum of makespan and total weighted tardiness.",
        "abstract": "This paper presents a cloud theory-based iterated greedy (CTIG) algorithm for solving the no-wait flowshop scheduling problem (NWFSP) with the objective of minimizing the sum of makespan and total weighted tardiness. The performance of the proposed CTIG algorithm is evaluated by comparing its computational results to those of the best-to-date meta-heuristic algorithm, particle swarm optimization (PSO), as presented in this paper. The experimental results concerning two sets of benchmark problem instances in this paper demonstrate that the CTIG algorithm obtains more (near) optimal solution in less computational time than the PSO algorithm. The computational results in this paper fill the research gap in the development of a novel algorithm to improve the solution quality in the case of the NWFSP with the objective of minimizing the sum of makespan and total weighted tardiness.",
        "authors": "Chung-Cheng Lu, Kuo-Ching Ying, Shih-Wei Lin",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885137",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Handheld virtual reality (VR) controllers are necessary for creating immersive experiences. In this paper, we propose a gated RNN-based sequence model that estimates the joint torques of a serially linked handheld VR system interface from a sequential position input. In our previous study, we proposed a motion planning algorithm for articulated systems based on the active contour model that optimizes the positions of each joint torque based on the measured base position (6-Degrees of Freedom). Because the position-to-position scheme, which calculates the joint positions from a given base position, illustrated several limitations concerning safety (i.e. unable to handle unexpected contact with the surroundings), our current study proposes a position-to-torque generation scheme that estimates the joint torques from the measured base position sequences. To that end, we trained the sequences of joint torques and the sequence of the 6-DoF base position as a supervised learning task. To model the multivariate temporal information of the sequences, we employed a gated recurrent unit. The experimental results validate the successful generation of joint trajectory profiles.",
        "abstract": "Handheld virtual reality (VR) controllers are necessary for creating immersive experiences. In this paper, we propose a gated RNN-based sequence model that estimates the joint torques of a serially linked handheld VR system interface from a sequential position input. In our previous study, we proposed a motion planning algorithm for articulated systems based on the active contour model that optimizes the positions of each joint torque based on the measured base position (6-Degrees of Freedom). Because the position-to-position scheme, which calculates the joint positions from a given base position, illustrated several limitations concerning safety (i.e. unable to handle unexpected contact with the surroundings), our current study proposes a position-to-torque generation scheme that estimates the joint torques from the measured base position sequences. To that end, we trained the sequences of joint torques and the sequence of the 6-DoF base position as a supervised learning task. To model the multivariate temporal information of the sequences, we employed a gated recurrent unit. The experimental results validate the successful generation of joint trajectory profiles.",
        "authors": "Byung-Kil Han, Dong-Soo Kwon, Seung-Chan Kim",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2880882",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The adaptive Runge–Kutta (ARK) method on multi-general-purpose graphical processing units (GPUs) is used for solving large nonlinear systems of first-order ordinary differential equations (ODEs) with over∼10000variables describing a large genetic network in systems biology for the biological clock. To carry out the computation of the trajectory of the system, a hierarchical structure of the ODEs is exploited, and an ARK solver is implemented in compute unified device architecture/C++ (CUDA/C++) on GPUs. The result is a 75-fold speedup for calculations of 2436 independent modules within the genetic network describing clock function relative to a comparable CPU architecture. These 2436 modules span one-quarter of the entire genome of a model fungal system, Neurospora crassa. The power of a GPU can in principle be harnessed by using warp-level parallelism, instruction level parallelism or both of them. Since the ARK ODE solver is entirely sequential, we propose a new parallel processing algorithm using warp-level parallelism for solving∼10000ODEs that belong to a large genetic network describing clock genome-level dynamics. A video is attached illustrating the general idea of the method on GPUs that can be used to provide new insights into the biological clock through single cell measurements on the clock.",
        "abstract": "The adaptive Runge–Kutta (ARK) method on multi-general-purpose graphical processing units (GPUs) is used for solving large nonlinear systems of first-order ordinary differential equations (ODEs) with over∼10000variables describing a large genetic network in systems biology for the biological clock. To carry out the computation of the trajectory of the system, a hierarchical structure of the ODEs is exploited, and an ARK solver is implemented in compute unified device architecture/C++ (CUDA/C++) on GPUs. The result is a 75-fold speedup for calculations of 2436 independent modules within the genetic network describing clock function relative to a comparable CPU architecture. These 2436 modules span one-quarter of the entire genome of a model fungal system, Neurospora crassa. The power of a GPU can in principle be harnessed by using warp-level parallelism, instruction level parallelism or both of them. Since the ARK ODE solver is entirely sequential, we propose a new parallel processing algorithm using warp-level parallelism for solving∼10000ODEs that belong to a large genetic network describing clock genome-level dynamics. A video is attached illustrating the general idea of the method on GPUs that can be used to provide new insights into the biological clock through single cell measurements on the clock.",
        "authors": "Ahmad Al-Omari, Heinz-Bernd Schüttler, Jonathan Arnold, Thiab Taha",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2290623",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, we propose a novel means of verifying document originality using chipless RFID systems. The document sender prints a chipless RFID tag into the paper and does a frequency scanning in the 57-64 GHz spectrum of the document. The results of scattering parameters in individual step frequencies are stored in a cloud database, denoised and passed to pattern classifiers, such as support vector machines or ensemble networks. These supervised learners train themselves based on these data on the remote/cloud computer. The document receiver verifies this frequency fingerprint by using the same scanning method, sending the scattering parameters to the cloud server and getting the decoded data. Paper originality is verified if the decoded data are as expected. The advantages of our cloud chipless RFID processing deployments are cost reduction and increased security and scalability.",
        "abstract": "In this paper, we propose a novel means of verifying document originality using chipless RFID systems. The document sender prints a chipless RFID tag into the paper and does a frequency scanning in the 57-64 GHz spectrum of the document. The results of scattering parameters in individual step frequencies are stored in a cloud database, denoised and passed to pattern classifiers, such as support vector machines or ensemble networks. These supervised learners train themselves based on these data on the remote/cloud computer. The document receiver verifies this frequency fingerprint by using the same scanning method, sending the scattering parameters to the cloud server and getting the decoded data. Paper originality is verified if the decoded data are as expected. The advantages of our cloud chipless RFID processing deployments are cost reduction and increased security and scalability.",
        "authors": "Grishma Khadka, Larry M. Arjomandi, Nemai C. Karmakar, Zixiang Xiong",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884651",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Cographs is a well-known class of graphs in graph theory, which can be generated from a single vertex by applying a series of complement (or equivalently join operations) and disjoint union operations. The distance spectrum of graphs is a rather active topic in spectral graph theory these years. This paper denotes to revealing some properties for the distance spectrum of cographs. More precisely, we present an algorithm, usingO(n)time and space, to diagonalize the distance matrix of cographs, from which one can deduce a diagonal matrix congruent to matrixD+λI, whereDis the distance matrix of a cograph,λis a real number, andIis the identity matrix. Besides, we also give some applications of such algorithm about the inertia of distance matrix of complete multipartite graphs.",
        "abstract": "Cographs is a well-known class of graphs in graph theory, which can be generated from a single vertex by applying a series of complement (or equivalently join operations) and disjoint union operations. The distance spectrum of graphs is a rather active topic in spectral graph theory these years. This paper denotes to revealing some properties for the distance spectrum of cographs. More precisely, we present an algorithm, usingO(n)time and space, to diagonalize the distance matrix of cographs, from which one can deduce a diagonal matrix congruent to matrixD+λI, whereDis the distance matrix of a cograph,λis a real number, andIis the identity matrix. Besides, we also give some applications of such algorithm about the inertia of distance matrix of complete multipartite graphs.",
        "authors": "Zhibin Du",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884621",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The performance of tactical weapons inertial navigation system (INS) greatly depends on the accuracy and rapidity of transfer alignment (TA). The major challenge is the estimation of the attitude of INS rapidly and accurately, under the condition of random initial misalignment angles and launch time constraint in tactical scenarios. The objective of this paper is to derive a unified nonsingular alignment scheme for rapid TA at random initial misalignment angles, which transforms the alignment problem into the estimation of the relative attitude matrix between the master INS and slave INS. Employing the attitude matrix as estimate state, we formulate a unified nonsingular linear dynamics model allowing us to estimate the attitude matrix directly. Furthermore, the attitude matrix estimation algorithm has been extended to work in a state matrix Kalman filter (MKF) framework, which we call MKFTA algorithm. It works without any assumption on the misalignment angles. Finally, Monte Carlo simulations demonstrate that the MKFTA can converge within 10 s and exhibits a better transient and steady-state accuracy than the traditional scheme.",
        "abstract": "The performance of tactical weapons inertial navigation system (INS) greatly depends on the accuracy and rapidity of transfer alignment (TA). The major challenge is the estimation of the attitude of INS rapidly and accurately, under the condition of random initial misalignment angles and launch time constraint in tactical scenarios. The objective of this paper is to derive a unified nonsingular alignment scheme for rapid TA at random initial misalignment angles, which transforms the alignment problem into the estimation of the relative attitude matrix between the master INS and slave INS. Employing the attitude matrix as estimate state, we formulate a unified nonsingular linear dynamics model allowing us to estimate the attitude matrix directly. Furthermore, the attitude matrix estimation algorithm has been extended to work in a state matrix Kalman filter (MKF) framework, which we call MKFTA algorithm. It works without any assumption on the misalignment angles. Finally, Monte Carlo simulations demonstrate that the MKFTA can converge within 10 s and exhibits a better transient and steady-state accuracy than the traditional scheme.",
        "authors": "Gongmin Yan, Qi Zhou, Qiangwen Fu, Xiao Cui, Zhenbo Liu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885144",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Scientific studies on species composition and abundance distribution of fishes have considerable importance to the fishery industry, biodiversity protection, and marine ecosystem. In these studies, fish images are typically collected with the help of scuba divers or autonomous underwater vehicles. These images are then annotated manually by marine biologists. Such a process is certainly a tremendous waste of manpower and material resources. In recent years, the introduction of deep learning has helped making remarkable progress in this area. However, fish image classification can be considered as fine-grained problem, which is more challenging than common image classification, especially with low-quality and small-scale data. Meanwhile, well-known effective convolutional neural networks (CNNs) consistently require a large quantity of high-quality data. This paper presents a new method by improving transfer learning and squeeze-and-excitation networks for fine-grained fish image classification on low-quality and small-scale datasets. Our method enhances data augmentation through super-resolution reconstruction to enlarge the dataset with high-quality images, pre-pretrains, and pretrains to learn common and domain knowledge simultaneously while fine-tuning with professional skill. In addition, refined squeeze-and-excitation blocks are designed to improve bilinear CNNs for a fine-grained classification. Unlike well-known CNNs for image classification, our method can classify images with insufficient low-quality training data. Moreover, we compare the performance of our method with commonly used CNNs on small-scale fine-grained datasets, namely, Croatian and QUT fish datasets. The experimental results show that our method outperforms popular CNNs with higher fish classification accuracy, which indicates its potential applications in combination with other newly updated CNNs.",
        "abstract": "Scientific studies on species composition and abundance distribution of fishes have considerable importance to the fishery industry, biodiversity protection, and marine ecosystem. In these studies, fish images are typically collected with the help of scuba divers or autonomous underwater vehicles. These images are then annotated manually by marine biologists. Such a process is certainly a tremendous waste of manpower and material resources. In recent years, the introduction of deep learning has helped making remarkable progress in this area. However, fish image classification can be considered as fine-grained problem, which is more challenging than common image classification, especially with low-quality and small-scale data. Meanwhile, well-known effective convolutional neural networks (CNNs) consistently require a large quantity of high-quality data. This paper presents a new method by improving transfer learning and squeeze-and-excitation networks for fine-grained fish image classification on low-quality and small-scale datasets. Our method enhances data augmentation through super-resolution reconstruction to enlarge the dataset with high-quality images, pre-pretrains, and pretrains to learn common and domain knowledge simultaneously while fine-tuning with professional skill. In addition, refined squeeze-and-excitation blocks are designed to improve bilinear CNNs for a fine-grained classification. Unlike well-known CNNs for image classification, our method can classify images with insufficient low-quality training data. Moreover, we compare the performance of our method with commonly used CNNs on small-scale fine-grained datasets, namely, Croatian and QUT fish datasets. The experimental results show that our method outperforms popular CNNs with higher fish classification accuracy, which indicates its potential applications in combination with other newly updated CNNs.",
        "authors": "Bing Zheng, Chao Wang, Chenchen Qiu, Haiyong Zheng, Shaoyong Zhang, Zhibin Yu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885055",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Relation detection plays a crucial role in knowledge base question answering, and it is challenging because of the high variance of relation expression in real-world questions. Traditional relation detection models based on deep learning follow an encoding-comparing paradigm, where the question and the candidate relation are represented as vectors to compare their semantic similarity. Max- or average-pooling operation, which is used to compress the sequence of words into fixed-dimensional vectors, becomes the bottleneck of information flow. In this paper, we propose an attention-based word-level interaction model (ABWIM) to alleviate the information loss issue caused by aggregating the sequence into a fixed-dimensional vector before the comparison. First, attention mechanism is adopted to learn the soft alignments between words from the question and the relation. Then, fine-grained comparisons are performed on the aligned words. Finally, the comparison results are merged with a simple recurrent layer to estimate the semantic similarity. Besides, a dynamic sample selection strategy is proposed to accelerate the training procedure without decreasing the performance. Experimental results of relation detection on both SimpleQuestions and WebQuestions datasets show that ABWIM achieves the state-of-the-art accuracy, demonstrating its effectiveness.",
        "abstract": "Relation detection plays a crucial role in knowledge base question answering, and it is challenging because of the high variance of relation expression in real-world questions. Traditional relation detection models based on deep learning follow an encoding-comparing paradigm, where the question and the candidate relation are represented as vectors to compare their semantic similarity. Max- or average-pooling operation, which is used to compress the sequence of words into fixed-dimensional vectors, becomes the bottleneck of information flow. In this paper, we propose an attention-based word-level interaction model (ABWIM) to alleviate the information loss issue caused by aggregating the sequence into a fixed-dimensional vector before the comparison. First, attention mechanism is adopted to learn the soft alignments between words from the question and the relation. Then, fine-grained comparisons are performed on the aligned words. Finally, the comparison results are merged with a simple recurrent layer to estimate the semantic similarity. Besides, a dynamic sample selection strategy is proposed to accelerate the training procedure without decreasing the performance. Experimental results of relation detection on both SimpleQuestions and WebQuestions datasets show that ABWIM achieves the state-of-the-art accuracy, demonstrating its effectiveness.",
        "authors": "Feng Li, Guandong Xu, Guangluan Xu, Hongzhi Zhang, Kun Fu, Xiao Liang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883304",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Recently, a chaotic image encryption algorithm based on information entropy (IEAIE) was proposed. This paper scrutinizes the security properties of the algorithm and evaluates the validity of the used quantifiable security metrics. When the round number is only one, the equivalent secret key of every basic operation of IEAIE can be recovered with a differential attack separately. Some common insecurity problems in the field of chaotic image encryption are found in IEAIE, e.g., the short orbits of the digital chaotic system and the invalid sensitivity mechanism built on information entropy of the plain image. Even worse, each security metric is questionable, which undermines the security credibility of IEAIE. Hence, IEAIE can only serve as a counterexample for illustrating common pitfalls in designing secure communication method for image data.",
        "abstract": "Recently, a chaotic image encryption algorithm based on information entropy (IEAIE) was proposed. This paper scrutinizes the security properties of the algorithm and evaluates the validity of the used quantifiable security metrics. When the round number is only one, the equivalent secret key of every basic operation of IEAIE can be recovered with a differential attack separately. Some common insecurity problems in the field of chaotic image encryption are found in IEAIE, e.g., the short orbits of the digital chaotic system and the invalid sensitivity mechanism built on information entropy of the plain image. Even worse, each security metric is questionable, which undermines the security credibility of IEAIE. Hence, IEAIE can only serve as a counterexample for illustrating common pitfalls in designing secure communication method for image data.",
        "authors": "Bingbing Feng, Chengqing Li, Dongdong Lin, Feng Hao, Jinhu Lü",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883690",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The past few years have witnessed a trend that the deep learning techniques have been increasingly applied in healthcare due to the explosive growth of big data. The online medical community, where users can ask qualified doctors about medical questions with just a few keystrokes and mouse clicks anytime and anywhere, has become quite popular recently. In this paper, we investigate the problem of Chinese medical question answer selection, which is a crucial subtask of automatic question answering and fairly challenging because of its language and domain characteristics. We introduce an end-to-end multiscale interactive networks framework to address the issue. The framework consists of several multi-scale deep neural layers which extract the deep semantic information of medical text from different levels of granularity, shortcut connections which prevent network degradation problem, and attentive interaction which mines the correlation between questions and answers. To evaluate our framework, we update and expand a dataset called cMedQA v2.0. Experimental results demonstrate that our model outperforms the existing state-ofthe-art models with noticeable margins.",
        "abstract": "The past few years have witnessed a trend that the deep learning techniques have been increasingly applied in healthcare due to the explosive growth of big data. The online medical community, where users can ask qualified doctors about medical questions with just a few keystrokes and mouse clicks anytime and anywhere, has become quite popular recently. In this paper, we investigate the problem of Chinese medical question answer selection, which is a crucial subtask of automatic question answering and fairly challenging because of its language and domain characteristics. We introduce an end-to-end multiscale interactive networks framework to address the issue. The framework consists of several multi-scale deep neural layers which extract the deep semantic information of medical text from different levels of granularity, shortcut connections which prevent network degradation problem, and attentive interaction which mines the correlation between questions and answers. To evaluate our framework, we update and expand a dataset called cMedQA v2.0. Experimental results demonstrate that our model outperforms the existing state-ofthe-art models with noticeable margins.",
        "authors": "Hui Wang, Lixiang Guo, Shanshan Liu, Sheng Zhang, Xin Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883637",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Information retrieval systems embed temporal information for retrieving the news documents related to temporal queries. One of the important aspects of a news document is the focus time, a time to which the content of document refers. The contemporary state-of-the-art does not exploit focus time to retrieve relevant news document. This paper investigates the inverted pyramid news paradigm to determine the focus time of news documents by extracting temporal expressions, normalizing their value and assigning them a score on the basis of their position in the text. In this method, the news documents are first divided into three sections following the inverted pyramid news paradigm. This paper presents a comprehensive analysis of four methods for splitting news document into sections: the paragraph-based method, the words-based method, the sentence-based method, and the semantic-based method (SeBM). Temporal expressions in each section are assigned weights using a linear regression model. Finally, a scoring function is used to calculate a temporal score for each time expression appearing in the document. These temporal expressions are then ranked on the basis of their temporal score, where the most suitable expression appears on top. The effectiveness of the proposed method is evaluated on a diverse dataset of news related to popular events; the results revealed that the proposed splitting methods achieved an average error of less than 5.6 years, whereas the SeBM achieved a high precision score of 0.35 and 0.77 at positions 1 and 2, respectively.",
        "abstract": "Information retrieval systems embed temporal information for retrieving the news documents related to temporal queries. One of the important aspects of a news document is the focus time, a time to which the content of document refers. The contemporary state-of-the-art does not exploit focus time to retrieve relevant news document. This paper investigates the inverted pyramid news paradigm to determine the focus time of news documents by extracting temporal expressions, normalizing their value and assigning them a score on the basis of their position in the text. In this method, the news documents are first divided into three sections following the inverted pyramid news paradigm. This paper presents a comprehensive analysis of four methods for splitting news document into sections: the paragraph-based method, the words-based method, the sentence-based method, and the semantic-based method (SeBM). Temporal expressions in each section are assigned weights using a linear regression model. Finally, a scoring function is used to calculate a temporal score for each time expression appearing in the document. These temporal expressions are then ranked on the basis of their temporal score, where the most suitable expression appears on top. The effectiveness of the proposed method is evaluated on a diverse dataset of news related to popular events; the results revealed that the proposed splitting methods achieved an average error of less than 5.6 years, whereas the SeBM achieved a high precision score of 0.35 and 0.77 at positions 1 and 2, respectively.",
        "authors": "Muhammad Aleem, Muhammad Arshad Islam, Muhammad Azhar Iqbal, Shafiq Ur Rehman Khan, Usman Ahmed",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2882988",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Rainfed lowland rice is the most popular choice for rice cultivation in Sarawak, Borneo. In general, rice production in Sarawak consists of seven phases, namely, preparing land, establishing crop, transplanting, managing crop, harvesting, post-harvesting, and milling. Most farmers in Sarawak depend on indigenous knowledge and experience for rice cultivation. In this paper, an improved fuzzy failure mode and effect analysis (FMEA) with genetic algorithm-based design of fuzzy membership functions and monotone fuzzy rules relabeling is employed as a knowledge-based tool for risk analysis and assessment pertaining to rice production in Sarawak. The specific focus is on issues related to the environment as well as health and safety of farmers and consumers. With the support from the Sarawak Government, we analyze useful data and information pertaining to various rice fields from experienced farmers to develop the fuzzy FMEA model. Specifically, we develop fuzzy FMEA to inculcate the best practices for farmers to improve yield and enhance food safety. Through this paper, we identify that musculoskeletal disorders due to bad postures is the most noticeable occupational health hazard; as a result, new techniques and tools are invented and introduced to mitigate this risk. In summary, this is a new attempt to implement a quality and risk assessment tool that contributes toward enhancing rice productivity in Sarawak, and modernizing the local agricultural sector.",
        "abstract": "Rainfed lowland rice is the most popular choice for rice cultivation in Sarawak, Borneo. In general, rice production in Sarawak consists of seven phases, namely, preparing land, establishing crop, transplanting, managing crop, harvesting, post-harvesting, and milling. Most farmers in Sarawak depend on indigenous knowledge and experience for rice cultivation. In this paper, an improved fuzzy failure mode and effect analysis (FMEA) with genetic algorithm-based design of fuzzy membership functions and monotone fuzzy rules relabeling is employed as a knowledge-based tool for risk analysis and assessment pertaining to rice production in Sarawak. The specific focus is on issues related to the environment as well as health and safety of farmers and consumers. With the support from the Sarawak Government, we analyze useful data and information pertaining to various rice fields from experienced farmers to develop the fuzzy FMEA model. Specifically, we develop fuzzy FMEA to inculcate the best practices for farmers to improve yield and enhance food safety. Through this paper, we identify that musculoskeletal disorders due to bad postures is the most noticeable occupational health hazard; as a result, new techniques and tools are invented and introduced to mitigate this risk. In summary, this is a new attempt to implement a quality and risk assessment tool that contributes toward enhancing rice productivity in Sarawak, and modernizing the local agricultural sector.",
        "authors": "Anisia Jati Sang, Chee Peng Lim, Kai Meng Tay, Saeid Nahavandi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883115",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Kalman filters (KFs) and dynamic observers are two main classes of the dynamic state estimation (DSE) routines. The Power system DSE has been implemented by various KFs, such as the extended KF (EKF) and the unscented KF (UKF). In this paper, we discuss two challenges for an effective power system DSE: 1) model uncertainty and 2) potential cyber attacks and measurement faults. To address this, the cubature KF (CKF) and a nonlinear observer are introduced and implemented. Various KFs and the dynamic observer are then tested on the 16-machine 68-bus system given realistic scenarios under model uncertainty and different types of cyber attacks against synchrophasor measurements. It is shown that the CKF and the observer are more robust to model uncertainty and cyber attacks than their counterparts. Based on the tests, a thorough qualitative comparison is also performed for KF routines and observers.",
        "abstract": "Kalman filters (KFs) and dynamic observers are two main classes of the dynamic state estimation (DSE) routines. The Power system DSE has been implemented by various KFs, such as the extended KF (EKF) and the unscented KF (UKF). In this paper, we discuss two challenges for an effective power system DSE: 1) model uncertainty and 2) potential cyber attacks and measurement faults. To address this, the cubature KF (CKF) and a nonlinear observer are introduced and implemented. Various KFs and the dynamic observer are then tested on the 16-machine 68-bus system given realistic scenarios under model uncertainty and different types of cyber attacks against synchrophasor measurements. It is shown that the CKF and the observer are more robust to model uncertainty and cyber attacks than their counterparts. Based on the tests, a thorough qualitative comparison is also performed for KF routines and observers.",
        "authors": "Ahmad F. Taha, Jianhui Wang, Junjian Qi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2876883",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper provides the design and implementation of anL1-optimal control of a quadrotor unmanned aerial vehicle (UAV). The quadrotor UAV is an underactuated rigid body with four propellers that generate forces along the rotor axes. These four forces are used to achieve asymptotic tracking of four outputs, namely the position of the center of mass of the UAV and the heading. With perfect knowledge of plant parameters and no measurement noise, the magnitudes of the errors are shown to exponentially converge to zero. In the case of parametric uncertainty and measurement noise, the controller yields an exponential decrease of the magnitude of the errors in anL1-optimal sense. In other words, the controller is designed so that it minimizes theL∞-gain of the plant with respect to disturbances. The performance of the controller is evaluated in experiments and compared with that of a related robust nonlinear controller in the literature. The experimental data shows that the proposed controller rejects persistent disturbances, which is quantified by a very small magnitude of the mean error.",
        "abstract": "This paper provides the design and implementation of anL1-optimal control of a quadrotor unmanned aerial vehicle (UAV). The quadrotor UAV is an underactuated rigid body with four propellers that generate forces along the rotor axes. These four forces are used to achieve asymptotic tracking of four outputs, namely the position of the center of mass of the UAV and the heading. With perfect knowledge of plant parameters and no measurement noise, the magnitudes of the errors are shown to exponentially converge to zero. In the case of parametric uncertainty and measurement noise, the controller yields an exponential decrease of the magnitude of the errors in anL1-optimal sense. In other words, the controller is designed so that it minimizes theL∞-gain of the plant with respect to disturbances. The performance of the controller is evaluated in experiments and compared with that of a related robust nonlinear controller in the literature. The experimental data shows that the proposed controller rejects persistent disturbances, which is quantified by a very small magnitude of the mean error.",
        "authors": "Aykut C. Satici, Hasan Poonawala, Mark W. Spong",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2260794",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The economic and social impact of poor air quality in towns and cities is increasingly being recognized, together with the need for effective ways of creating awareness of real-time air quality levels and their impact on human health. With local authority maintained monitoring stations being geographically sparse and the resultant datasets also featuring missing labels, computational data-driven mechanisms are needed to address the data sparsity challenge. In this paper, we propose a machine learning-based method to accurately predict the air quality index, using environmental monitoring data together with meteorological measurements. To do so, we develop an air quality estimation framework that implements a neural network that is enhanced with a novel non-linear autoregressive neural network with exogenous input model, especially designed for time series prediction. The framework is applied to a case study featuring different monitoring sites in London, with comparisons against other standard machine-learning-based predictive algorithms showing the feasibility and robust performance of the proposed method for different kinds of areas within an urban region.",
        "abstract": "The economic and social impact of poor air quality in towns and cities is increasingly being recognized, together with the need for effective ways of creating awareness of real-time air quality levels and their impact on human health. With local authority maintained monitoring stations being geographically sparse and the resultant datasets also featuring missing labels, computational data-driven mechanisms are needed to address the data sparsity challenge. In this paper, we propose a machine learning-based method to accurately predict the air quality index, using environmental monitoring data together with meteorological measurements. To do so, we develop an air quality estimation framework that implements a neural network that is enhanced with a novel non-linear autoregressive neural network with exogenous input model, especially designed for time series prediction. The framework is applied to a case study featuring different monitoring sites in London, with comparisons against other standard machine-learning-based predictive algorithms showing the feasibility and robust performance of the proposed method for different kinds of areas within an urban region.",
        "authors": "Charith Perera, Gideon Ewa, Klaus Moessner, Suparna De, Yuchao Zhou",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884647",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper develops an adaptive radical basis function neural-network (NN)-based controller design strategy that uses integral Lyapunov functions for a class of non-strict-feedback nonlinear systems subject to perturbations. The design difficulty caused by the non-strict-feedback system structure is handled by using the inherent property of the square of neural network’s base vector. The design procedure of the adaptive NN tracking controller is presented by using backstepping technique, which can update the adaptive laws at any time and solve the design problem derived from the correlation degree of the controlled plant. The uniform ultimate boundedness and good tracking performance of the derived closed-loop system are ensured with the design controller. Finally, a comparative simulation example is carried out to prove the effectiveness of the proposed control method.",
        "abstract": "This paper develops an adaptive radical basis function neural-network (NN)-based controller design strategy that uses integral Lyapunov functions for a class of non-strict-feedback nonlinear systems subject to perturbations. The design difficulty caused by the non-strict-feedback system structure is handled by using the inherent property of the square of neural network’s base vector. The design procedure of the adaptive NN tracking controller is presented by using backstepping technique, which can update the adaptive laws at any time and solve the design problem derived from the correlation degree of the controlled plant. The uniform ultimate boundedness and good tracking performance of the derived closed-loop system are ensured with the design controller. Finally, a comparative simulation example is carried out to prove the effectiveness of the proposed control method.",
        "authors": "Ben Niu, Dong Yang, Guo-qiang Wu, Jun-Qing Li, Pei-yong Duan, Xiao-Mei Wang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884080",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Based on the LaSalle invariant principle of stochastic differential equations and the graph theory, several sufficient conditions are obtained by adaptive feedback control and impulsive control. The error dynamical system is globally almost surely asymptotically stable. The conclusion is new and feasible. Finally, an example is given to show the validity of the results.",
        "abstract": "Based on the LaSalle invariant principle of stochastic differential equations and the graph theory, several sufficient conditions are obtained by adaptive feedback control and impulsive control. The error dynamical system is globally almost surely asymptotically stable. The conclusion is new and feasible. Finally, an example is given to show the validity of the results.",
        "authors": "Chaolong Zhang, Feiqi Deng, Ting Hou, Weihua Mao",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2879871",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A team of robots are deployed to accomplish a task while maintaining a viable ad-hoc network capable of supporting data transmissions necessary for task fulfillment. Solving this problem necessitates: 1) estimation of the wireless propagation environment to identify viable point-to-point communication links; 2) determination of end-to-end routes to support data traffic; and 3) motion control algorithms to navigate through spatial configurations that guarantee required minimum levels of service. Therefore, we present methods for: 1) estimation of point-to-point channels using pathloss and spatial Gaussian process models; 2) data routing so as to determine suitable end-to-end communication routes given estimates of point-to-point channel rates; and 3) motion planning to determine robot trajectories restricted to configurations that ensure survival of the communication network. Because of the inherent uncertainty of wireless channels, the model of links and routes is stochastic. The criteria for route selection is to maximize the probability of network survival—defined as the ability to support target communication rates—given achievable rates on local point-to-point links. Maximum survival probability routes for present and future positions are input into a mobility control module that determines robot trajectories restricted to configurations that ensure the probability of network survival stays above a minimum reliability level. Local trajectory planning is proposed for simple environments and global planning is proposed for complex surroundings. The three proposed components are integrated and tested in experiments run in two different environments. Experimental results show successful navigation with continuous end-to-end connectivity.",
        "abstract": "A team of robots are deployed to accomplish a task while maintaining a viable ad-hoc network capable of supporting data transmissions necessary for task fulfillment. Solving this problem necessitates: 1) estimation of the wireless propagation environment to identify viable point-to-point communication links; 2) determination of end-to-end routes to support data traffic; and 3) motion control algorithms to navigate through spatial configurations that guarantee required minimum levels of service. Therefore, we present methods for: 1) estimation of point-to-point channels using pathloss and spatial Gaussian process models; 2) data routing so as to determine suitable end-to-end communication routes given estimates of point-to-point channel rates; and 3) motion planning to determine robot trajectories restricted to configurations that ensure survival of the communication network. Because of the inherent uncertainty of wireless channels, the model of links and routes is stochastic. The criteria for route selection is to maximize the probability of network survival—defined as the ability to support target communication rates—given achievable rates on local point-to-point links. Maximum survival probability routes for present and future positions are input into a mobility control module that determines robot trajectories restricted to configurations that ensure the probability of network survival stays above a minimum reliability level. Local trajectory planning is proposed for simple environments and global planning is proposed for complex surroundings. The three proposed components are integrated and tested in experiments run in two different environments. Experimental results show successful navigation with continuous end-to-end connectivity.",
        "authors": "Alejandro Ribeiro, Jonathan Fink, Vijay Kumar",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2262013",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Important progress has been made by many researchers in extracting fundamental design principles from patterns in design parameters along the nondominated front generated by evolutionary algorithms in biobjective optimization problems. However, to the best of our knowledge, no attention has been given to discovering design principles from the wealth of additional information available from patterns in dominated solutions. To explore the same, we use heatmaps of dominated solutions to visualize how relevant variables self-organize with respect to the objectives throughout the feasible region. We overlay ceteris paribus lines on these heatmaps to show how the objective values change when a given design variable is varied while all others are held constant. We use three biobjective optimization problems to demonstrate various ways in which these visualization techniques can provide additional useful information beyond that which can be determined from the nondominated front. Specifically, we investigate a simple two-member truss design problem, a simple welded beam design problem, and a real-world watershed management design problem to illustrate: 1) how principles derived from the nondominated front alone can be misleading; 2) how new principles can be derived from the dominated solutions; and 3) how nondominated solutions can often be fragile with respect to assumptions about uncertain external forcing conditions, whereas solutions a short distance inside the front are often much more robust.",
        "abstract": "Important progress has been made by many researchers in extracting fundamental design principles from patterns in design parameters along the nondominated front generated by evolutionary algorithms in biobjective optimization problems. However, to the best of our knowledge, no attention has been given to discovering design principles from the wealth of additional information available from patterns in dominated solutions. To explore the same, we use heatmaps of dominated solutions to visualize how relevant variables self-organize with respect to the objectives throughout the feasible region. We overlay ceteris paribus lines on these heatmaps to show how the objective values change when a given design variable is varied while all others are held constant. We use three biobjective optimization problems to demonstrate various ways in which these visualization techniques can provide additional useful information beyond that which can be determined from the nondominated front. Specifically, we investigate a simple two-member truss design problem, a simple welded beam design problem, and a real-world watershed management design problem to illustrate: 1) how principles derived from the nondominated front alone can be misleading; 2) how new principles can be derived from the dominated solutions; and 3) how nondominated solutions can often be fragile with respect to assumptions about uncertain external forcing conditions, whereas solutions a short distance inside the front are often much more robust.",
        "authors": "Karim J. Chichakly, Margaret J. Eppstein",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2262491",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Differential evolution (DE) is a competitive and reliable computing technique for continuous optimization. A diversity-based selection has been proved to be valid to improve the performance of DE. However, further study can be done. In this paper, we propose two versions of colony fitness, fitness with the consideration of diversity information. Selection based on the first version of the colony is embodied in DE/rand/1, a basic DE algorithm, while selection based on the second version is used in CoBiDE, a state-of-the-art DE algorithm. Our experiments are based on the 2005 Congress on Evolutionary Computation and the 2014 Congress on Evolutionary Computation benchmark functions. Experimental results show that our modification on algorithms leads to significantly better solutions than before.",
        "abstract": "Differential evolution (DE) is a competitive and reliable computing technique for continuous optimization. A diversity-based selection has been proved to be valid to improve the performance of DE. However, further study can be done. In this paper, we propose two versions of colony fitness, fitness with the consideration of diversity information. Selection based on the first version of the colony is embodied in DE/rand/1, a basic DE algorithm, while selection based on the second version is used in CoBiDE, a state-of-the-art DE algorithm. Our experiments are based on the 2005 Congress on Evolutionary Computation and the 2014 Congress on Evolutionary Computation benchmark functions. Experimental results show that our modification on algorithms leads to significantly better solutions than before.",
        "authors": "Jinyi Guo, Shijie Peng, Xuechao Wu, Yang Li, Zi Ming",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884982",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In order to forecast the traffic flow more precisely, a novel hybrid model is proposed with multiple sources of traffic data in the spatiotemporal dimension. In the practical application of the proposed model, multiple sources of data are captured and fused from five toll collection gates and one remote microwave sensor based on the correlation analysis. A hybrid model, including the structure of stacked autoencoders and long short-term memory, is used. Stacked autoencoders are used to extract the spatial features. Long short-term memory is used to learn the temporal features. The comparisons of the hybrid model, non-hybrid model, fused data, and non-fused data are provided. The effectiveness of the hybrid model and the fused data demonstrated the best performance. The fused data presented more effective forecast, which encourages that the forecasting model could include more data source to improve the accuracy. Meanwhile, the selection of a suitable model should also be studied for better forecasting result in consideration of difference feature of the data source. The high-accuracy prediction could contribute to further traffic control and prompt the development of the intelligent transport system.",
        "abstract": "In order to forecast the traffic flow more precisely, a novel hybrid model is proposed with multiple sources of traffic data in the spatiotemporal dimension. In the practical application of the proposed model, multiple sources of data are captured and fused from five toll collection gates and one remote microwave sensor based on the correlation analysis. A hybrid model, including the structure of stacked autoencoders and long short-term memory, is used. Stacked autoencoders are used to extract the spatial features. Long short-term memory is used to learn the temporal features. The comparisons of the hybrid model, non-hybrid model, fused data, and non-fused data are provided. The effectiveness of the hybrid model and the fused data demonstrated the best performance. The fused data presented more effective forecast, which encourages that the forecasting model could include more data source to improve the accuracy. Meanwhile, the selection of a suitable model should also be studied for better forecasting result in consideration of difference feature of the data source. The high-accuracy prediction could contribute to further traffic control and prompt the development of the intelligent transport system.",
        "authors": "Erlong Tan, Guiping Wang, Jun Wang, Li Li, Ping Wang, Yinli Jin",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884933",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Low-dose computed tomography (LDCT) images are polluted by mottle noise and streak artifacts. To improve LDCT images quality, this paper proposes a novel total variation (NTV) model. A weighted coefficient of the regularization term of NTV model is constructed by standard deviation, gray-level probability and gradient magnitude to smooth LDCT images adaptively, since the standard deviation and the gray-level probability of detail region are higher than that of the noisy background, and the gradient magnitude of edges is higher than that of the noisy background. Besides, to preserve details and edges effectively, the fidelity term of the proposed NTV model is constructed by the block-matching 3d filter because it performs well in details and edges preservation. The experiments are performed on the computer simulated phantom and the actual phantom. Compared with several other competitive methods, both subjective visual effect and objective evaluation criteria show that the proposed NTV model can improve LDCT images quality more effectively such as noise and artifacts suppression, details, and edges preservation.",
        "abstract": "Low-dose computed tomography (LDCT) images are polluted by mottle noise and streak artifacts. To improve LDCT images quality, this paper proposes a novel total variation (NTV) model. A weighted coefficient of the regularization term of NTV model is constructed by standard deviation, gray-level probability and gradient magnitude to smooth LDCT images adaptively, since the standard deviation and the gray-level probability of detail region are higher than that of the noisy background, and the gradient magnitude of edges is higher than that of the noisy background. Besides, to preserve details and edges effectively, the fidelity term of the proposed NTV model is constructed by the block-matching 3d filter because it performs well in details and edges preservation. The experiments are performed on the computer simulated phantom and the actual phantom. Compared with several other competitive methods, both subjective visual effect and objective evaluation criteria show that the proposed NTV model can improve LDCT images quality more effectively such as noise and artifacts suppression, details, and edges preservation.",
        "authors": "Linhong Yao, Quan Zhang, Wenbin Chen, Yanling Shao, Yanling Wang, Yi Liu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885514",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Brain health quality pre-monitoring has become an urgent need, and this is a system of complex engineering. From the perspective of intelligent decision-making based on big data, the intelligent air index prediction is introduced, the popular classification algorithm is introduced, the hidden information of historical data is mined, and the brain health quality prediction is realized. The brain health quality monitoring system based on the Internet of Things is constructed, and the classification algorithm is used to realize real-time acquisition, intelligent processing of data. In order to improve the data processing speed and enhance the real-time performance of brain health quality prediction, this paper introduces cloud computing technology to accelerate data processing. In order to enable users to understand the air index, anytime and anywhere, it is also designed based on the problem of large historical data of air index and real-time data collection. The Android platform develops an air index forecast client.",
        "abstract": "Brain health quality pre-monitoring has become an urgent need, and this is a system of complex engineering. From the perspective of intelligent decision-making based on big data, the intelligent air index prediction is introduced, the popular classification algorithm is introduced, the hidden information of historical data is mined, and the brain health quality prediction is realized. The brain health quality monitoring system based on the Internet of Things is constructed, and the classification algorithm is used to realize real-time acquisition, intelligent processing of data. In order to improve the data processing speed and enhance the real-time performance of brain health quality prediction, this paper introduces cloud computing technology to accelerate data processing. In order to enable users to understand the air index, anytime and anywhere, it is also designed based on the problem of large historical data of air index and real-time data collection. The Android platform develops an air index forecast client.",
        "authors": "Qiang Zhao, Qianyu Zhou, Wanchang Jiang, Yuan Huang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885142",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Software change impact analysis (CIA) is a key technique to identify the potential ripple effects of the changes to software. Coarse-grained CIA techniques such as file, class and method level techniques often gain less precise change impacts, which are difficult for practical use. Fine-grained CIA techniques, such as slicing, can be used to gain more precise change impacts, but need more time and space cost. In this paper, by combining the features of the coarse-grained technique and the fine-grained technique, a variable-method (VM) correlation-based CIA technique called VM-CIA is proposed to improve the precision of static CIA. First, the VM-CIA technique uses the abstract syntax tree (AST) of program to construct a novel intermediate representation called variable and method triple (VMT), which is used to analyze the correlation between the variables and methods. Second, the VM-CIA technique proposes the single-change impact analysis algorithm and multi-change impact analysis algorithm to compute the impact set based on the VMT representation. In addition, the VM-CIA technique can get a sorted impact set which is more accurate than the existing CIA techniques. The empirical results show that the VM-CIA technique can greatly improve the precision (19%) over traditional the CIA techniques, while at the cost of a little recall (5%). Moreover, the empirical studies also show that the VM-CIA technique predicts a ranked list of potential impact results according to the distance measure, which can greatly facilitate the practical use.",
        "abstract": "Software change impact analysis (CIA) is a key technique to identify the potential ripple effects of the changes to software. Coarse-grained CIA techniques such as file, class and method level techniques often gain less precise change impacts, which are difficult for practical use. Fine-grained CIA techniques, such as slicing, can be used to gain more precise change impacts, but need more time and space cost. In this paper, by combining the features of the coarse-grained technique and the fine-grained technique, a variable-method (VM) correlation-based CIA technique called VM-CIA is proposed to improve the precision of static CIA. First, the VM-CIA technique uses the abstract syntax tree (AST) of program to construct a novel intermediate representation called variable and method triple (VMT), which is used to analyze the correlation between the variables and methods. Second, the VM-CIA technique proposes the single-change impact analysis algorithm and multi-change impact analysis algorithm to compute the impact set based on the VMT representation. In addition, the VM-CIA technique can get a sorted impact set which is more accurate than the existing CIA techniques. The empirical results show that the VM-CIA technique can greatly improve the precision (19%) over traditional the CIA techniques, while at the cost of a little recall (5%). Moreover, the empirical studies also show that the VM-CIA technique predicts a ranked list of potential impact results according to the distance measure, which can greatly facilitate the practical use.",
        "authors": "Bixin Li, Chunling Hu, Xiaobing Sun",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883533",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A better understanding of the degradation modes and rates for photovoltaic (PV) modules is necessary to optimize and extend the lifetime of these modules. Lifetime and degradation science (L&DS) is used to understand degradation modes, mechanisms and rates of materials, components and systems to predict lifetime of PV modules. A PV module lifetime and degradation science (PVM L&DS) model is an essential component to predict lifetime and mitigate degradation of PV modules using reproducible open data science. Previously published accelerated testing data from Underwriter Laboratories on PV modules with fluorinated polyester backsheets, which included eight modules that were exposed up to 4000 hrs of damp heat (85% relative humidity at 85∘C) and eight exposed up to 4000 hrs of ultraviolet light (80W/m2of 280–400 nm wavelengths at 60∘C) (UV preconditioning) were used to determine statistically significant relationships between the applied stresses and measured responses. There were 15 different variables tracking aspects of system performance, degradation mechanisms, component metrics and time. Modules were analyzed for three system performance metrics (fill factor, peak power, and wet insulation). The results were statistically analyzed to identify variable transformations, statistically significant relationships (SSRs) and to develop the PVM L&DS model informed by a generalization of structural equation modeling techniques. The SSRs and significant model coefficients, combined with domain analytics, incorporating materials science, chemistry, and physics expertise, produced a pathway diagram ranking the variables' impact on the system performance, which were iteratively examined using sound statistical analysis and diagnostics. The SSRs determined from the damp heat exposure for the system response of Pmax corresponded to the degradation pathway of polyester terephthalate (PET) and ethylene vinyl acetate (EVA) hydroly...",
        "abstract": "A better understanding of the degradation modes and rates for photovoltaic (PV) modules is necessary to optimize and extend the lifetime of these modules. Lifetime and degradation science (L&DS) is used to understand degradation modes, mechanisms and rates of materials, components and systems to predict lifetime of PV modules. A PV module lifetime and degradation science (PVM L&DS) model is an essential component to predict lifetime and mitigate degradation of PV modules using reproducible open data science. Previously published accelerated testing data from Underwriter Laboratories on PV modules with fluorinated polyester backsheets, which included eight modules that were exposed up to 4000 hrs of damp heat (85% relative humidity at 85∘C) and eight exposed up to 4000 hrs of ultraviolet light (80W/m2of 280–400 nm wavelengths at 60∘C) (UV preconditioning) were used to determine statistically significant relationships between the applied stresses and measured responses. There were 15 different variables tracking aspects of system performance, degradation mechanisms, component metrics and time. Modules were analyzed for three system performance metrics (fill factor, peak power, and wet insulation). The results were statistically analyzed to identify variable transformations, statistically significant relationships (SSRs) and to develop the PVM L&DS model informed by a generalization of structural equation modeling techniques. The SSRs and significant model coefficients, combined with domain analytics, incorporating materials science, chemistry, and physics expertise, produced a pathway diagram ranking the variables' impact on the system performance, which were iteratively examined using sound statistical analysis and diagnostics. The SSRs determined from the damp heat exposure for the system response of Pmax corresponded to the degradation pathway of polyester terephthalate (PET) and ethylene vinyl acetate (EVA) hydroly...",
        "authors": "Carl K. Wang, Ethan Wang, Ivan Chou, Junheng Ma, Laura S. Bruckman, Nicholas R. Wheeler",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2267611",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Traditional methods of stripe noise removal based on space domain or transformation domain generally cannot handle the case where the noise is extremely sparse. To solve this problem, we propose a novel approach to accurately detect and remove the stripe noise by analyzing the directional and structural information of the stripe noise. First, we build a preselected stripe noise lines set by using local progressive probabilistic Hough transform. Subsequently, the real stripe noise lines are screened out from this set according to the feature of grayscale discontinuities. Finally, our approach uses the strategy of neighborhood grayscale weighted replacement and a local Gaussian filter to perform image destriping. Extensive experiments demonstrate that our approach proposed in this paper outperforms other recent promising methods in terms of quantitative assessments, qualitative assessments, and computing time.",
        "abstract": "Traditional methods of stripe noise removal based on space domain or transformation domain generally cannot handle the case where the noise is extremely sparse. To solve this problem, we propose a novel approach to accurately detect and remove the stripe noise by analyzing the directional and structural information of the stripe noise. First, we build a preselected stripe noise lines set by using local progressive probabilistic Hough transform. Subsequently, the real stripe noise lines are screened out from this set according to the feature of grayscale discontinuities. Finally, our approach uses the strategy of neighborhood grayscale weighted replacement and a local Gaussian filter to perform image destriping. Extensive experiments demonstrate that our approach proposed in this paper outperforms other recent promising methods in terms of quantitative assessments, qualitative assessments, and computing time.",
        "authors": "Chenggui Li, Qianyi Wang, Xuan Zhang, Yufu Qu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883459",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "We comprehensively study the characteristic-mode analysis (CMA) of arbitrarily shaped 3-D conducting objects embedded in a lossless planarly layered medium. The electric field integral equation (EFIE) with the layered medium Green's function is applied as the core equation for numerical analysis. The modal behavior of an object in a layered medium is first comparatively studied with that in free space. The low-frequency breakdown problem of the EFIE-based CMA and its remedy is then elaborated. The asymptotic frequency dependence of the characteristic eigenvalues for the low-order modes is numerically investigated. A theoretical analysis is conducted by constructing a frequency-independent generalized eigenvalue problem and applying Taylor expansions. Finally, the modal expansion for scattering problems and parameter extraction for circuit problems are discussed, both in the context of a layered background. Since the electromagnetic property of an electrically small object is mainly governed by a few of low-order modes, this paper may provide an effective way of model order reduction for complicated structures residing in a layered medium.",
        "abstract": "We comprehensively study the characteristic-mode analysis (CMA) of arbitrarily shaped 3-D conducting objects embedded in a lossless planarly layered medium. The electric field integral equation (EFIE) with the layered medium Green's function is applied as the core equation for numerical analysis. The modal behavior of an object in a layered medium is first comparatively studied with that in free space. The low-frequency breakdown problem of the EFIE-based CMA and its remedy is then elaborated. The asymptotic frequency dependence of the characteristic eigenvalues for the low-order modes is numerically investigated. A theoretical analysis is conducted by constructing a frequency-independent generalized eigenvalue problem and applying Taylor expansions. Finally, the modal expansion for scattering problems and parameter extraction for circuit problems are discussed, both in the context of a layered background. Since the electromagnetic property of an electrically small object is mainly governed by a few of low-order modes, this paper may provide an effective way of model order reduction for complicated structures residing in a layered medium.",
        "authors": "Min Meng, Zaiping Nie",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883577",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Real-time kinematic (RTK) ambiguity resolution can be improved by engaging multiple reference antennas. The baseline information between the antennas can be taken as true values and used as a hard-constraint; or it can be regarded as weighted, a priori measurements, and used as a softconstraint. In this contribution, a comparison between the two constraints is made to explicate their difference in RTK ambiguity resolution for some specific applications, in which the coordinates of the reference antennas cannot be accurately calibrated in advance. The functional and stochastic models of the hard and soft-constrained RTK positioning are given. The float ambiguity precision of the unconstrained, the hard and the soft-constrained cases is compared. The closed-formulae of the ambiguity dilution of precision (ADOP) for the unconstrained and the hard-constrained models are derived, and they are proven to be the upper and lower bound of the soft-constrained ADOP, respectively. A simulated and a real data show that, with the hard or soft-constraints, the single-frequency ambiguity resolution success rate, as well as the performance in time-to-first-fix (TTFF), is improved significantly. When there is a bias smaller than 6 cm in the a priori baseline vectors between the reference antennas, the two constraints have a comparable performance in ambiguity success rate. Also, a sharp increase in the hard-constrained TTFF is observed at the same time. When the bias gets larger, the hard-constrained ambiguity success rates are decreased drastically. In contrast, the soft-constraint maintains a relatively good performance, showing a much greater tolerance for bias.",
        "abstract": "Real-time kinematic (RTK) ambiguity resolution can be improved by engaging multiple reference antennas. The baseline information between the antennas can be taken as true values and used as a hard-constraint; or it can be regarded as weighted, a priori measurements, and used as a softconstraint. In this contribution, a comparison between the two constraints is made to explicate their difference in RTK ambiguity resolution for some specific applications, in which the coordinates of the reference antennas cannot be accurately calibrated in advance. The functional and stochastic models of the hard and soft-constrained RTK positioning are given. The float ambiguity precision of the unconstrained, the hard and the soft-constrained cases is compared. The closed-formulae of the ambiguity dilution of precision (ADOP) for the unconstrained and the hard-constrained models are derived, and they are proven to be the upper and lower bound of the soft-constrained ADOP, respectively. A simulated and a real data show that, with the hard or soft-constraints, the single-frequency ambiguity resolution success rate, as well as the performance in time-to-first-fix (TTFF), is improved significantly. When there is a bias smaller than 6 cm in the a priori baseline vectors between the reference antennas, the two constraints have a comparable performance in ambiguity success rate. Also, a sharp increase in the hard-constrained TTFF is observed at the same time. When the bias gets larger, the hard-constrained ambiguity success rates are decreased drastically. In contrast, the soft-constraint maintains a relatively good performance, showing a much greater tolerance for bias.",
        "authors": "Chunlei Pang, Liang Zhang, Shaoshi Wu, Xiubin Zhao",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884838",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The vehicle test big data has important significance for the study of vehicle performance and characteristics. Aiming at the test of the dynamic stiffness characteristics of a bogie suspension system of rail vehicles, a simplified rigid-flexible hybrid model of the bogie is established, and the force condition of the bogie suspension system is analyzed. Based on this simplified model, a mathematical model of the dynamic stiffness of the primary suspension, secondary suspension, and integrated suspension is established. In addition, a method for testing the three-way dynamic stiffness of a bogie suspension system in a complete, assembled state is proposed, and a dynamic stiffness test model of the bogie primary suspension, secondary suspension, and comprehensive suspension is established. Dynamic stiffness tests of the primary suspension, secondary suspension, and integrated suspension were carried out, and according to the big data analysis of the bench test, the dynamic stiffness curve of each suspension in the frequency of 0.5-5 Hz is obtained. The test results show that the dynamic stiffness of the suspension of the bogies varies nonlinearly with the change in frequency. The dynamic stiffness values of the suspensions at different frequencies vary greatly. As a result, the vehicle's operating characteristics cannot be evaluated based on the suspension static stiffness or the suspension stiffness at a single frequency, indicating the necessity of the suspension dynamic stiffness test after the bogie is assembled.",
        "abstract": "The vehicle test big data has important significance for the study of vehicle performance and characteristics. Aiming at the test of the dynamic stiffness characteristics of a bogie suspension system of rail vehicles, a simplified rigid-flexible hybrid model of the bogie is established, and the force condition of the bogie suspension system is analyzed. Based on this simplified model, a mathematical model of the dynamic stiffness of the primary suspension, secondary suspension, and integrated suspension is established. In addition, a method for testing the three-way dynamic stiffness of a bogie suspension system in a complete, assembled state is proposed, and a dynamic stiffness test model of the bogie primary suspension, secondary suspension, and comprehensive suspension is established. Dynamic stiffness tests of the primary suspension, secondary suspension, and integrated suspension were carried out, and according to the big data analysis of the bench test, the dynamic stiffness curve of each suspension in the frequency of 0.5-5 Hz is obtained. The test results show that the dynamic stiffness of the suspension of the bogies varies nonlinearly with the change in frequency. The dynamic stiffness values of the suspensions at different frequencies vary greatly. As a result, the vehicle's operating characteristics cannot be evaluated based on the suspension static stiffness or the suspension stiffness at a single frequency, indicating the necessity of the suspension dynamic stiffness test after the bogie is assembled.",
        "authors": "Huiying Lin, Jian Su, Yinrui Zhang, Zhihui Niu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884957",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Using machine intelligence on device and process performance prediction is an emerging methodology in the IC industry. While semiconductor technology computer-aided design (TCAD) has been researched and developed for over 30 years, it should contribute to or be used in conjunction with machine learning algorithms in solution finding procedure. Here, we propose an adaptive weighting neural network (AWNN) model that combines the advantages of statistical the machine learning model and the physical TCAD model. Using aspect ratio dependent etching as an example, our proposed AWNN outperforms conventional artificial neural network in terms of mean square errors in the test set where 5-10 times reduction is observed. The effectiveness of the TCAD AWNN model can be especially effective in the case of sampling over a vast sample space since the under-sampling problem can be compensated by the TCAD model. The large and nearly unbounded sample space is very common in IC technology, where cascaded and repeated process steps exist (~150 process steps and ~20 masks for 90-nm CMOS process).",
        "abstract": "Using machine intelligence on device and process performance prediction is an emerging methodology in the IC industry. While semiconductor technology computer-aided design (TCAD) has been researched and developed for over 30 years, it should contribute to or be used in conjunction with machine learning algorithms in solution finding procedure. Here, we propose an adaptive weighting neural network (AWNN) model that combines the advantages of statistical the machine learning model and the physical TCAD model. Using aspect ratio dependent etching as an example, our proposed AWNN outperforms conventional artificial neural network in terms of mean square errors in the test set where 5-10 times reduction is observed. The effectiveness of the TCAD AWNN model can be especially effective in the case of sampling over a vast sample space since the under-sampling problem can be compensated by the TCAD model. The large and nearly unbounded sample space is very common in IC technology, where cascaded and repeated process steps exist (~150 process steps and ~20 masks for 90-nm CMOS process).",
        "authors": "Albert S. Lin, Chandni Akbar, Chien Y. Huang, Chun H. Chen, Parag Parashar, Sze M. Fu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885024",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper presents a novel robot vision architecture for perceiving generic 3-D clothes configurations. Our architecture is hierarchically structured, starting from low-level curvature features to mid-level geometric shapes and topology descriptions, and finally, high-level semantic surface descriptions. We demonstrate our robot vision architecture in a customized dual-arm industrial robot with our inhouse developed stereo vision system, carrying out autonomous grasping and dual-arm flattening. The experimental results show the effectiveness of the proposed dual-arm flattening using the stereo vision system compared with the single-arm flattening using the widely cited Kinect-like sensor as the baseline. In addition, the proposed grasping approach achieves satisfactory performance when grasping various kind of garments, verifying the capability of the proposed visual perception architecture to be adapted to more than one clothing manipulation tasks.",
        "abstract": "This paper presents a novel robot vision architecture for perceiving generic 3-D clothes configurations. Our architecture is hierarchically structured, starting from low-level curvature features to mid-level geometric shapes and topology descriptions, and finally, high-level semantic surface descriptions. We demonstrate our robot vision architecture in a customized dual-arm industrial robot with our inhouse developed stereo vision system, carrying out autonomous grasping and dual-arm flattening. The experimental results show the effectiveness of the proposed dual-arm flattening using the stereo vision system compared with the single-arm flattening using the widely cited Kinect-like sensor as the baseline. In addition, the proposed grasping approach achieves satisfactory performance when grasping various kind of garments, verifying the capability of the proposed visual perception architecture to be adapted to more than one clothing manipulation tasks.",
        "authors": "Gerardo Aragon-Camarasa, Jan Paul Siebert, Li Sun, Simon Rogers",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883072",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A genetic algorithm (GA) combines the restriction enzyme mining core of single nucleotide polymorphism (SNP) restriction fragment length polymorphism (RFLP) to design polymerase chain reaction (PCR)-RFLP primer pairs for SNP-based genotyping with feasible estimated GA parameters. However, this GA method is easily trapped into local optima. An improved design of PCR-RFLP assay primers for SNP genotyping is needed. A memetic algorithm (MA) was used to design more robust primers for the PCR-RFLP assay to enable SNP genotyping. The novel restriction enzymes hunting (REHUNT) package was embedded into the MA method to provide available restriction enzymes. A formula to calculate more accurate thermodynamic primer melting temperatures was also introduced. Using the criteria of the GA method, in silico simulations for the MA method under different parameter settings were performed with the SNPs of SLC6A4, and results were compared. Appropriate MA parameter settings were superior in providing robust PCR-RFLP primers to achieve SNP genotyping compared with the GA method. Improvements included an accurate thermodynamic SantaLucia’s formula for the calculation of melting temperature, use of the novel REHUNT for restriction enzymes mining, and selection of primers that better conformed to the primer constraints. The appropriate parameter settings for the proposed MA method were identified and carefully evaluated to design robust PCR-RFLP primers for SNP genotyping. Compared with the former GA method, the MA method is more feasible for PCR-RFLP SNP genotyping.",
        "abstract": "A genetic algorithm (GA) combines the restriction enzyme mining core of single nucleotide polymorphism (SNP) restriction fragment length polymorphism (RFLP) to design polymerase chain reaction (PCR)-RFLP primer pairs for SNP-based genotyping with feasible estimated GA parameters. However, this GA method is easily trapped into local optima. An improved design of PCR-RFLP assay primers for SNP genotyping is needed. A memetic algorithm (MA) was used to design more robust primers for the PCR-RFLP assay to enable SNP genotyping. The novel restriction enzymes hunting (REHUNT) package was embedded into the MA method to provide available restriction enzymes. A formula to calculate more accurate thermodynamic primer melting temperatures was also introduced. Using the criteria of the GA method, in silico simulations for the MA method under different parameter settings were performed with the SNPs of SLC6A4, and results were compared. Appropriate MA parameter settings were superior in providing robust PCR-RFLP primers to achieve SNP genotyping compared with the GA method. Improvements included an accurate thermodynamic SantaLucia’s formula for the calculation of melting temperature, use of the novel REHUNT for restriction enzymes mining, and selection of primers that better conformed to the primer constraints. The appropriate parameter settings for the proposed MA method were identified and carefully evaluated to design robust PCR-RFLP primers for SNP genotyping. Compared with the former GA method, the MA method is more feasible for PCR-RFLP SNP genotyping.",
        "authors": "Che-Nan Kuo, Ching-Ming Lai, Jiashen Teh, Yu-Huei Cheng",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884189",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, the problem of estimating the spatial parameters of multiple coherently distributed linear chirp sources is considered. In the fractional Fourier domain, a novel low-complexity algorithm based on the MUSIC criterion is proposed to estimate the central angle and the extension width. For small extension width scenarios, the 2-D spatial angle search in traditional estimators for the distributed sources estimation is replaced by a two-step 1-D MUSIC search which dramatically decreases the computational complexity. In the case of chirp signals with different frequency parameters, sources can be separated, and the spatial parameters of each source can be estimated individually. When sources cannot be separated, this problem can be considered as a classical direction-of-arrival estimation of multiple signals. In addition, the central angle estimation does not require any assumption on the distribution of angular spread. Simulation results show that the proposed algorithm obtains comparable performance in the parameters estimation accuracy for small extension width scenarios with the reduced complexity compared with our previous work.",
        "abstract": "In this paper, the problem of estimating the spatial parameters of multiple coherently distributed linear chirp sources is considered. In the fractional Fourier domain, a novel low-complexity algorithm based on the MUSIC criterion is proposed to estimate the central angle and the extension width. For small extension width scenarios, the 2-D spatial angle search in traditional estimators for the distributed sources estimation is replaced by a two-step 1-D MUSIC search which dramatically decreases the computational complexity. In the case of chirp signals with different frequency parameters, sources can be separated, and the spatial parameters of each source can be estimated individually. When sources cannot be separated, this problem can be considered as a classical direction-of-arrival estimation of multiple signals. In addition, the central angle estimation does not require any assumption on the distribution of angular spread. Simulation results show that the proposed algorithm obtains comparable performance in the parameters estimation accuracy for small extension width scenarios with the reduced complexity compared with our previous work.",
        "authors": "Jiexiao Yu, Kaihua Liu, Liang Zhang, Yanbei Liu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883530",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The lack of good and up-to-date lab experiments form a major impediment in the domain of engineering education. Often, the lab experiments are outdated. The Virtual Labs project addresses the issue of lack of good lab facilities, as well as trained teachers, by making remote experimentation possible. The pedagogy is student-centric. The Virtual Labs project has also developed a novel methodology for field trials, outreach, and quality control. Virtual Labs also provide tremendous cost advantage. The Virtual Labs project is a wonderful example of an open educational resource developed by a multiinstitution multidiscipline project team. Over 100000 students are currently using the online labs under the Virtual Labs project. Many of these labs are being accessed outside the regular lab hours.",
        "abstract": "The lack of good and up-to-date lab experiments form a major impediment in the domain of engineering education. Often, the lab experiments are outdated. The Virtual Labs project addresses the issue of lack of good lab facilities, as well as trained teachers, by making remote experimentation possible. The pedagogy is student-centric. The Virtual Labs project has also developed a novel methodology for field trials, outreach, and quality control. Virtual Labs also provide tremendous cost advantage. The Virtual Labs project is a wonderful example of an open educational resource developed by a multiinstitution multidiscipline project team. Over 100000 students are currently using the online labs under the Virtual Labs project. Many of these labs are being accessed outside the regular lab hours.",
        "authors": "Ranjan Bose",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2286202",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The multilevel generalized assignment problem (MGAP) consists of minimizing the assignment cost of a set of jobs to machines, each having associated therewith a capacity constraint. Each machine can perform a job with different efficiency levels that entail different costs and amount of resources required. The MGAP was introduced in the context of large manufacturing systems as a more general variant of the well-known generalized assignment problem, where a single efficiency level is associated with each machine. In this paper, we propose a branch-and-cut algorithm whose core is an exact separation procedure for the multiple-choice knapsack polytope induced by the capacity constraints and single-level execution constraints. A computational experience on a set of benchmark instances is reported, showing the effectiveness of the proposed approach.",
        "abstract": "The multilevel generalized assignment problem (MGAP) consists of minimizing the assignment cost of a set of jobs to machines, each having associated therewith a capacity constraint. Each machine can perform a job with different efficiency levels that entail different costs and amount of resources required. The MGAP was introduced in the context of large manufacturing systems as a more general variant of the well-known generalized assignment problem, where a single efficiency level is associated with each machine. In this paper, we propose a branch-and-cut algorithm whose core is an exact separation procedure for the multiple-choice knapsack polytope induced by the capacity constraints and single-level execution constraints. A computational experience on a set of benchmark instances is reported, showing the effectiveness of the proposed approach.",
        "authors": "Igor Vasilyev, Maurizio Boccia, Pasquale Avella",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2273268",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "We consider the problem of adaptively designing compressive measurement matrices for estimating time-varying sparse signals. We formulate this problem as a partially observable Markov decision process. This formulation allows us to use Bellman's principle of optimality in the implementation of multi-step lookahead designs of compressive measurements. We compare the performance of adaptive versus traditional non-adaptive designs and study the value of multi-step (non-myopic) versus one-step (myopic) lookahead adaptive schemes by introducing two variations of the compressive measurement design problem. In the first variation, we consider the problem of sequentially selecting measurement matrices with fixed dimensions from a prespecified library of measurement matrices. In the second variation, the number of compressive measurements, i.e., the number of rows of the measurement matrix, is adaptively chosen. Once the number of measurements is determined, the matrix entries are chosen according to a prespecified adaptive scheme. Each of these two problems is judged by a separate performance criterion. The gauge of efficiency in the first problem is the conditional mutual information between the sparse signal support and measurements. The second problem applies a linear combination of the number of measurements and conditional mutual information as the performance measure. Through several simulations, we study the effectiveness of different designs in various settings. The primary focus in these simulations is the application of a method known as rollout. However, the computational load required for using the rollout method has also inspired us to adapt two data association heuristics to the compressive sensing paradigm. These heuristics show promising decreases in the amount of computation for propagating distributions and searching for optimal solutions.",
        "abstract": "We consider the problem of adaptively designing compressive measurement matrices for estimating time-varying sparse signals. We formulate this problem as a partially observable Markov decision process. This formulation allows us to use Bellman's principle of optimality in the implementation of multi-step lookahead designs of compressive measurements. We compare the performance of adaptive versus traditional non-adaptive designs and study the value of multi-step (non-myopic) versus one-step (myopic) lookahead adaptive schemes by introducing two variations of the compressive measurement design problem. In the first variation, we consider the problem of sequentially selecting measurement matrices with fixed dimensions from a prespecified library of measurement matrices. In the second variation, the number of compressive measurements, i.e., the number of rows of the measurement matrix, is adaptively chosen. Once the number of measurements is determined, the matrix entries are chosen according to a prespecified adaptive scheme. Each of these two problems is judged by a separate performance criterion. The gauge of efficiency in the first problem is the conditional mutual information between the sparse signal support and measurements. The second problem applies a linear combination of the number of measurements and conditional mutual information as the performance measure. Through several simulations, we study the effectiveness of different designs in various settings. The primary focus in these simulations is the application of a method known as rollout. However, the computational load required for using the rollout method has also inspired us to adapt two data association heuristics to the compressive sensing paradigm. These heuristics show promising decreases in the amount of computation for propagating distributions and searching for optimal solutions.",
        "authors": "Ali Pezeshki, Edwin K. P. Chong, Lucas W. Krakow, Ramin Zahedi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2272664",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Frequent structures have emerged as resolving the structural pattern mining issues, such as chemistry, Web applications, and other related problems. Top-k subgraph query as an important technology of graph search is widely used in emerging fields. Different from the traditional subgraph query, the query requirement studied in this paper has two unique properties: 1) data graphs change dynamically over time, including vertices/edges insert, delete, and frequent labels update and 2) query with limits of labels and k. Existing graph index and query techniques rarely considered these. Therefore, this paper proposes a method called frequent subgraph dynamic Top-k query with label value constraint to address both the challenges. We propose a two-level index with the frequent structure mapping and label value aggregation, which locates the query structure quickly, then prunes, and filters according to the constraint to narrow search range and improve query efficiency. The method uses a two-level index to filter the initial result and combines the dynamic changes of graphs to modify query results. In addition, it also proposed an incremental dynamic maintenance strategy over the proposed index, which only updated the partial contents to avoid the high cost caused by global update. The experimental results demonstrate that the proposed Top-k query method outperforms the baseline approaches up to 37%.",
        "abstract": "Frequent structures have emerged as resolving the structural pattern mining issues, such as chemistry, Web applications, and other related problems. Top-k subgraph query as an important technology of graph search is widely used in emerging fields. Different from the traditional subgraph query, the query requirement studied in this paper has two unique properties: 1) data graphs change dynamically over time, including vertices/edges insert, delete, and frequent labels update and 2) query with limits of labels and k. Existing graph index and query techniques rarely considered these. Therefore, this paper proposes a method called frequent subgraph dynamic Top-k query with label value constraint to address both the challenges. We propose a two-level index with the frequent structure mapping and label value aggregation, which locates the query structure quickly, then prunes, and filters according to the constraint to narrow search range and improve query efficiency. The method uses a two-level index to filter the initial result and combines the dynamic changes of graphs to modify query results. In addition, it also proposed an incremental dynamic maintenance strategy over the proposed index, which only updated the partial contents to avoid the high cost caused by global update. The experimental results demonstrate that the proposed Top-k query method outperforms the baseline approaches up to 37%.",
        "authors": "Baoyan Song, Guangxiang Wang, Linlin Ding, Xiaohuan Shan, Yan Xu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885038",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "As the core part of the new generation of information technology, the Internet of Things has accumulated a large number of real-time data streams of various types and structures. The data stream is generated at an extremely fast speed, and its content and distribution characteristics are all in high-speed dynamic changes, which must be processed in real time. Therefore, the feature learning algorithm is required to support incremental updates and learn the characteristics of high-speed dynamic change data in real time. Most of the current machine learning models for processing big data belong to the static learning model. The batch learning method makes it impossible to analyze data streams in real time, and the learning ability of dynamic data streams is poor. Therefore, this paper proposes an incremental high-order deep learning model to extend the data from the vector space to the tensor space and update the parameters and structure of the network model in the high-order tensor space. In the process of parameter updating, the firstorder approximation concept is introduced to avoid incrementing parameters by the iterative method and to improve the parameter update efficiency, so that the updated model can quickly learn the characteristics of dynamically changing big data and satisfy the real-time requirements of big data feature learning while maintaining the original knowledge of the neural network model as much as possible. To evaluate the performance of the proposed model, experiments were performed on real image data sets-MNIST, and the model was evaluated for stability, plasticity, and run time. The experimental results show that the model not only has the ability to incrementally learn the characteristics of new data online but also retains the ability to learn the original data features, improve the model update efficiency, and maximize the online analysis and real-time processing of dynamic data streams.",
        "abstract": "As the core part of the new generation of information technology, the Internet of Things has accumulated a large number of real-time data streams of various types and structures. The data stream is generated at an extremely fast speed, and its content and distribution characteristics are all in high-speed dynamic changes, which must be processed in real time. Therefore, the feature learning algorithm is required to support incremental updates and learn the characteristics of high-speed dynamic change data in real time. Most of the current machine learning models for processing big data belong to the static learning model. The batch learning method makes it impossible to analyze data streams in real time, and the learning ability of dynamic data streams is poor. Therefore, this paper proposes an incremental high-order deep learning model to extend the data from the vector space to the tensor space and update the parameters and structure of the network model in the high-order tensor space. In the process of parameter updating, the firstorder approximation concept is introduced to avoid incrementing parameters by the iterative method and to improve the parameter update efficiency, so that the updated model can quickly learn the characteristics of dynamically changing big data and satisfy the real-time requirements of big data feature learning while maintaining the original knowledge of the neural network model as much as possible. To evaluate the performance of the proposed model, experiments were performed on real image data sets-MNIST, and the model was evaluated for stability, plasticity, and run time. The experimental results show that the model not only has the ability to incrementally learn the characteristics of new data online but also retains the ability to learn the original data features, improve the model update efficiency, and maximize the online analysis and real-time processing of dynamic data streams.",
        "authors": "Min Zhang, Wei Wang, Yuliang Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883666",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Distributed optical fiber sensors have gained an increasingly prominent role in structural-health monitoring. These are composed of an optical fiber cable in which a light impulse is launched by an opto-electronic device. The scattered light is of interest in the spectral domain: the spontaneous Brillouin spectrum is centered on the Brillouin frequency, which is related to the local strain and temperature changes in the optical fiber. When coupled with an industrial Brillouin optical time-domain analyzer (B-OTDA), an optical fiber cable can provide distributed measurements of strain and/or temperature, with a spatial resolution over kilometers of 40 cm. This paper focuses on the functioning of a B-OTDA device, where we address the problem of the improvement of spatial resolution. We model a Brillouin spectrum measured within an integration base of 1 m as the superposition of the elementary spectra contained in the base. Then, the spectral distortion phenomenon can be mathematically explained: if the strain is not constant within the integration base, the Brillouin spectrum is composed of several elementary spectra that are centered on different local Brillouin frequencies. We propose a source separation methodology approach to decompose a measured Brillouin spectrum into its spectral components. The local Brillouin frequencies and amplitudes are related to a portion of the integration base where the strain is constant. A layout algorithm allows the estimation of a strain profile with new spatial resolution chosen by the user. Numerical tests enable the finding of the optimal parameters, which provides a reduction to 1 cm of the 40-cm spatial resolution of the B-OTDA device. These parameters are highlighted during a comparison with a reference strain profile acquired by a 5-cm-resolution Rayleigh scatter analyzer under controlled conditions. In comparison with the B-OTDA strain profile, our estimated strain profile has better accuracy, with centimeter spatial resolut...",
        "abstract": "Distributed optical fiber sensors have gained an increasingly prominent role in structural-health monitoring. These are composed of an optical fiber cable in which a light impulse is launched by an opto-electronic device. The scattered light is of interest in the spectral domain: the spontaneous Brillouin spectrum is centered on the Brillouin frequency, which is related to the local strain and temperature changes in the optical fiber. When coupled with an industrial Brillouin optical time-domain analyzer (B-OTDA), an optical fiber cable can provide distributed measurements of strain and/or temperature, with a spatial resolution over kilometers of 40 cm. This paper focuses on the functioning of a B-OTDA device, where we address the problem of the improvement of spatial resolution. We model a Brillouin spectrum measured within an integration base of 1 m as the superposition of the elementary spectra contained in the base. Then, the spectral distortion phenomenon can be mathematically explained: if the strain is not constant within the integration base, the Brillouin spectrum is composed of several elementary spectra that are centered on different local Brillouin frequencies. We propose a source separation methodology approach to decompose a measured Brillouin spectrum into its spectral components. The local Brillouin frequencies and amplitudes are related to a portion of the integration base where the strain is constant. A layout algorithm allows the estimation of a strain profile with new spatial resolution chosen by the user. Numerical tests enable the finding of the optimal parameters, which provides a reduction to 1 cm of the 40-cm spatial resolution of the B-OTDA device. These parameters are highlighted during a comparison with a reference strain profile acquired by a 5-cm-resolution Rayleigh scatter analyzer under controlled conditions. In comparison with the B-OTDA strain profile, our estimated strain profile has better accuracy, with centimeter spatial resolut...",
        "authors": "Alexandre Girard, Edouard Buchoud, Guy D'Urso, Jérôme. I. Mars, Sylvain Blairon, Valeriu D. Vrabie",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2288113",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Fractional order controllers have a growing popularity in last years and they give more flexibility to researchers for designing a controller. In this paper, a fractional order PID controller is designed for the position control of a two wheeled inverted pendulum. The pendulum is modeled with DC electrical motors to obtain a more realistic model. Integer order PID controller is also designed to make a comparison with fractional order controller and all controllers are optimized by swarm algorithms to be sure obtained the best performance for each controller. A fractional order PID controller has two extra parameters (λ and μ) and totally five parameters to be optimized. Optimization algorithms are powerful tools for designing a controller, and guarantee finding an optimum result. However, each optimization algorithm has a different performance not only because of the structure of the algorithm but also depending on the optimization problem. Because of this, four popular optimization algorithms (artificial bee colony, particle swarm optimization, grey wolf optimizer, and cuckoo search algorithm) are used to tune controller parameters, and compared regard with the optimized system performance. The results show that the best performance is obtained by the fractional order PID controller, which optimized by artificial bee colony algorithm. The fractional order PID controllers have also better performance than integer order PIDs when used the same optimization algorithm for tuning.",
        "abstract": "Fractional order controllers have a growing popularity in last years and they give more flexibility to researchers for designing a controller. In this paper, a fractional order PID controller is designed for the position control of a two wheeled inverted pendulum. The pendulum is modeled with DC electrical motors to obtain a more realistic model. Integer order PID controller is also designed to make a comparison with fractional order controller and all controllers are optimized by swarm algorithms to be sure obtained the best performance for each controller. A fractional order PID controller has two extra parameters (λ and μ) and totally five parameters to be optimized. Optimization algorithms are powerful tools for designing a controller, and guarantee finding an optimum result. However, each optimization algorithm has a different performance not only because of the structure of the algorithm but also depending on the optimization problem. Because of this, four popular optimization algorithms (artificial bee colony, particle swarm optimization, grey wolf optimizer, and cuckoo search algorithm) are used to tune controller parameters, and compared regard with the optimized system performance. The results show that the best performance is obtained by the fractional order PID controller, which optimized by artificial bee colony algorithm. The fractional order PID controllers have also better performance than integer order PIDs when used the same optimization algorithm for tuning.",
        "authors": "Hüseyin Oktay Erkol",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883504",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, we investigate a phase cancellation problem that occurs in tag-to-tag communication systems. These are the systems wherein several tags communicate with each other by backscattering an external constant wave signal. A transmitting tag modulates baseband information into the reflected signal using backscatter modulation. At the receiving tag, the received signal is the superposition of the transmitted signals by the carrier emitter and the transmitting tag. The resulting signal is demodulated using the envelope detector that causes the phase cancellation problem. This problem exists in all tag-to-tag communication systems and largely impacts on the reliability and communication range. We theoretically analyze and experimentally demonstrate this problem for the tags that use amplitude-shift keying-based backscattering. We propose an algorithm for the mitigation of the phase cancellation based on the phase rotation control. We examine the performance of the proposed algorithm through theoretical analysis and computer simulations. We confirm that the proposed algorithm can significantly mitigate the phase cancellation.",
        "abstract": "In this paper, we investigate a phase cancellation problem that occurs in tag-to-tag communication systems. These are the systems wherein several tags communicate with each other by backscattering an external constant wave signal. A transmitting tag modulates baseband information into the reflected signal using backscatter modulation. At the receiving tag, the received signal is the superposition of the transmitted signals by the carrier emitter and the transmitting tag. The resulting signal is demodulated using the envelope detector that causes the phase cancellation problem. This problem exists in all tag-to-tag communication systems and largely impacts on the reliability and communication range. We theoretically analyze and experimentally demonstrate this problem for the tags that use amplitude-shift keying-based backscattering. We propose an algorithm for the mitigation of the phase cancellation based on the phase rotation control. We examine the performance of the proposed algorithm through theoretical analysis and computer simulations. We confirm that the proposed algorithm can significantly mitigate the phase cancellation.",
        "authors": "Dong In Kim, Isaac Sim, Jin Young Kim, Yoan Shin, Young Ghyu Sun, Yu Min Hwang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883238",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Business processes (BPs) are often modeled to elaborate process-related business requirements (BRs). This leads to verify the complex BRs in early automation stages. Among various BP languages, event-driven process chain (EPC) is a well-known semi-formal modeling language, which is verifiable after transforming it into any other formal language, such as, timed automata or Petri nets. However, full potential of EPC cannot be exploited as yet because existing EPC tools can only model or verify the simple patterns and they lack the modeling/verification of complex patterns. Moreover, only the proprietary tools are available, which limit its applicability toward overwhelming utilization amongst widespread practitioners and research community. This research work is the first attempt to make EPC more expressive in terms of modeling complex patterns for real time systems. Particularly, the UMLPACE (Unified Modeling Language Profile for Atomic and Complex events in EPC) has been developed, which adapts the concepts of UML activity diagram for representing both simple as well as complex patterns in EPC. As a part of research, a complete open source transformation engine is developed to transform UMLPACE source models into timed automata target models for the verification of complex BPs. The implementation of transformation engine is carried out in JAVA language and Acceleo tool through model-to-text transformation approach. Finally, the broader applications of UMLPACE are demonstrated through two benchmark case studies.",
        "abstract": "Business processes (BPs) are often modeled to elaborate process-related business requirements (BRs). This leads to verify the complex BRs in early automation stages. Among various BP languages, event-driven process chain (EPC) is a well-known semi-formal modeling language, which is verifiable after transforming it into any other formal language, such as, timed automata or Petri nets. However, full potential of EPC cannot be exploited as yet because existing EPC tools can only model or verify the simple patterns and they lack the modeling/verification of complex patterns. Moreover, only the proprietary tools are available, which limit its applicability toward overwhelming utilization amongst widespread practitioners and research community. This research work is the first attempt to make EPC more expressive in terms of modeling complex patterns for real time systems. Particularly, the UMLPACE (Unified Modeling Language Profile for Atomic and Complex events in EPC) has been developed, which adapts the concepts of UML activity diagram for representing both simple as well as complex patterns in EPC. As a part of research, a complete open source transformation engine is developed to transform UMLPACE source models into timed automata target models for the verification of complex BPs. The implementation of transformation engine is carried out in JAVA language and Acceleo tool through model-to-text transformation approach. Finally, the broader applications of UMLPACE are demonstrated through two benchmark case studies.",
        "authors": "Aamir Naeem, Anam Amjad, Farooque Azam, Muhammad Rashid, Muhammad Waseem Anwar, Wasi Haider Butt",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883610",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Decades of heavy investment in laboratory-based brain imaging and neuroscience have led to foundational insights into how humans sense, perceive, and interact with the external world. However, it is argued that fundamental differences between laboratory-based and naturalistic human behavior may exist. Thus, it remains unclear how well the current knowledge of human brain function translates into the highly dynamic real world. While some demonstrated successes in real-world neurotechnologies are observed, particularly in the area of brain-computer interaction technologies, innovations and developments to date are limited to a small science and technology community. We posit that advancements in real-world neuroimaging tools for use by a broad-based workforce will dramatically enhance neurotechnology applications that have the potential to radically alter human–system interactions across all aspects of everyday life. We discuss the efforts of a joint government-academic-industry team to take an integrative, interdisciplinary, and multi-aspect approach to translate current technologies into devices that are truly fieldable across a range of environments. Results from initial work, described here, show promise for dramatic advances in the field that will rapidly enhance our ability to assess brain activity in real-world scenarios.",
        "abstract": "Decades of heavy investment in laboratory-based brain imaging and neuroscience have led to foundational insights into how humans sense, perceive, and interact with the external world. However, it is argued that fundamental differences between laboratory-based and naturalistic human behavior may exist. Thus, it remains unclear how well the current knowledge of human brain function translates into the highly dynamic real world. While some demonstrated successes in real-world neurotechnologies are observed, particularly in the area of brain-computer interaction technologies, innovations and developments to date are limited to a small science and technology community. We posit that advancements in real-world neuroimaging tools for use by a broad-based workforce will dramatically enhance neurotechnology applications that have the potential to radically alter human–system interactions across all aspects of everyday life. We discuss the efforts of a joint government-academic-industry team to take an integrative, interdisciplinary, and multi-aspect approach to translate current technologies into devices that are truly fieldable across a range of environments. Results from initial work, described here, show promise for dramatic advances in the field that will rapidly enhance our ability to assess brain activity in real-world scenarios.",
        "authors": "Chin-Teng Lin, Kaleb Mcdowell, Keith W. Whitaker, Kelvin S. Oie, Stephen Gordon, Tzyy-Ping Jung",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2260791",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Securing medical images are a very essential process in medical image authentication. Medical image watermarking is a very popular tool to achieve this goal. In this paper, an extremely fast, highly accurate, and robust algorithm is proposed for watermarking both gray-level and color medical images. In the proposed method, a scrambled binary watermark is embedded in the host medical image. Simplified exact kernels are used to compute the moments of the polar complex exponential transform (PCET) for the host gray-level images and the moments of the quaternion PCET for the host color images without approximation errors. The stability of the computed moments enables us to use higher order moments in a perfect reconstruction of the watermarked medical images. The accurate moment invariant to rotation, scaling, and translation ensures the robustness of the proposed watermarking algorithm against geometric attacks. Performed experiments clearly show very high visual imperceptibility and robustness to different levels of geometric distortions and common signal processing attacks. The implementation of parallel multi-core CPU and GPU result in a tremendous reduction of the overall watermarking times. For a color image of size 256 × 256, the watermarking time is accelerated by 20× and 11× using a GPU and a CPU with 16 cores, respectively.",
        "abstract": "Securing medical images are a very essential process in medical image authentication. Medical image watermarking is a very popular tool to achieve this goal. In this paper, an extremely fast, highly accurate, and robust algorithm is proposed for watermarking both gray-level and color medical images. In the proposed method, a scrambled binary watermark is embedded in the host medical image. Simplified exact kernels are used to compute the moments of the polar complex exponential transform (PCET) for the host gray-level images and the moments of the quaternion PCET for the host color images without approximation errors. The stability of the computed moments enables us to use higher order moments in a perfect reconstruction of the watermarked medical images. The accurate moment invariant to rotation, scaling, and translation ensures the robustness of the proposed watermarking algorithm against geometric attacks. Performed experiments clearly show very high visual imperceptibility and robustness to different levels of geometric distortions and common signal processing attacks. The implementation of parallel multi-core CPU and GPU result in a tremendous reduction of the overall watermarking times. For a color image of size 256 × 256, the watermarking time is accelerated by 20× and 11× using a GPU and a CPU with 16 cores, respectively.",
        "authors": "Ahmad Salah, Kenli Li, Khalid M. Hosny, Mohamed M. Darwish",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2879919",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Non-rigid point set registration is a fundamental problem in many fields related to computer vision, medical image processing, and pattern recognition. In this paper, we develop a new point set registration method by using an adaptive weighted objective function, which formulates the alignment of two point sets as a mixture model estimation problem. The correspondences and the transformation are jointly recovered by using the expectation-maximization algorithm to obtain the promising results. First, the correspondences are established using local feature descriptors, and the adaptation parameters for the mixture model are computed from these correspondences. Then, the underlying transformation is recovered by minimizing the adaptive weighted objective function deduced from the mixture model. We demonstrate the advantages of the proposed method on various types of synthetic and real data and compare the results against those obtained using the state-of-the-art methods. The experimental results show that the proposed method is robust and outperforms the other registration approaches.",
        "abstract": "Non-rigid point set registration is a fundamental problem in many fields related to computer vision, medical image processing, and pattern recognition. In this paper, we develop a new point set registration method by using an adaptive weighted objective function, which formulates the alignment of two point sets as a mixture model estimation problem. The correspondences and the transformation are jointly recovered by using the expectation-maximization algorithm to obtain the promising results. First, the correspondences are established using local feature descriptors, and the adaptation parameters for the mixture model are computed from these correspondences. Then, the underlying transformation is recovered by minimizing the adaptive weighted objective function deduced from the mixture model. We demonstrate the advantages of the proposed method on various types of synthetic and real data and compare the results against those obtained using the state-of-the-art methods. The experimental results show that the proposed method is robust and outperforms the other registration approaches.",
        "authors": "Changcai Yang, Lifang Wei, Taotao Lai, Xingyu Jiang, Yizhang Liu, Zejun Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883689",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In software development, stakeholders of the same project often collaborate asynchronously through shared artifacts. A traceability system links a project's artifacts and therefore provides support for collaboration among stakeholders. Different stakeholders are interested in different types of traceability links. The literature often states that traceability is useful but expensive to build and maintain. However, there is no study showing reduction in effort when traceability links among various software artifacts are provided and used during the maintenance phase. This paper presents a study evaluating the benefits of using traceability among requirements, design, code, code inspections, builds, defects, and tests artifacts in the maintenance phase. Before the study, a survey was conducted at a large industrial firm to determine the type of links that different stakeholders are interested in. Twenty-five stakeholders from this firm participated in a survey to define the type of traceability links that were of interest to them. With this result, a traceability link model is proposed that categorizes different types of traceability links based on stakeholders' roles. This link model was used in the study. Twenty-eight subjects from industry and academia participated in the empirical study that was conducted after the survey. A prototype link-tracing tool, TraceLink, was developed and used in the study to present traceability links to the experimental group, whereas the control group was not given any links to solve the tasks. Five maintenance tasks were used in the study. The results show a significant improvement in task accuracy (86.06%) when traceability links were given to the subjects. In conclusion, a traceability model based on an industrial survey provided traceability views that are based on stakeholders' roles. This empirical study provides evidence that traceability links are effective in solving maintenance tasks with higher accuracy.",
        "abstract": "In software development, stakeholders of the same project often collaborate asynchronously through shared artifacts. A traceability system links a project's artifacts and therefore provides support for collaboration among stakeholders. Different stakeholders are interested in different types of traceability links. The literature often states that traceability is useful but expensive to build and maintain. However, there is no study showing reduction in effort when traceability links among various software artifacts are provided and used during the maintenance phase. This paper presents a study evaluating the benefits of using traceability among requirements, design, code, code inspections, builds, defects, and tests artifacts in the maintenance phase. Before the study, a survey was conducted at a large industrial firm to determine the type of links that different stakeholders are interested in. Twenty-five stakeholders from this firm participated in a survey to define the type of traceability links that were of interest to them. With this result, a traceability link model is proposed that categorizes different types of traceability links based on stakeholders' roles. This link model was used in the study. Twenty-eight subjects from industry and academia participated in the empirical study that was conducted after the survey. A prototype link-tracing tool, TraceLink, was developed and used in the study to present traceability links to the experimental group, whereas the control group was not given any links to solve the tasks. Five maintenance tasks were used in the study. The results show a significant improvement in task accuracy (86.06%) when traceability links were given to the subjects. In conclusion, a traceability model based on an industrial survey provided traceability views that are based on stakeholders' roles. This empirical study provides evidence that traceability links are effective in solving maintenance tasks with higher accuracy.",
        "authors": "Bonita Sharif, Chang Liu, Khaled Jaber",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2286822",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Flywheel bearing is a key mechanical part of a satellite. Its health plays an important role in the fatigue life of the satellite. However, it is rather difficult to diagnose the health state of the bearings due to the complex satellite system. This paper attempts to propose a three-direction correlation dimension method to diagnose the bearings of a satellite flywheel at three typical states based on K-medoids clustering technology. A set of spatial spheres representing different bearings are modeled to recognize these three states of the bearings. To avoid misdiagnosis or loss of the bearing state, a twice-cluster scheme is employed. A series of tests is carried out to observe the effectiveness of the proposed method. The result shows that the proposed method is capable of diagnosing the different states of the bearings and its accuracy is higher than 99% at given conditions.",
        "abstract": "Flywheel bearing is a key mechanical part of a satellite. Its health plays an important role in the fatigue life of the satellite. However, it is rather difficult to diagnose the health state of the bearings due to the complex satellite system. This paper attempts to propose a three-direction correlation dimension method to diagnose the bearings of a satellite flywheel at three typical states based on K-medoids clustering technology. A set of spatial spheres representing different bearings are modeled to recognize these three states of the bearings. To avoid misdiagnosis or loss of the bearing state, a twice-cluster scheme is employed. A series of tests is carried out to observe the effectiveness of the proposed method. The result shows that the proposed method is capable of diagnosing the different states of the bearings and its accuracy is higher than 99% at given conditions.",
        "authors": "Changrui Chen, Dengyun Wu, Hong Wang, Qiang Pan, Tian He, Xiaofeng Liu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885046",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The recognition, counting, and sorting of mussels in marine cultures for seed production are currently performed by visual examination experts (i.e., entirely dependent on human resources). In this paper, we present the development of an automatic mussel classifier system based on the morphological characteristics for the simultaneous recognition and sorting of five mussel species. The proposed system provides rich statistical information needed for tracking the long-term evolution of culture parameters. In our experimental demonstration, we have achieved a recognition rate of 95% in most of the test probes for the five studied mussel species. A single sample of dozens of specimens can be classified within seconds with real-time capability when the vision interface is not used. Finally, the system has the potential to be extended for the automatic classification of mussels worldwide.",
        "abstract": "The recognition, counting, and sorting of mussels in marine cultures for seed production are currently performed by visual examination experts (i.e., entirely dependent on human resources). In this paper, we present the development of an automatic mussel classifier system based on the morphological characteristics for the simultaneous recognition and sorting of five mussel species. The proposed system provides rich statistical information needed for tracking the long-term evolution of culture parameters. In our experimental demonstration, we have achieved a recognition rate of 95% in most of the test probes for the five studied mussel species. A single sample of dozens of specimens can be classified within seconds with real-time capability when the vision interface is not used. Finally, the system has the potential to be extended for the automatic classification of mussels worldwide.",
        "authors": "Carlos E. Saavedra-Rubilar, Eduardo Tarifeño, Juan P. Staforelli, MARIA J. Gallardo-Nelson, Pablo A. Coelho-Caro, Victor Guaquin",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884394",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Reversible data hiding is an important topic of data hiding. This paper proposes a novel separable and error-free reversible data hiding in an encrypted image based on two-layer pixel errors. Specifically, the proposed scheme divides the original image into a series of non-overlapped blocks and permutes these blocks. Then, a closed Hilbert curve is used for scanning each block to obtain a one-dimensional pixel sequence. The pixels of the sequence are encrypted with key transmission. During data hiding, each non-overlapped block of the encrypted image is scanned in the closed Hilbert order to generate a one-dimensional encrypted pixel sequence. Finally, it exploits the histogram of two-layer adjacent encrypted pixel errors to embed secret data by histogram shifting and generate a marked encrypted image. Many experiments are carried out, and the results demonstrate that the proposed scheme reaches a high payload and outperforms some reversible data hiding schemes in the encrypted image.",
        "abstract": "Reversible data hiding is an important topic of data hiding. This paper proposes a novel separable and error-free reversible data hiding in an encrypted image based on two-layer pixel errors. Specifically, the proposed scheme divides the original image into a series of non-overlapped blocks and permutes these blocks. Then, a closed Hilbert curve is used for scanning each block to obtain a one-dimensional pixel sequence. The pixels of the sequence are encrypted with key transmission. During data hiding, each non-overlapped block of the encrypted image is scanned in the closed Hilbert order to generate a one-dimensional encrypted pixel sequence. Finally, it exploits the histogram of two-layer adjacent encrypted pixel errors to embed secret data by histogram shifting and generate a marked encrypted image. Many experiments are carried out, and the results demonstrate that the proposed scheme reaches a high payload and outperforms some reversible data hiding schemes in the encrypted image.",
        "authors": "Chunqiang Yu, Xianquan Zhang, Xiaojun Xie, Zhenjun Tang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2882563",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper considers active vibration control of a dynamic hysteresis system with a piezoelectric actuator using acceleration sensors for data acquisition. A Hammerstein model is proposed with a BP neural network model and an autoregressive model with exogenous input representing, respectively, the static hysteresis and the rate-dependent characteristics. It is shown that the tracking errors of our model are less than 7% within a frequency range of 10-100 Hz, showing that the obtained model can describe the system hysteresis well. A μ-synthesis strategy is used for vibration control with a nonlinear inverse compensation based on the obtained model. The experimental results show that the proposed robust control method can effectively attenuate the vibration with more than 60% vibration reduction within a frequency range of 35-80 Hz.",
        "abstract": "This paper considers active vibration control of a dynamic hysteresis system with a piezoelectric actuator using acceleration sensors for data acquisition. A Hammerstein model is proposed with a BP neural network model and an autoregressive model with exogenous input representing, respectively, the static hysteresis and the rate-dependent characteristics. It is shown that the tracking errors of our model are less than 7% within a frequency range of 10-100 Hz, showing that the obtained model can describe the system hysteresis well. A μ-synthesis strategy is used for vibration control with a nonlinear inverse compensation based on the obtained model. The experimental results show that the proposed robust control method can effectively attenuate the vibration with more than 60% vibration reduction within a frequency range of 35-80 Hz.",
        "authors": "Jiabin Liu, Kemin Zhou",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884611",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, a compact passive equalizer for differential transmission channel is designed in TSV-based three-dimensional integrated circuits (3-D ICs). The compact size of the equalizer is achieved by a square shunt metal line. Three simplified odd-mode half circuit models are proposed for ground-signal-signal-ground (G-S-S-G) type TSVs, differential on-interposer interconnects, and differential channels, respectively. Those simplified models merely consist of frequency-independent elements and can accurately predict the differential insertion losses up to 20 GHz. Moreover, the electrical parameters of the proposed serial resistance-inductance (RL) type equalizers are derived from the system transfer functions and optimized by virtue of the time-domain inter-symbol interference cancellation technique. Further, the geometrical parameters of the RL equalizers are calculated by using a genetic algorithm based multi-objective optimization method. Finally, the performance of the designed RL equalizer is validated by both frequency- and time-domain simulations for 20 Gb/s high-speed differential signaling.",
        "abstract": "In this paper, a compact passive equalizer for differential transmission channel is designed in TSV-based three-dimensional integrated circuits (3-D ICs). The compact size of the equalizer is achieved by a square shunt metal line. Three simplified odd-mode half circuit models are proposed for ground-signal-signal-ground (G-S-S-G) type TSVs, differential on-interposer interconnects, and differential channels, respectively. Those simplified models merely consist of frequency-independent elements and can accurately predict the differential insertion losses up to 20 GHz. Moreover, the electrical parameters of the proposed serial resistance-inductance (RL) type equalizers are derived from the system transfer functions and optimized by virtue of the time-domain inter-symbol interference cancellation technique. Further, the geometrical parameters of the RL equalizers are calculated by using a genetic algorithm based multi-objective optimization method. Finally, the performance of the designed RL equalizer is validated by both frequency- and time-domain simulations for 20 Gb/s high-speed differential signaling.",
        "authors": "Da-Wei Wang, Gaofeng Wang, Kai Fu, Madhavan Swaminathan, Wen-Sheng Zhao, Wen-Yan Yin",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884036",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The high complexity of numerous optimal classic communication schemes, such as the maximum likelihood (ML) multiuser detector (MUD), often prevents their practical implementation. In this paper, we present an extensive review and tutorial on quantum search algorithms (QSA) and their potential applications, and we employ a QSA that finds the minimum of a function in order to perform optimal hard MUD with a quadratic reduction in the computational complexity when compared to that of the ML MUD. Furthermore, we follow a quantum approach to achieve the same performance as the optimal soft-input soft-output classic detectors by replacing them with a quantum algorithm, which estimates the weighted sum of a function's evaluations. We propose a soft-input soft-output quantum-assisted MUD (QMUD) scheme, which is the quantum-domain equivalent of the ML MUD. We then demonstrate its application using the design example of a direct-sequence code division multiple access system employing bit-interleaved coded modulation relying on iterative decoding, and compare it with the optimal ML MUD in terms of its performance and complexity. Both our extrinsic information transfer charts and bit error ratio curves show that the performance of the proposed QMUD and that of the optimal classic MUD are equivalent, but the QMUD's computational complexity is significantly lower.",
        "abstract": "The high complexity of numerous optimal classic communication schemes, such as the maximum likelihood (ML) multiuser detector (MUD), often prevents their practical implementation. In this paper, we present an extensive review and tutorial on quantum search algorithms (QSA) and their potential applications, and we employ a QSA that finds the minimum of a function in order to perform optimal hard MUD with a quadratic reduction in the computational complexity when compared to that of the ML MUD. Furthermore, we follow a quantum approach to achieve the same performance as the optimal soft-input soft-output classic detectors by replacing them with a quantum algorithm, which estimates the weighted sum of a function's evaluations. We propose a soft-input soft-output quantum-assisted MUD (QMUD) scheme, which is the quantum-domain equivalent of the ML MUD. We then demonstrate its application using the design example of a direct-sequence code division multiple access system employing bit-interleaved coded modulation relying on iterative decoding, and compare it with the optimal ML MUD in terms of its performance and complexity. Both our extrinsic information transfer charts and bit error ratio curves show that the performance of the proposed QMUD and that of the optimal classic MUD are equivalent, but the QMUD's computational complexity is significantly lower.",
        "authors": "Lajos Hanzo, Panagiotis Botsinis, Soon Xin Ng",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2259536",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Single-beacon navigation for group autonomous underwater vehicles (AUVs) suffers from long navigation period and asynchronization. This paper addresses the problem by introducing the single-input-multiple-output (SIMO) model, where, a single AUV transmits the signal and other AUVs receive the signal. Utilizing the time-delay of signals from the beacon and the sender, the navigation for all AUVs is achieved simultaneously, and the navigation period is dramatically decreased. Moreover, based on the new SIMO model, a tracking method utilizing extended Kalman filter is also put forward to increase the navigation precision and reduce the computational burden. Finally, a coordinate fusion algorithm based on the location, error, and time-delay is presented, which fuses the independent AUV coordinates to be a higher precision holistic coordinate system for group AUVs. Simulation results show the effectiveness of the proposed method.",
        "abstract": "Single-beacon navigation for group autonomous underwater vehicles (AUVs) suffers from long navigation period and asynchronization. This paper addresses the problem by introducing the single-input-multiple-output (SIMO) model, where, a single AUV transmits the signal and other AUVs receive the signal. Utilizing the time-delay of signals from the beacon and the sender, the navigation for all AUVs is achieved simultaneously, and the navigation period is dramatically decreased. Moreover, based on the new SIMO model, a tracking method utilizing extended Kalman filter is also put forward to increase the navigation precision and reduce the computational burden. Finally, a coordinate fusion algorithm based on the location, error, and time-delay is presented, which fuses the independent AUV coordinates to be a higher precision holistic coordinate system for group AUVs. Simulation results show the effectiveness of the proposed method.",
        "authors": "Chunhui Zhao, Jin Fu, Shuangning Yu, Sibo Sun, Zhibo Shi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883435",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Practical applications often require learning algorithms capable of addressing data streams with concept drift and class imbalance. This paper proposes an online active learning paired ensemble for drifting streams with class imbalance. The paired ensemble consists of a long-term stable classifier and a dynamic classifier to address both sudden concept drift and gradual concept drift. To select the most representative instances for learning, a hybrid labeling strategy which includes an uncertainty strategy and an imbalance strategy is proposed. The uncertainty strategy applies a margin-based uncertainty criterion and a dynamic adjustment threshold. Based on the categorical distribution of the last data block, the imbalance strategy prefers to learn instances of the minority category. In addition, it also incorporates the advantages of the traditional random strategy and helps to capture the drifts away from the decision boundary. Experiments on real datasets and synthetic datasets utilize prequential AUC as an evaluation index, comparing the classification performance of our method with semi-supervised and supervised learning methods. The results show that the proposed methods can obtain higher AUC values at an even lower labeling cost. Moreover, it is noteworthy that the labeling cost can be dynamically allocated according to the concept drift and imbalance ratio.",
        "abstract": "Practical applications often require learning algorithms capable of addressing data streams with concept drift and class imbalance. This paper proposes an online active learning paired ensemble for drifting streams with class imbalance. The paired ensemble consists of a long-term stable classifier and a dynamic classifier to address both sudden concept drift and gradual concept drift. To select the most representative instances for learning, a hybrid labeling strategy which includes an uncertainty strategy and an imbalance strategy is proposed. The uncertainty strategy applies a margin-based uncertainty criterion and a dynamic adjustment threshold. Based on the categorical distribution of the last data block, the imbalance strategy prefers to learn instances of the minority category. In addition, it also incorporates the advantages of the traditional random strategy and helps to capture the drifts away from the decision boundary. Experiments on real datasets and synthetic datasets utilize prequential AUC as an evaluation index, comparing the classification performance of our method with semi-supervised and supervised learning methods. The results show that the proposed methods can obtain higher AUC values at an even lower labeling cost. Moreover, it is noteworthy that the labeling cost can be dynamically allocated according to the concept drift and imbalance ratio.",
        "authors": "Hang Zhang, Jicheng Shan, Qingbao Liu, Weike Liu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2882872",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper is concerned with the protocol-based fault detection problem for a class of discrete systems with mixed time delays and missing measurements under uncertain missing probabilities. The phenomenon of missing measurements is characterized by a set of Bernoulli random variables, where each sensor could have individual missing probability and the corresponding occurrence probability could be uncertain. In order to mitigate the communication load of the network and reduce the incidence of the data collisions in the engineering reality, the round-robin (RR) protocol is employed to regulate the data transmission orders. The purpose of the addressed problem is to design a fault detection filter such that, in the simultaneous presence of mixed time delays, missing measurements, and RR protocol mechanism, the resulted filtering error system is asymptotically mean-square stable with a satisfactory H∞performance. In particular, some sufficient conditions are derived in terms of certain matrix inequalities and the explicit expression of the required filter parameters is proposed. Finally, a numerical example is employed to illustrate the effectiveness of the designed fault detection scheme.",
        "abstract": "This paper is concerned with the protocol-based fault detection problem for a class of discrete systems with mixed time delays and missing measurements under uncertain missing probabilities. The phenomenon of missing measurements is characterized by a set of Bernoulli random variables, where each sensor could have individual missing probability and the corresponding occurrence probability could be uncertain. In order to mitigate the communication load of the network and reduce the incidence of the data collisions in the engineering reality, the round-robin (RR) protocol is employed to regulate the data transmission orders. The purpose of the addressed problem is to design a fault detection filter such that, in the simultaneous presence of mixed time delays, missing measurements, and RR protocol mechanism, the resulted filtering error system is asymptotically mean-square stable with a satisfactory H∞performance. In particular, some sufficient conditions are derived in terms of certain matrix inequalities and the explicit expression of the required filter parameters is proposed. Finally, a numerical example is employed to illustrate the effectiveness of the designed fault detection scheme.",
        "authors": "Dongyan Chen, Jun Hu, Weilu Chen, Xiaoyang Yu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884132",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, we extend polar decoding function to our previous design, and propose a flexible quad-mode forward error correction application specific instruction-set processor (QFEC ASIP) that supports polar, low-density parity-check (LDPC), turbo, and convolutional code (CC) decoding with multiple code lengths and code rates. A unified polar/LDPC/turbo/CC quad-mode algorithm framework is presented. The top level architecture of QFEC ASIP and the polar data path are designed on the basis of the algorithm framework. A quad-mode confliction-free global memory system is proposed. 65.2% of global memory banks, 48.9% of global memory bits, and 29.7% of global memory area are saved via hardware sharing. Specially accelerated FEC decoding instructions make the decoding procedure fully programmable and ensure the high throughput. Synthesis using 65-nm technology shows that the total area of QFEC ASIP is 4.26 mm2. QFEC ASIP provides the maximum throughput of 1345 Mb/s for polar, 917 Mb/s for LDPC (WiMAX), 320 Mb/s for turbo, and 387 Mb/s for CC (64 states) at the clock frequency of 344 MHz. QFEC ASIP occupies much smaller silicon area than the sum of the silicon area of 4 single-mode FEC decoders that together provide a similar function range as QFEC ASIP.",
        "abstract": "In this paper, we extend polar decoding function to our previous design, and propose a flexible quad-mode forward error correction application specific instruction-set processor (QFEC ASIP) that supports polar, low-density parity-check (LDPC), turbo, and convolutional code (CC) decoding with multiple code lengths and code rates. A unified polar/LDPC/turbo/CC quad-mode algorithm framework is presented. The top level architecture of QFEC ASIP and the polar data path are designed on the basis of the algorithm framework. A quad-mode confliction-free global memory system is proposed. 65.2% of global memory banks, 48.9% of global memory bits, and 29.7% of global memory area are saved via hardware sharing. Specially accelerated FEC decoding instructions make the decoding procedure fully programmable and ensure the high throughput. Synthesis using 65-nm technology shows that the total area of QFEC ASIP is 4.26 mm2. QFEC ASIP provides the maximum throughput of 1345 Mb/s for polar, 917 Mb/s for LDPC (WiMAX), 320 Mb/s for turbo, and 387 Mb/s for CC (64 states) at the clock frequency of 344 MHz. QFEC ASIP occupies much smaller silicon area than the sum of the silicon area of 4 single-mode FEC decoders that together provide a similar function range as QFEC ASIP.",
        "authors": "Dake Liu, Shaohan Liu, Wan Qiao",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883292",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Incomplete specifications on new products are very common resulting in unreliable products and costly scope creeps and design changes. Designing out failures using lessons learned can result in very high reliability and significantly lower warranty costs. The quickest way to make use of the lessons learned is to learn from heuristics which are short statements based on wisdom from lessons learned over many products. The article covers examples to show how the heuristics were applied to successful products.",
        "abstract": "Incomplete specifications on new products are very common resulting in unreliable products and costly scope creeps and design changes. Designing out failures using lessons learned can result in very high reliability and significantly lower warranty costs. The quickest way to make use of the lessons learned is to learn from heuristics which are short statements based on wisdom from lessons learned over many products. The article covers examples to show how the heuristics were applied to successful products.",
        "authors": "Dev Raheja",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2259535",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The manual creation of complex 3D structures for use in engineering analysis is a major obstacle to analyzing physically realistic structures. A bias is invariably imposed when a mixture is manually composed, and the structure is rarely representative of the process by which composites are fabricated. Properties such as packing density and anisotropies that seem to easily occur in nature are very difficult to obtain with manual arrangements. This paper addresses the creation of complex 3D mixtures, comprising crystals embedded in a matrix, for subsequent electromagnetic (EM) analysis. The physically realistic arrangement of the crystals is facilitated by the use of physics engine software, specifically the Bullet physics library, which renders the realistic effects in advanced computer games. A composite mixture of crystals is created by pouring a series of random crystals into a box with the crystals bouncing against each other and aligning just as they do in the real world. Higher packing densities are obtained than can be reasonably obtained with manual construction. The arrangement of the obtained crystals reflects the real world alignment of asymmetric crystals. A composite is created here and used with EM simulation software to investigate energy localization in materials.",
        "abstract": "The manual creation of complex 3D structures for use in engineering analysis is a major obstacle to analyzing physically realistic structures. A bias is invariably imposed when a mixture is manually composed, and the structure is rarely representative of the process by which composites are fabricated. Properties such as packing density and anisotropies that seem to easily occur in nature are very difficult to obtain with manual arrangements. This paper addresses the creation of complex 3D mixtures, comprising crystals embedded in a matrix, for subsequent electromagnetic (EM) analysis. The physically realistic arrangement of the crystals is facilitated by the use of physics engine software, specifically the Bullet physics library, which renders the realistic effects in advanced computer games. A composite mixture of crystals is created by pouring a series of random crystals into a box with the crystals bouncing against each other and aligning just as they do in the real world. Higher packing densities are obtained than can be reasonably obtained with manual construction. The arrangement of the obtained crystals reflects the real world alignment of asymmetric crystals. A composite is created here and used with EM simulation software to investigate energy localization in materials.",
        "authors": "Austin J. Pickles, Ian M. Kilgore, Michael B. Steer",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2262014",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Dempster–Shafer evidence theory has been extensively applied in a variety of fields due to its ability to solve knowledge reasoning and decision-making problem under uncertain environments. Nevertheless, it is still an open issue about how to determine the basic probability assignment (BPA). In this paper, a new non-parametric method based on kernel density estimation is proposed to determine BPA. First, the probability density function of each attribute is calculated, which can be regarded as the probability model for the related attribute using the training sample. Then, a nested BPA function is constructed using the intersections point of test sample and probability models. Finally, Dempster’s combination rule is used to combine multiple BPAs to get the final BPA. Some classification experiments are conducted on several datasets. The experimental results demonstrate that the proposed method is more effective and reasonable in determining BPAs, which has a better classification performance than the existing method.",
        "abstract": "Dempster–Shafer evidence theory has been extensively applied in a variety of fields due to its ability to solve knowledge reasoning and decision-making problem under uncertain environments. Nevertheless, it is still an open issue about how to determine the basic probability assignment (BPA). In this paper, a new non-parametric method based on kernel density estimation is proposed to determine BPA. First, the probability density function of each attribute is calculated, which can be regarded as the probability model for the related attribute using the training sample. Then, a nested BPA function is constructed using the intersections point of test sample and probability models. Finally, Dempster’s combination rule is used to combine multiple BPAs to get the final BPA. Some classification experiments are conducted on several datasets. The experimental results demonstrate that the proposed method is more effective and reasonable in determining BPAs, which has a better classification performance than the existing method.",
        "authors": "Bowen Qin, Fuyuan Xiao",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883513",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Electroencephalogram (EEG) is getting special attention of late in the detection of sleep apnea as it is directly related to the neural activity. But apnea detection through visual monitoring of EEG signal by an expert is expensive, difficult, and susceptible to human error. To counter this problem, an automatic apnea detection scheme is proposed in this paper using a single lead EEG signal, which can differentiate apnea patients and healthy subjects and also classify apnea and non-apnea frames in the data of an apnea patient. Each sub-frame of a given frame of EEG data is first decomposed into band-limited intrinsic mode functions (BLIMFs) by using the variational mode decomposition (VMD). The advantage of using VMD is to obtain compact BLIMFs with adaptive center frequencies, which give an opportunity to capture the local information corresponding to varying neural activity. Furthermore, by extracting features from each BLIMF, a temporal within-frame feature variation pattern is obtained for each mode. We propose to fit the resulting pattern with the Rician model (RiM) and utilize the fitted model parameters as features. The use of such VMD-RiM features not only offers better feature quality but also ensures very low feature dimension. In order to evaluate the performance of the proposed method, K nearest neighbor classifier is used and various cross-validation schemes are carried out. Detailed experimentation is carried out on several apnea and healthy subjects of various apnea-hypopnea indices from three publicly available datasets and it is found that the proposed method achieves superior classification performances in comparison to those obtained by the existing methods, in terms of sensitivity, specificity, and accuracy.",
        "abstract": "Electroencephalogram (EEG) is getting special attention of late in the detection of sleep apnea as it is directly related to the neural activity. But apnea detection through visual monitoring of EEG signal by an expert is expensive, difficult, and susceptible to human error. To counter this problem, an automatic apnea detection scheme is proposed in this paper using a single lead EEG signal, which can differentiate apnea patients and healthy subjects and also classify apnea and non-apnea frames in the data of an apnea patient. Each sub-frame of a given frame of EEG data is first decomposed into band-limited intrinsic mode functions (BLIMFs) by using the variational mode decomposition (VMD). The advantage of using VMD is to obtain compact BLIMFs with adaptive center frequencies, which give an opportunity to capture the local information corresponding to varying neural activity. Furthermore, by extracting features from each BLIMF, a temporal within-frame feature variation pattern is obtained for each mode. We propose to fit the resulting pattern with the Rician model (RiM) and utilize the fitted model parameters as features. The use of such VMD-RiM features not only offers better feature quality but also ensures very low feature dimension. In order to evaluate the performance of the proposed method, K nearest neighbor classifier is used and various cross-validation schemes are carried out. Detailed experimentation is carried out on several apnea and healthy subjects of various apnea-hypopnea indices from three publicly available datasets and it is found that the proposed method achieves superior classification performances in comparison to those obtained by the existing methods, in terms of sensitivity, specificity, and accuracy.",
        "authors": "Arnab Bhattacharjee, M. Omair Ahmad, Shaikh Anowarul Fattah, Wei-Ping Zhu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883062",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The lightning transient computation is performed in this paper for large-scale multiconductor systems. The circuit parameters of the branches in a large-scale multiconductor system are represented by the resistances, inductances, and capacitances. An efficient algorithm is presented for evaluating the circuit parameters. With a segmentation for the branches, each group of the coupled segments in the multiconductor system is replaced with a coupled circuit unit. The electromagnetic coupling among the segments is taken into account by a simplified approach. The multiconductor system is then converted into a complete circuit model that comprises a large number of coupled circuit units. The lightning transient responses can be obtained from the transient solutions based on the circuit model. Laboratory measurement is made with a reduced-scale experimental setup. The measured results are compared with the computed ones to examine the validity of the circuit model. A numerical example is also given for investigating the distribution characteristic of lightning transient responses in an actual multiconductor system.",
        "abstract": "The lightning transient computation is performed in this paper for large-scale multiconductor systems. The circuit parameters of the branches in a large-scale multiconductor system are represented by the resistances, inductances, and capacitances. An efficient algorithm is presented for evaluating the circuit parameters. With a segmentation for the branches, each group of the coupled segments in the multiconductor system is replaced with a coupled circuit unit. The electromagnetic coupling among the segments is taken into account by a simplified approach. The multiconductor system is then converted into a complete circuit model that comprises a large number of coupled circuit units. The lightning transient responses can be obtained from the transient solutions based on the circuit model. Laboratory measurement is made with a reduced-scale experimental setup. The measured results are compared with the computed ones to examine the validity of the circuit model. A numerical example is also given for investigating the distribution characteristic of lightning transient responses in an actual multiconductor system.",
        "authors": "Xiaoqing Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883385",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A nanoslit is capable of performing versatile functions in nanophotonic applications. In this invited paper, we discuss the physics and manipulation methods of surface plasmon excitation such as directional switching at a nanoslit. Furthermore, enhancing the light intensity passing through a nanoslit by employing embedded metallic nanoislands is experimentally presented and numerically analyzed.",
        "abstract": "A nanoslit is capable of performing versatile functions in nanophotonic applications. In this invited paper, we discuss the physics and manipulation methods of surface plasmon excitation such as directional switching at a nanoslit. Furthermore, enhancing the light intensity passing through a nanoslit by employing embedded metallic nanoislands is experimentally presented and numerically analyzed.",
        "authors": "Hansik Yun, Il-Min Lee, Seong-Woo Cho, Seung-Yeol Lee, Taerin Chung, Yongjun Lim",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2265551",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper considers a group of drogues whose objective is to estimate the physical parameters that determine the dynamics of ocean nonlinear internal waves. Internal waves are important in oceanography because, as they travel, they are capable of displacing small animals, such as plankton, larva, and fish. These waves are described by models that employ trigonometric functions parameterized by a set of constants such as amplitude, wavenumber, and temporal frequency. While underwater, individual drogues do not have access to absolute position information and only rely on inter-drogue measurements. Building on this data and the study of the drogue dynamics under the flow induced by the internal wave, we design two strategies, referred to as the Vanishing Derivative Method and the Passing Wave Method, that are able to determine the wavenumber and the speed ratio. Either of these strategies can be employed in the Parameter Determination Strategy to determine all the remaining wave parameters. We analyze the correctness of the proposed strategies and discuss their robustness against different sources of error. Simulations illustrate the algorithm performance under noisy measurements as well as the effect of different initial drogue configurations.",
        "abstract": "This paper considers a group of drogues whose objective is to estimate the physical parameters that determine the dynamics of ocean nonlinear internal waves. Internal waves are important in oceanography because, as they travel, they are capable of displacing small animals, such as plankton, larva, and fish. These waves are described by models that employ trigonometric functions parameterized by a set of constants such as amplitude, wavenumber, and temporal frequency. While underwater, individual drogues do not have access to absolute position information and only rely on inter-drogue measurements. Building on this data and the study of the drogue dynamics under the flow induced by the internal wave, we design two strategies, referred to as the Vanishing Derivative Method and the Passing Wave Method, that are able to determine the wavenumber and the speed ratio. Either of these strategies can be employed in the Parameter Determination Strategy to determine all the remaining wave parameters. We analyze the correctness of the proposed strategies and discuss their robustness against different sources of error. Simulations illustrate the algorithm performance under noisy measurements as well as the effect of different initial drogue configurations.",
        "authors": "Jorge Cortés, Michael Ouimet",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2271211",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper reports the latest technological advances made by the Industrial Technology Research Institute (ITRI) in flexible displays, especially the flexible substrate, thin-film transistor (TFT) backplane, and active matrix organic light-emitting diode display. Using the leading cholesteric liquid crystal technology of ITRI, we develop a rewritable, environmentally friendly thermal printable e-paper. The e-paper, devised to reduce traditional paper consumption, achieves a high resolution of 300 dpi with a memory function. In addition, we report on the ITRI's initial success in demonstrating a complete R2R process for multisensing touch panels on 100-μmthick flexible glass substrates provided by Corning.",
        "abstract": "This paper reports the latest technological advances made by the Industrial Technology Research Institute (ITRI) in flexible displays, especially the flexible substrate, thin-film transistor (TFT) backplane, and active matrix organic light-emitting diode display. Using the leading cholesteric liquid crystal technology of ITRI, we develop a rewritable, environmentally friendly thermal printable e-paper. The e-paper, devised to reduce traditional paper consumption, achieves a high resolution of 300 dpi with a memory function. In addition, we report on the ITRI's initial success in demonstrating a complete R2R process for multisensing touch panels on 100-μmthick flexible glass substrates provided by Corning.",
        "authors": "C. T. Liu, Janglin Chen",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2260792",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "There are a large number of high-dimensional data in the Internet of Brain Things, and the data is reduced from the high-dimensional to low-dimensional to maintain the similarity between the data, thereby effectively ensuring the operating speed of the Internet of Brain Things. In the traditional method, the distributed dimensionality reduction reconstruction algorithm of the High dimensional data has poor dimensionality reduction effect and serious data distortion after reconstruction. A method for dimensionality reconstruction of high-dimensional data in the Internet of Brain Things is proposed. By using the algorithm of linear discriminant analysis, the projection matrix of high dimensional data is constructed to solve it. According to the solution results, using improved implicit variable model to establish a high dimensional large data dimensionality reduction model for the Internet of Brain Things. The fitness value of data after dimensionality reduction is calculated by quantum immune clonal algorithm, and the optimal individual and optimal solution are determined. The data reconfiguration is realized through the optimal solution marshalling. Experimental results show that the proposed algorithm can effectively improve the dimensionality reduction of High dimensional data in the Internet of Brain Things. After reconstruction, the reconstructed data retain accurate data information, the reliability of reconstructed data is high, and the computational complexity is not high, the need for small storage space, and the advantages of strong promotion ability.",
        "abstract": "There are a large number of high-dimensional data in the Internet of Brain Things, and the data is reduced from the high-dimensional to low-dimensional to maintain the similarity between the data, thereby effectively ensuring the operating speed of the Internet of Brain Things. In the traditional method, the distributed dimensionality reduction reconstruction algorithm of the High dimensional data has poor dimensionality reduction effect and serious data distortion after reconstruction. A method for dimensionality reconstruction of high-dimensional data in the Internet of Brain Things is proposed. By using the algorithm of linear discriminant analysis, the projection matrix of high dimensional data is constructed to solve it. According to the solution results, using improved implicit variable model to establish a high dimensional large data dimensionality reduction model for the Internet of Brain Things. The fitness value of data after dimensionality reduction is calculated by quantum immune clonal algorithm, and the optimal individual and optimal solution are determined. The data reconfiguration is realized through the optimal solution marshalling. Experimental results show that the proposed algorithm can effectively improve the dimensionality reduction of High dimensional data in the Internet of Brain Things. After reconstruction, the reconstructed data retain accurate data information, the reliability of reconstructed data is high, and the computational complexity is not high, the need for small storage space, and the advantages of strong promotion ability.",
        "authors": "Huang Hang, Ling Tian, Yimin Zhou",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883460",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper discusses the L2- L∞filter design problem for non-linear two-dimensional (2-D) uncertain continuous systems with state delays and saturation. The non-linear function under consideration is assumed to satisfy the Lipschitz condition while the saturation term is being dealt by using a memory-less sector region methodology. A suitable Lyapunov-Krasovskii functional is considered, and the Wirtinger-based integral inequality method is used to derive some sufficient conditions which ensure that the resultant filtering error system is robustly asymptotically stable along-with the specified L2- L∞disturbance attenuation level γ. A suitable example explains the derived results' usefulness.",
        "abstract": "This paper discusses the L2- L∞filter design problem for non-linear two-dimensional (2-D) uncertain continuous systems with state delays and saturation. The non-linear function under consideration is assumed to satisfy the Lipschitz condition while the saturation term is being dealt by using a memory-less sector region methodology. A suitable Lyapunov-Krasovskii functional is considered, and the Wirtinger-based integral inequality method is used to derive some sufficient conditions which ensure that the resultant filtering error system is robustly asymptotically stable along-with the specified L2- L∞disturbance attenuation level γ. A suitable example explains the derived results' usefulness.",
        "authors": "Imran Ghous, Jahanzeb Akhtar, Khurram Ali, Mujtaba Hussain Jeffery, Zhaoxia Duan",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884254",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Emergency plans can be regarded as the effective guidance of hazard emergency responses, and they include the textual descriptions of emergency response processes in terms of natural language. In this paper, we propose an approach to automatically extract emergency response process models from Chinese emergency plans. First, the emergency plan is represented as a text tree according to its layout markups and sentence-sequential relations. Then, process model elements, including four-level response condition formulas, executive roles, response tasks, and flow relations, are identified by rule-based approaches. Finally, an emergency response process tree is generated from both the text tree and extracted process model elements, and is transformed to an emergency response process that is modeled as business process modeling notation. Extensive experiments on real-world emergency plans demonstrate that the proposed approach is capable of extracting emergency response process models that are highly consistent with both the manually extracted models and the textual plans.",
        "abstract": "Emergency plans can be regarded as the effective guidance of hazard emergency responses, and they include the textual descriptions of emergency response processes in terms of natural language. In this paper, we propose an approach to automatically extract emergency response process models from Chinese emergency plans. First, the emergency plan is represented as a text tree according to its layout markups and sentence-sequential relations. Then, process model elements, including four-level response condition formulas, executive roles, response tasks, and flow relations, are identified by rule-based approaches. Finally, an emergency response process tree is generated from both the text tree and extracted process model elements, and is transformed to an emergency response process that is modeled as business process modeling notation. Extensive experiments on real-world emergency plans demonstrate that the proposed approach is capable of extracting emergency response process models that are highly consistent with both the manually extracted models and the textual plans.",
        "authors": "Cong Liu, Guiyuan Yuan, Hua Duan, Qingtian Zeng, Weijian Ni, Wenyan Guo",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2880515",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Visible Light Communication (VLC) is a rising communication technology which uses Light-Emitting Diode (LED) luminaries for high-speed data transferring; however, the optical signal is usually degraded by the noise in the practical VLC system. In this paper, Allan Variance is introduced for noise analysis and modeling in VLC for the first time, which provides an efficient method to measure different kinds of noise in the VLC systems. By applying Allan variance to analyze the signals collected from the real-world environment, both white noise and random walk are observed in the VLC systems. The observed white noise and random walk are also modeled by using the Allan variance. The noise analysis and modeling based on Allan variance provide a method to improve the performance of VLC.",
        "abstract": "Visible Light Communication (VLC) is a rising communication technology which uses Light-Emitting Diode (LED) luminaries for high-speed data transferring; however, the optical signal is usually degraded by the noise in the practical VLC system. In this paper, Allan Variance is introduced for noise analysis and modeling in VLC for the first time, which provides an efficient method to measure different kinds of noise in the VLC systems. By applying Allan variance to analyze the signals collected from the real-world environment, both white noise and random walk are observed in the VLC systems. The observed white noise and random walk are also modeled by using the Allan variance. The noise analysis and modeling based on Allan variance provide a method to improve the performance of VLC.",
        "authors": "Jun Yang, Longning Qi, Longxing Shi, Luchi Hua, Yuan Zhuang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883737",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In 1993, Krishnapuram and Keller first proposed possibilistic C-means (PCM) clustering by relaxing the constraint in fuzzy C-means of which memberships for a data point across classes sum to 1. The PCM algorithm tends to produce coincident clusters that can be a merit of PCM as a good mode-seeking algorithm, and so various extensions of PCM had been proposed in the literature. However, the performance of PCM and its extensions heavily depends on initializations and parameters selection with a number of clusters to be given a priori. In this paper, we propose a novel PCM algorithm, termed a fully unsupervised PCM (FU-PCM), without any initialization and parameter selection that can automatically find a good number of clusters. We start by constructing a generalized framework for PCM clustering that can be a generalization of most existing PCM algorithms. Based on the generalized PCM framework, we propose the new type FU-PCM so that the proposed FU-PCM algorithm is free of parameter selection and initializations without a given number of clusters. That is, the FU-PCM becomes a FU-PCM clustering algorithm. Comparisons between the proposed FU-PCM and other existing methods are made. The computational complexity of the FU-PCM algorithm is also analyzed. Some numerical data and real data sets are used to show these good aspects of FU-PCM. Experimental results and comparisons actually demonstrate the proposed FU-PCM is an effective parameter-free clustering algorithm that can also automatically find the optimal number of clusters.",
        "abstract": "In 1993, Krishnapuram and Keller first proposed possibilistic C-means (PCM) clustering by relaxing the constraint in fuzzy C-means of which memberships for a data point across classes sum to 1. The PCM algorithm tends to produce coincident clusters that can be a merit of PCM as a good mode-seeking algorithm, and so various extensions of PCM had been proposed in the literature. However, the performance of PCM and its extensions heavily depends on initializations and parameters selection with a number of clusters to be given a priori. In this paper, we propose a novel PCM algorithm, termed a fully unsupervised PCM (FU-PCM), without any initialization and parameter selection that can automatically find a good number of clusters. We start by constructing a generalized framework for PCM clustering that can be a generalization of most existing PCM algorithms. Based on the generalized PCM framework, we propose the new type FU-PCM so that the proposed FU-PCM algorithm is free of parameter selection and initializations without a given number of clusters. That is, the FU-PCM becomes a FU-PCM clustering algorithm. Comparisons between the proposed FU-PCM and other existing methods are made. The computational complexity of the FU-PCM algorithm is also analyzed. Some numerical data and real data sets are used to show these good aspects of FU-PCM. Experimental results and comparisons actually demonstrate the proposed FU-PCM is an effective parameter-free clustering algorithm that can also automatically find the optimal number of clusters.",
        "authors": "Miin-Shen Yang, Shou-Jen Chang-Chien, Yessica Nataliani",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884956",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The advent of cloud platform has promoted the complexity and scales of industries increasingly. Any deliberate or non-deliberate faults may cause enormous impact on system performance and server costs. Anomaly detection is a good way to identify anomalies and improve the dependability of the cloud platform. However, some of the anomaly detection methods are labeled data dependency, and some of them are sensitive to the dynamic runtime environment of the cloud platform. To address the problems, an adaptive and incremental clustering anomaly detection algorithm for virtual machines under the cloud platform runtime environment is proposed. Compared with the previous detection methods, the effect of the runtime environment factor is taken into account. Owning to the high level of dynamic cloud platform manages and the resources allocation of virtual machines, the environmental factors play an important role in the running performance of the virtual machines. In this paper, an improved adaptive and incremental clustering algorithm is introduced to perform the detection with the considerations of the cloud platform runtime environment. To demonstrate the effectiveness, two sets of experiments are performed. The experimental results indicate that the proposed anomaly detection method can greatly improve the detection accuracy rate even the cloud platform runtime environment changes.",
        "abstract": "The advent of cloud platform has promoted the complexity and scales of industries increasingly. Any deliberate or non-deliberate faults may cause enormous impact on system performance and server costs. Anomaly detection is a good way to identify anomalies and improve the dependability of the cloud platform. However, some of the anomaly detection methods are labeled data dependency, and some of them are sensitive to the dynamic runtime environment of the cloud platform. To address the problems, an adaptive and incremental clustering anomaly detection algorithm for virtual machines under the cloud platform runtime environment is proposed. Compared with the previous detection methods, the effect of the runtime environment factor is taken into account. Owning to the high level of dynamic cloud platform manages and the resources allocation of virtual machines, the environmental factors play an important role in the running performance of the virtual machines. In this paper, an improved adaptive and incremental clustering algorithm is introduced to perform the detection with the considerations of the cloud platform runtime environment. To demonstrate the effectiveness, two sets of experiments are performed. The experimental results indicate that the proposed anomaly detection method can greatly improve the detection accuracy rate even the cloud platform runtime environment changes.",
        "authors": "Hancui Zhang, Jun Liu, Tianshu Wu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884508",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Unlike conventional CMOS circuits, reversible circuits do not have latent faults, so faults occurring in internal circuit nodes always result in an error at the output. This is a unique feature for online or concurrent fault tolerance and the main motivation of this paper with an aim of achieving highly efficient fault-tolerant CMOS logic circuits. For this purpose, we first implement fault-tolerant reversible circuits. We develop two techniques to make a reversible circuit fault-tolerant by using multiple-control Toffoli gates. The first technique is based on single parity preserving and offers error detection for odd number of errors at the output. The second technique is constructed on Hamming codes, which results in circuits detecting any number of errors unless the number of errors at the output is the order of d or correcting (d - 1)/2 bit errors, where d is the minimum Hamming distance between any pair of bit patterns. We select d = 3 in this paper. We also claim that 100% error detection is possible with conservative reversible gates, such as a Fredkin gate. For this purpose, we develop a greedy synthesis algorithm that implements an arbitrary reversible function with multiple-control Fredkin gates. As the next step, we utilize the proposed reversible circuits with conventional CMOS gates. This certainly approves the practical use of the proposed techniques. The effectiveness of our techniques is demonstrated on benchmark circuits, implemented by both reversible and CMOS gates, in terms of fault tolerance performances and area costs. Comparisons with the related studies in the literature as well as with dual-modular redundancy and triple-modular redundancy-based circuits clearly favor the proposed designs.",
        "abstract": "Unlike conventional CMOS circuits, reversible circuits do not have latent faults, so faults occurring in internal circuit nodes always result in an error at the output. This is a unique feature for online or concurrent fault tolerance and the main motivation of this paper with an aim of achieving highly efficient fault-tolerant CMOS logic circuits. For this purpose, we first implement fault-tolerant reversible circuits. We develop two techniques to make a reversible circuit fault-tolerant by using multiple-control Toffoli gates. The first technique is based on single parity preserving and offers error detection for odd number of errors at the output. The second technique is constructed on Hamming codes, which results in circuits detecting any number of errors unless the number of errors at the output is the order of d or correcting (d - 1)/2 bit errors, where d is the minimum Hamming distance between any pair of bit patterns. We select d = 3 in this paper. We also claim that 100% error detection is possible with conservative reversible gates, such as a Fredkin gate. For this purpose, we develop a greedy synthesis algorithm that implements an arbitrary reversible function with multiple-control Fredkin gates. As the next step, we utilize the proposed reversible circuits with conventional CMOS gates. This certainly approves the practical use of the proposed techniques. The effectiveness of our techniques is demonstrated on benchmark circuits, implemented by both reversible and CMOS gates, in terms of fault tolerance performances and area costs. Comparisons with the related studies in the literature as well as with dual-modular redundancy and triple-modular redundancy-based circuits clearly favor the proposed designs.",
        "authors": "M. Hüsrev Cilasun, Mustafa Altun, Sajjad Parvin",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883833",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Large number of outlier detection methods have emerged in recent years due to their importance in many real-world applications. The graph-based methods, which can effectively capture the inter-dependencies of related objects, is one of the most powerful methods in this area. However, most of the graph-based methods ignore the local information around each node, which leads to a high false-positive rate for outlier detection. In this study, we present a new outlier detection model, which combines the graph representation with the local information around each object to construct a local information graph, and calculates the outlier score by performing a random walk process on the graph. Local information graph is constructed to capture the asymmetric inter-dependencies relationship between various types of data objects. Based on two different types of restart vectors to solve the dangling link problem, we propose two distinct algorithms for outlier detection. Experiments on synthetic datasets indicate that the proposed algorithms could efficiently distinguish outlier objects in different distributed datasets. Furthermore, the results on a number of real-world datasets also show that our approaches outperform the state-of-the-art outlier detection methods.",
        "abstract": "Large number of outlier detection methods have emerged in recent years due to their importance in many real-world applications. The graph-based methods, which can effectively capture the inter-dependencies of related objects, is one of the most powerful methods in this area. However, most of the graph-based methods ignore the local information around each node, which leads to a high false-positive rate for outlier detection. In this study, we present a new outlier detection model, which combines the graph representation with the local information around each object to construct a local information graph, and calculates the outlier score by performing a random walk process on the graph. Local information graph is constructed to capture the asymmetric inter-dependencies relationship between various types of data objects. Based on two different types of restart vectors to solve the dangling link problem, we propose two distinct algorithms for outlier detection. Experiments on synthetic datasets indicate that the proposed algorithms could efficiently distinguish outlier objects in different distributed datasets. Furthermore, the results on a number of real-world datasets also show that our approaches outperform the state-of-the-art outlier detection methods.",
        "authors": "Chao Wang, Hui Gao, Yan Fu, Zhen Liu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883681",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Community detection plays a vital role in network analysis, simplification, and compression, which reveals the network structure by dividing a network into several internally dense modules. Among plenty of methods, those based on statistical inference are widely used because they are theoretically sound and consistent. However, in many of them, the number of communities needs to be provided in advance or computed in a time-consuming way and parameters are usually initialized randomly, resulting in unstable accuracy and low convergence rate. In this paper, we present a community detection method based on modified PageRank and stochastic block model, which is able to compute the number of communities by finding community centers and initialize community assignments according to the centers and distance. Experiments on both synthetic and real-world networks prove that our method can intuitively give the number of communities, steadily get results of high NMI and modularity and efficiently speed up the convergence of optimizing likelihood probability.",
        "abstract": "Community detection plays a vital role in network analysis, simplification, and compression, which reveals the network structure by dividing a network into several internally dense modules. Among plenty of methods, those based on statistical inference are widely used because they are theoretically sound and consistent. However, in many of them, the number of communities needs to be provided in advance or computed in a time-consuming way and parameters are usually initialized randomly, resulting in unstable accuracy and low convergence rate. In this paper, we present a community detection method based on modified PageRank and stochastic block model, which is able to compute the number of communities by finding community centers and initialize community assignments according to the centers and distance. Experiments on both synthetic and real-world networks prove that our method can intuitively give the number of communities, steadily get results of high NMI and modularity and efficiently speed up the convergence of optimizing likelihood probability.",
        "authors": "Guangluan Xu, Jing Chen, Lei Wang, Xian Sun, Yang Wang, Yuanben Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2873675",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The use of coupled-bias common-mode feedback allows a low-voltage differential signaling driver, implemented in a deep-submicron process, to adjust its output common-mode voltage, as well as the output swing and equalization strength independently. This form of differential signaling driver has been incorporated in a 16-channel transmitter for the timing controller of an intra-panel interface. The flexibility of its output characteristics allows it to accommodate the range of channel characteristics commonly found in the intra-panel interface of an ultra-high-definition display. Fabricated in a 28-nm bulk CMOS process, our 16-channel transmitter occupies 1.39 mm2. Its total power consumption is 203.2 mW with a supply voltage of 1.05 V, running at 3.2 Gb/s with the maximum output swing. Measurements confirm that the transmitter can operate from 0.8 to 3.2 Gb/s with channels that replicate those encountered in an intra-panel interface. The maximum controllable range of the output swing, the output common-mode voltage, and equalization are 216-1072 mVdiff, 291-604 mV, and 0-14.6 dB, respectively.",
        "abstract": "The use of coupled-bias common-mode feedback allows a low-voltage differential signaling driver, implemented in a deep-submicron process, to adjust its output common-mode voltage, as well as the output swing and equalization strength independently. This form of differential signaling driver has been incorporated in a 16-channel transmitter for the timing controller of an intra-panel interface. The flexibility of its output characteristics allows it to accommodate the range of channel characteristics commonly found in the intra-panel interface of an ultra-high-definition display. Fabricated in a 28-nm bulk CMOS process, our 16-channel transmitter occupies 1.39 mm2. Its total power consumption is 203.2 mW with a supply voltage of 1.05 V, running at 3.2 Gb/s with the maximum output swing. Measurements confirm that the transmitter can operate from 0.8 to 3.2 Gb/s with channels that replicate those encountered in an intra-panel interface. The maximum controllable range of the output swing, the output common-mode voltage, and equalization are 216-1072 mVdiff, 291-604 mV, and 0-14.6 dB, respectively.",
        "authors": "Gi-Moon Hong, Jihwan Park, Joo-Hyung Chae, Mino Kim, Suhwan Kim",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884727",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Person Re-Identification (Re-Id) is among the main constituents of an automated visual surveillance system. It aims at finding out true matches of a given query person from a large repository of non-overlapping camera images/videos. In this paper, we have proposed an efficient Re-Id approach that is based on a highly discriminative hybrid person representation which combines the low-level hand-crafted appearance based features together with the mid-level attributes and semantic based deep features. The lowlevel hand crafted features are extracted by using hierarchical Gaussian and local histogram distributions in different color spaces. These features incorporate discriminative texture, shape and color information which is invariant to distractors, e.g., variations in pose, viewpoint and illumination, and so on. The mid-level attribute based deep features are extracted to incorporate contextual- and semantic-based information. The feature space is optimized and self-learned using cross-view quadratic discriminant analysis and multiple metric learning, with the aim to reduce the intra-class differences and increase the inter-class variations for robust person matching. The proposed framework is evaluated on publicly available small scale (VIPeR, PRID450s, and GRID) and large scale (CUHK01, Market1501, and DukeMTMC-ReID) person Re-Id datasets. The experimental results show that the hybrid hand-crafted and deep features outperformed the existing state-of-the-art in approaches in the unsupervised paradigm.",
        "abstract": "Person Re-Identification (Re-Id) is among the main constituents of an automated visual surveillance system. It aims at finding out true matches of a given query person from a large repository of non-overlapping camera images/videos. In this paper, we have proposed an efficient Re-Id approach that is based on a highly discriminative hybrid person representation which combines the low-level hand-crafted appearance based features together with the mid-level attributes and semantic based deep features. The lowlevel hand crafted features are extracted by using hierarchical Gaussian and local histogram distributions in different color spaces. These features incorporate discriminative texture, shape and color information which is invariant to distractors, e.g., variations in pose, viewpoint and illumination, and so on. The mid-level attribute based deep features are extracted to incorporate contextual- and semantic-based information. The feature space is optimized and self-learned using cross-view quadratic discriminant analysis and multiple metric learning, with the aim to reduce the intra-class differences and increase the inter-class variations for robust person matching. The proposed framework is evaluated on publicly available small scale (VIPeR, PRID450s, and GRID) and large scale (CUHK01, Market1501, and DukeMTMC-ReID) person Re-Id datasets. The experimental results show that the hybrid hand-crafted and deep features outperformed the existing state-of-the-art in approaches in the unsupervised paradigm.",
        "authors": "Muhammad Moazam Fraz, Muhammad Shahzad, Nazia Perwaiz",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2882254",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, a novel method for the documentation and evaluation of the machine rooms in hospitals is presented. The approach is based on data acquired with a portable mobile mapping system (PMMS), GeoSlam Zeb-Revo, which has proved to be effective for three-dimensional (3D) mapping of indoor environments, as well as for 3D modeling of individual thermal and fluid-mechanics equipment. An automatic data processing workflow based on the extraction of quantitative and qualitative geometrical features from the point clouds provided by the PMMS is developed with the aim of evaluating the state and adequate distributions of machineries and, in this way, generating a complete three-dimensional map of the industrial environment to be used for maintenance, inspection, and inventory tasks in accordance with safety standards. The extracted parameters are statistically tested to evaluate the adequacy of the proposed methodology and, in this way, demonstrate its potential for the application in the context of hospital facilities.",
        "abstract": "In this paper, a novel method for the documentation and evaluation of the machine rooms in hospitals is presented. The approach is based on data acquired with a portable mobile mapping system (PMMS), GeoSlam Zeb-Revo, which has proved to be effective for three-dimensional (3D) mapping of indoor environments, as well as for 3D modeling of individual thermal and fluid-mechanics equipment. An automatic data processing workflow based on the extraction of quantitative and qualitative geometrical features from the point clouds provided by the PMMS is developed with the aim of evaluating the state and adequate distributions of machineries and, in this way, generating a complete three-dimensional map of the industrial environment to be used for maintenance, inspection, and inventory tasks in accordance with safety standards. The extracted parameters are statistically tested to evaluate the adequacy of the proposed methodology and, in this way, demonstrate its potential for the application in the context of hospital facilities.",
        "authors": "Diego González-Aguilera, Erica Nocerino, Manuel Rodríguez-Martín, Pablo Rodríguez-Gonzálvez",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884922",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Using the array technique, the multi-channel synthetic aperture radar (SAR) has the capability of high-resolution imaging, detection, and location of the ground moving targets, which is a powerful tool during the remote sensing of smart city. Recently, the theory of compressive sensing has been applied to radar imaging with data of compressive sampling, which can effectively reduce the burden of current radar system. In this paper, we focus on SAR moving targets imaging from sparse aperture (SA) data with accurate target motion compensation. The procedure of motion compensation is decomposed into two steps: the SPECAN processing in the range frequency and azimuth time domain is first applied and then followed by the residue component correction embedded into sparse imaging. To overcome the SA and target motion, a novel parametric sparse imaging approach is proposed by addressing the problems of Doppler ambiguity and phase errors. In the scheme, a parametric and dynamic dictionary is used to include these two important issues. Then, a modified orthogonal matching pursuit method is presented for high-quality imaging with Doppler ambiguity number estimation and phase error correction, which can deal with the case of multiple moving targets. Finally, experimental analysis is performed to confirm the effectiveness of the proposed algorithm.",
        "abstract": "Using the array technique, the multi-channel synthetic aperture radar (SAR) has the capability of high-resolution imaging, detection, and location of the ground moving targets, which is a powerful tool during the remote sensing of smart city. Recently, the theory of compressive sensing has been applied to radar imaging with data of compressive sampling, which can effectively reduce the burden of current radar system. In this paper, we focus on SAR moving targets imaging from sparse aperture (SA) data with accurate target motion compensation. The procedure of motion compensation is decomposed into two steps: the SPECAN processing in the range frequency and azimuth time domain is first applied and then followed by the residue component correction embedded into sparse imaging. To overcome the SA and target motion, a novel parametric sparse imaging approach is proposed by addressing the problems of Doppler ambiguity and phase errors. In the scheme, a parametric and dynamic dictionary is used to include these two important issues. Then, a modified orthogonal matching pursuit method is presented for high-quality imaging with Doppler ambiguity number estimation and phase error correction, which can deal with the case of multiple moving targets. Finally, experimental analysis is performed to confirm the effectiveness of the proposed algorithm.",
        "authors": "Gang Xu, Guobin Jing, Jialian Sheng, Junli Chen, Xin Lin, Yanyang Liu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884233",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "An earthquake is a natural disaster with great destructive power and the potential to cause severe damage to building structures. The rapid convergence of control systems is extremely important when a building structure suffers an earthquake. Here, an adaptive hyperbolic tangent sliding-mode control method is proposed to guarantee rapid convergence and eliminates the problem of chatter. Since seismic waves are uncertain, the proposed method combines traditional methodologies with adaptive compensation control. Simulation experiments show that the proposed method can effectively eliminate system chatter while maintaining rapid convergence, therefore effectively improving the seismic performance of the building structure.",
        "abstract": "An earthquake is a natural disaster with great destructive power and the potential to cause severe damage to building structures. The rapid convergence of control systems is extremely important when a building structure suffers an earthquake. Here, an adaptive hyperbolic tangent sliding-mode control method is proposed to guarantee rapid convergence and eliminates the problem of chatter. Since seismic waves are uncertain, the proposed method combines traditional methodologies with adaptive compensation control. Simulation experiments show that the proposed method can effectively eliminate system chatter while maintaining rapid convergence, therefore effectively improving the seismic performance of the building structure.",
        "authors": "Chunliang Zhang, Houyao Zhu, Jianhui Wang, Wenli Chen, Yunchang Huang, Zicong Chen",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883117",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Sports fans generate a large amount of tweets which reflect their opinions and feelings about what is happening during various sporting events. Given the popularity of football events, in this work, we focus on analyzing sentiment expressed by football fans through Twitter. These tweets reflect the changes in the fans’ sentiment as they watch the game and react to the events of the game, e.g., goal scoring, penalties, and so on. Collecting and examining the sentiment conveyed through these tweets will help to draw a complete picture which expresses fan interaction during a specific football event. The objective of this work is to propose a domain-specific approach for understanding sentiments expressed in football fans’ conversations. To achieve our goal, we start by developing a football-specific sentiment dataset which we label manually. We then utilize our dataset to automatically create a football-specific sentiment lexicon. Finally, we develop a sentiment classifier which is capable of recognizing sentiments expressed in football conversation. We conduct extensive experiments on our dataset to compare the performance of different learning algorithms in identifying the sentiment expressed in football related tweets. Our results show that our approach is effective in recognizing the fans’ sentiment during football events.",
        "abstract": "Sports fans generate a large amount of tweets which reflect their opinions and feelings about what is happening during various sporting events. Given the popularity of football events, in this work, we focus on analyzing sentiment expressed by football fans through Twitter. These tweets reflect the changes in the fans’ sentiment as they watch the game and react to the events of the game, e.g., goal scoring, penalties, and so on. Collecting and examining the sentiment conveyed through these tweets will help to draw a complete picture which expresses fan interaction during a specific football event. The objective of this work is to propose a domain-specific approach for understanding sentiments expressed in football fans’ conversations. To achieve our goal, we start by developing a football-specific sentiment dataset which we label manually. We then utilize our dataset to automatically create a football-specific sentiment lexicon. Finally, we develop a sentiment classifier which is capable of recognizing sentiments expressed in football conversation. We conduct extensive experiments on our dataset to compare the performance of different learning algorithms in identifying the sentiment expressed in football related tweets. Our results show that our approach is effective in recognizing the fans’ sentiment during football events.",
        "authors": "Abdulmotaleb El Saddik, Samah Aloufi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885117",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper proposes a method that extracts a feature set for accurate disease diagnosis from a feature (aptamer) array. Our method uses an artificial intelligence of the neural network and 10-fold crossvalidations and is verified by the p-value of the aptamer array response to specimens of 80 liver cancer patients and 310 healthy people. The proposed method is compared with the one-way ANOVA method in terms of accuracy, the number of features, and computing time to determine the feature set required to achieve the same accuracy. An increase in the number of features dramatically improves the diagnosis accuracy of the two methods for 2-10 features. The accuracies with 10 features are 93.5% and 87.5%, and the increases in accuracy per additional feature are 3.39% and 2.65% for our method and the one-way ANOVA, respectively. For the same accuracy, our method needs only 1/2-1/3 number of features of the ANOVA. An interesting statistical characteristic of cross-validation is that diagnostic accuracy saturates after 10000 cross-validations.",
        "abstract": "This paper proposes a method that extracts a feature set for accurate disease diagnosis from a feature (aptamer) array. Our method uses an artificial intelligence of the neural network and 10-fold crossvalidations and is verified by the p-value of the aptamer array response to specimens of 80 liver cancer patients and 310 healthy people. The proposed method is compared with the one-way ANOVA method in terms of accuracy, the number of features, and computing time to determine the feature set required to achieve the same accuracy. An increase in the number of features dramatically improves the diagnosis accuracy of the two methods for 2-10 features. The accuracies with 10 features are 93.5% and 87.5%, and the increases in accuracy per additional feature are 3.39% and 2.65% for our method and the one-way ANOVA, respectively. For the same accuracy, our method needs only 1/2-1/3 number of features of the ANOVA. An interesting statistical characteristic of cross-validation is that diagnostic accuracy saturates after 10000 cross-validations.",
        "authors": "Jusung Park, Sangman Kim",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884896",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Cross-department medical diagnosis processes typically involve several coordination patterns among different departments. In addition, medical task participants may come from different countries and use different languages, which makes diagnosis processes more difficult to be understood. In this paper, a multi-view and multi-language description generation method for cross-department medical diagnosis processes is proposed. We introduce a cross-department process view and an intra-department process view for cross-department medical diagnosis processes. Then, we present a multi-language description generation method for different views. The experimental results show that the multi-view and multi-language description of cross-department medical diagnosis processes not only enable medical participants to understand models from different perspectives, but also help participants that from different countries with different languages understand medical process models much easier.",
        "abstract": "Cross-department medical diagnosis processes typically involve several coordination patterns among different departments. In addition, medical task participants may come from different countries and use different languages, which makes diagnosis processes more difficult to be understood. In this paper, a multi-view and multi-language description generation method for cross-department medical diagnosis processes is proposed. We introduce a cross-department process view and an intra-department process view for cross-department medical diagnosis processes. Then, we present a multi-language description generation method for different views. The experimental results show that the multi-view and multi-language description of cross-department medical diagnosis processes not only enable medical participants to understand models from different perspectives, but also help participants that from different countries with different languages understand medical process models much easier.",
        "authors": "Chao Li, Cong Liu, Guiyuan Yuan, Hua Duan, Qingtian Zeng, Weijian Ni",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2882789",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The main techniques used for failure analysis in induction motors are limited because they are not capable of detecting all the faults present in an induction motor. However, a complementary technique that aids in fault diagnosis in induction motors is infrared imaging. This paper proposes an automatic methodology based on infrared imaging for thermal condition monitoring and failure analysis in induction motors and the kinematic chain; it can be considered an alternative tool for classical infrared imaging inspection procedures and for monitoring and failure analysis techniques in an induction motor in particular. The proposed methodology detects the region of interest using automatic image segmentation by means of the Otsu thresholding method, where it performs the feature extraction of temperatures for the thermal analysis of induction motors. The methodology is based on the standard ASTM E1934-99a for fault diagnosis in induction motors. To demonstrate the efficiency of the proposed methodology, this paper presents the failure analysis of three fault conditions in an induction motor: a broken rotor bar, bearing damage, and misalignment.",
        "abstract": "The main techniques used for failure analysis in induction motors are limited because they are not capable of detecting all the faults present in an induction motor. However, a complementary technique that aids in fault diagnosis in induction motors is infrared imaging. This paper proposes an automatic methodology based on infrared imaging for thermal condition monitoring and failure analysis in induction motors and the kinematic chain; it can be considered an alternative tool for classical infrared imaging inspection procedures and for monitoring and failure analysis techniques in an induction motor in particular. The proposed methodology detects the region of interest using automatic image segmentation by means of the Otsu thresholding method, where it performs the feature extraction of temperatures for the thermal analysis of induction motors. The methodology is based on the standard ASTM E1934-99a for fault diagnosis in induction motors. To demonstrate the efficiency of the proposed methodology, this paper presents the failure analysis of three fault conditions in an induction motor: a broken rotor bar, bearing damage, and misalignment.",
        "authors": "Emmanuel Resendiz-Ochoa, Juan Primo Benitez-Rangel, Luis Alberto Morales-Hernandez, Rene De J. Romero-Troncoso, Roque A. Osornio-Rios",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883988",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "On the basis of the assumption that no resource failure occurs, a variety of deadlock control policies have been developed for automated manufacturing systems (AMSs). However, in practical manufacturing systems, the occurrence of resource failures is always inescapable. In case, if resources fail to work, the existing deadlock control strategies can no longer be applied to the changed system; therefore, redesigning a new strategy is necessary. Because of their powerful modeling capabilities, Petri nets are used to model the considered AMSs allowing multi-type and multi-quantity resource acquisition. This paper focuses on extending a deadlock prevention method to be applied to AMSs with unreliable resources. Strict minimal siphons are controlled by added control places (monitors) to ensure the system's liveness. To prevent blocking issues caused by resource failures, we develop a set of shared resource constraints represented by a set of inequalities based on the minimal resource requirements of processes and the capacity of shared resources. Robust monitors are designed for them to limit the distribution of tokens in unreliable neighborhood places. Our objective is to control resource allocation such that those parts not necessarily requiring any failed resource can continue progressing smoothly even if some unreliable resources break down. Examples are given to elucidate our proposed method clearly.",
        "abstract": "On the basis of the assumption that no resource failure occurs, a variety of deadlock control policies have been developed for automated manufacturing systems (AMSs). However, in practical manufacturing systems, the occurrence of resource failures is always inescapable. In case, if resources fail to work, the existing deadlock control strategies can no longer be applied to the changed system; therefore, redesigning a new strategy is necessary. Because of their powerful modeling capabilities, Petri nets are used to model the considered AMSs allowing multi-type and multi-quantity resource acquisition. This paper focuses on extending a deadlock prevention method to be applied to AMSs with unreliable resources. Strict minimal siphons are controlled by added control places (monitors) to ensure the system's liveness. To prevent blocking issues caused by resource failures, we develop a set of shared resource constraints represented by a set of inequalities based on the minimal resource requirements of processes and the capacity of shared resources. Robust monitors are designed for them to limit the distribution of tokens in unreliable neighborhood places. Our objective is to control resource allocation such that those parts not necessarily requiring any failed resource can continue progressing smoothly even if some unreliable resources break down. Examples are given to elucidate our proposed method clearly.",
        "authors": "Hesuan Hu, Nan Du",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885116",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In magnetic resonance (MR) images, detection of focal cortical dysplasia (FCD) lesion as a main pathological cue of epilepsy is challenging because of the variability in the presentation of FCD lesions. Existing algorithms appear to have sufficient sensitivity in detecting lesions but also generate large numbers of false-positive (FP) results. In this paper, we propose a multiple classifier fusion and optimization schemes to automatically detect FCD lesions in MR images with reduced FPs through constructing an objective function based on the F-score. Thus, the proposed scheme obtains an improved tradeoff between minimizing FPs and maximizing true positives. The optimization is achieved by incorporating the genetic algorithm into the work scheme. Hence, the contribution of weighting coefficients to different classifications can be effectively determined. The resultant optimized weightings are applied to fuse the classification results. A set of six typical FCD features and six corresponding Z-score maps are evaluated through the mean F-score from multiple classifiers for each feature. From the experimental results, the proposed scheme can automatically detect FCD lesions in 9 out of 10 patients while correctly classifying 31 healthy controls. The proposed scheme acquires a lower FP rate and a higher F-score in comparison with two state-of-the-art methods.",
        "abstract": "In magnetic resonance (MR) images, detection of focal cortical dysplasia (FCD) lesion as a main pathological cue of epilepsy is challenging because of the variability in the presentation of FCD lesions. Existing algorithms appear to have sufficient sensitivity in detecting lesions but also generate large numbers of false-positive (FP) results. In this paper, we propose a multiple classifier fusion and optimization schemes to automatically detect FCD lesions in MR images with reduced FPs through constructing an objective function based on the F-score. Thus, the proposed scheme obtains an improved tradeoff between minimizing FPs and maximizing true positives. The optimization is achieved by incorporating the genetic algorithm into the work scheme. Hence, the contribution of weighting coefficients to different classifications can be effectively determined. The resultant optimized weightings are applied to fuse the classification results. A set of six typical FCD features and six corresponding Z-score maps are evaluated through the mean F-score from multiple classifiers for each feature. From the experimental results, the proposed scheme can automatically detect FCD lesions in 9 out of 10 patients while correctly classifying 31 healthy controls. The proposed scheme acquires a lower FP rate and a higher F-score in comparison with two state-of-the-art methods.",
        "authors": "Asli Kumcu, Bart Goossens, Danni Ai, Jian Yang, Ljiljana Platiš, Xiaoxia Qu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883583",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "As data sets continue to grow in size, visualization has become a vitally important tool for extracting meaningful knowledge. Scattered point data, which are unordered sets of point coordinates with associated measured values, arise in many contexts, such as scientific experiments, sensor networks, and numerical simulations. In this paper, we present a method for visualizing such scattered point data sets. Our method is based on volume ray casting, and distinguishes itself by operating directly on the unstructured samples, rather than resampling them to form voxels. We estimate the intensity of the volume at points along the rays by interpolation using nearby samples, taking advantage of an octree to facilitate efficient range search. The method has been implemented on multi-core CPUs, GPUs as well as multi-GPU systems. Our source code is available under a BSD license at https://github.com/acelster/scatter-pt-viz To test our method, actual X-ray diffraction data sets have been used, consisting of up to 240 million data points. We are able to generate images of good quality and achieve interactive frame rates in favorable cases. The GPU implementation (Nvidia Tesla K20) achieves speedups of 8–14 compared with our parallelized CPU version (4-core, hyperthreaded Intel i7 3770 K).",
        "abstract": "As data sets continue to grow in size, visualization has become a vitally important tool for extracting meaningful knowledge. Scattered point data, which are unordered sets of point coordinates with associated measured values, arise in many contexts, such as scientific experiments, sensor networks, and numerical simulations. In this paper, we present a method for visualizing such scattered point data sets. Our method is based on volume ray casting, and distinguishes itself by operating directly on the unstructured samples, rather than resampling them to form voxels. We estimate the intensity of the volume at points along the rays by interpolation using nearby samples, taking advantage of an octree to facilitate efficient range search. The method has been implemented on multi-core CPUs, GPUs as well as multi-GPU systems. Our source code is available under a BSD license at https://github.com/acelster/scatter-pt-viz To test our method, actual X-ray diffraction data sets have been used, consisting of up to 240 million data points. We are able to generate images of good quality and achieve interactive frame rates in favorable cases. The GPU implementation (Nvidia Tesla K20) achieves speedups of 8–14 compared with our parallelized CPU version (4-core, hyperthreaded Intel i7 3770 K).",
        "authors": "Anne C. Elster, Dag W. Breiby, Jostein BØ Fløystad, Thomas L. Falch",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2281080",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Cone-beam-computed tomography (CBCT) has shown enormous potential in recent years, but it is limited by severe scatter artifacts. This paper proposes a scatter-correction algorithm based on a deep convolutional neural network to reduce artifacts for CBCT in an image-guided radiation therapy (IGRT) system. A two-step registration method that is essential in our algorithm is implemented to preprocess data before training. The testing result on real data acquired from the IGRT system demonstrates the ability of our approach to learn artifacts distribution. Furthermore, the proposed method can be applied to enhance the performance on such applications as dose estimation and segmentation.",
        "abstract": "Cone-beam-computed tomography (CBCT) has shown enormous potential in recent years, but it is limited by severe scatter artifacts. This paper proposes a scatter-correction algorithm based on a deep convolutional neural network to reduce artifacts for CBCT in an image-guided radiation therapy (IGRT) system. A two-step registration method that is essential in our algorithm is implemented to preprocess data before training. The testing result on real data acquired from the IGRT system demonstrates the ability of our approach to learn artifacts distribution. Furthermore, the proposed method can be applied to enhance the performance on such applications as dose estimation and segmentation.",
        "authors": "Chengyuan Yang, Haibo Li, Shipeng Xie, Zijian Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884704",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The primary objective of this paper is to explore the spatial analysis of bikeshare ridership with a consideration of the diversity across different station categories using smart card data and points of interests (POIs) data. The bikeshare trip records were obtained from the Citi Bike system of New York City. The POI data in the vicinity of each station were collected through the Google Places API. K-means clustering method was employed to classify the bikeshare stations into five categories. Then, the geographically weighted regression (GWR) method was applied to establish the relationship between bikeshare ridership and various kinds of influencing factors. To account for the diversity across different station categories, five separate GWR models for each station category were developed and compared with the joint model of all station categories. The results of likelihood ratio test confirmed the superiority and importance of building separate models for each bikeshare station category instead of a joint model. In addition, all the developed bikeshare ridership models were applied to predict the ridership of the newly opened stations in the next year. The results were indicated that the prediction performance of separate bikeshare ridership models was generally better than that of the joint model. The findings of this paper could help transportation agency to develop specific planning and management strategies for each station category of the entire bikesharing system.",
        "abstract": "The primary objective of this paper is to explore the spatial analysis of bikeshare ridership with a consideration of the diversity across different station categories using smart card data and points of interests (POIs) data. The bikeshare trip records were obtained from the Citi Bike system of New York City. The POI data in the vicinity of each station were collected through the Google Places API. K-means clustering method was employed to classify the bikeshare stations into five categories. Then, the geographically weighted regression (GWR) method was applied to establish the relationship between bikeshare ridership and various kinds of influencing factors. To account for the diversity across different station categories, five separate GWR models for each station category were developed and compared with the joint model of all station categories. The results of likelihood ratio test confirmed the superiority and importance of building separate models for each bikeshare station category instead of a joint model. In addition, all the developed bikeshare ridership models were applied to predict the ridership of the newly opened stations in the next year. The results were indicated that the prediction performance of separate bikeshare ridership models was generally better than that of the joint model. The findings of this paper could help transportation agency to develop specific planning and management strategies for each station category of the entire bikesharing system.",
        "authors": "Hao Zhang, Jie Bao, Xiaomeng Shi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883462",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Accurate wind energy assessments require wind speed (WS) at the hub height. The cost of WS measurements grows enormously with height. This paper utilizes deep neural network (DNN) algorithm for the extrapolation of the WS to higher heights based on measured values at lower heights. LiDAR measurements at lower heights are used for training the system and at higher heights for performance analysis. These measurements are made at 10, 20, ..., and 120 m heights. First, the measured WS values at 10-40 m were used to extrapolate values up to 120 m. In the second scenario, the WS at 10-50 m were used to extrapolate values up to 120 m. This continued until the last scenario, in which the WS at 10-100 m were used to estimate values at 110 and 120 m. A relationship between heights of measurements and the accuracy of the WS estimation at hub height is presented. The WS extrapolated using the present approach is compared with the measured values and with local wind shear exponent (LWSE)-based extrapolated WS. Furthermore, to analyze the performance of the DNN relative to other machine learning methods, we compared its performance with that of classical feedforward artificial neural networks trained using a genetic algorithm to find the initial weights and the Levemberg-Marquardt (LM) method (GANN) for training. The mean absolute percent error between measured and extrapolated WS at height 120 m based on measurements between 10-50 musing DNN, GANN, and LWSE are 9.65%, 12.77%, and 9.79%, respectively.",
        "abstract": "Accurate wind energy assessments require wind speed (WS) at the hub height. The cost of WS measurements grows enormously with height. This paper utilizes deep neural network (DNN) algorithm for the extrapolation of the WS to higher heights based on measured values at lower heights. LiDAR measurements at lower heights are used for training the system and at higher heights for performance analysis. These measurements are made at 10, 20, ..., and 120 m heights. First, the measured WS values at 10-40 m were used to extrapolate values up to 120 m. In the second scenario, the WS at 10-50 m were used to extrapolate values up to 120 m. This continued until the last scenario, in which the WS at 10-100 m were used to estimate values at 110 and 120 m. A relationship between heights of measurements and the accuracy of the WS estimation at hub height is presented. The WS extrapolated using the present approach is compared with the measured values and with local wind shear exponent (LWSE)-based extrapolated WS. Furthermore, to analyze the performance of the DNN relative to other machine learning methods, we compared its performance with that of classical feedforward artificial neural networks trained using a genetic algorithm to find the initial weights and the Levemberg-Marquardt (LM) method (GANN) for training. The mean absolute percent error between measured and extrapolated WS at height 120 m based on measurements between 10-50 musing DNN, GANN, and LWSE are 9.65%, 12.77%, and 9.79%, respectively.",
        "authors": "M. A. Mohandes, S. Rehman",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883677",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The scattering mechanisms of dihedral corner reflectors have attracted great attention in the past decades. However, the scattering centers of dihedral corners are not discussed systematically. In this paper, the multi-bouncing scattering mechanism of dihedral corners is analyzed in detail based on the scattering characteristics of polygon plates and ray-tracing technique. Furthermore, the coupling scattering center (CSC) is proposed as a detailed explanation of the multi-bouncing scattering mechanism of dihedral corners and the locations of CSCs in the high-resolution range profile are deduced based on the monostatic-and-bistatic equivalence theorem. CSCs give a precise description of target structures and can improve the ability of radar target recognition. Finally, the simulation results via electromagnetic computation validate the theoretical analysis.",
        "abstract": "The scattering mechanisms of dihedral corner reflectors have attracted great attention in the past decades. However, the scattering centers of dihedral corners are not discussed systematically. In this paper, the multi-bouncing scattering mechanism of dihedral corners is analyzed in detail based on the scattering characteristics of polygon plates and ray-tracing technique. Furthermore, the coupling scattering center (CSC) is proposed as a detailed explanation of the multi-bouncing scattering mechanism of dihedral corners and the locations of CSCs in the high-resolution range profile are deduced based on the monostatic-and-bistatic equivalence theorem. CSCs give a precise description of target structures and can improve the ability of radar target recognition. Finally, the simulation results via electromagnetic computation validate the theoretical analysis.",
        "authors": "Feng Zhao, Qihua Wu, Shunping Xiao, Xiaofeng Ai, Zhiming Xu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885566",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The locomotion of lizards is characterized by lateral bending of the body, which is distinct from quadrupedal mammals such as dogs. We propose a method to control a physically simulated iguana character as a representative species of lizards, which can move while physically interacting with the environment, and reproduces the flexible characteristics of a lizard in real time. The main challenges lie in expressing the deformable characteristics of an iguana and adapting low-quality captured motions to various terrain conditions and iguana poses. Our iguana character is designed as a soft-body character, which models the elasticity of an iguana body so that it can express flexible and realistic motions. Applying the motion capture data obtained from an iguana is problematic because it is captured using only a sparse set of markers in an environment different to the simulation environment. To resolve these problems, we transform the low-quality captured motion into full-body motion and adapt it to the terrain in real time using our motion adaptation algorithm. To control the various movements of an iguana, a motion graph is constructed to choose an appropriate motion depending on the situation. The chosen reference motion is adapted to the local terrain, which has irregular height, in real time. A soft-body iguana model is then simulated by physically tracking the time-varying reference motion. We demonstrate that our approach can generate natural and flexible movements of an iguana on hilly terrain.",
        "abstract": "The locomotion of lizards is characterized by lateral bending of the body, which is distinct from quadrupedal mammals such as dogs. We propose a method to control a physically simulated iguana character as a representative species of lizards, which can move while physically interacting with the environment, and reproduces the flexible characteristics of a lizard in real time. The main challenges lie in expressing the deformable characteristics of an iguana and adapting low-quality captured motions to various terrain conditions and iguana poses. Our iguana character is designed as a soft-body character, which models the elasticity of an iguana body so that it can express flexible and realistic motions. Applying the motion capture data obtained from an iguana is problematic because it is captured using only a sparse set of markers in an environment different to the simulation environment. To resolve these problems, we transform the low-quality captured motion into full-body motion and adapt it to the terrain in real time using our motion adaptation algorithm. To control the various movements of an iguana, a motion graph is constructed to choose an appropriate motion depending on the situation. The chosen reference motion is adapted to the local terrain, which has irregular height, in real time. A soft-body iguana model is then simulated by physically tracking the time-varying reference motion. We demonstrate that our approach can generate natural and flexible movements of an iguana on hilly terrain.",
        "authors": "Hoimin Kim, Taesoo Kwon, Yoonsang Lee",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884493",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In order to meet the rigorous motion accuracy requirement and efficiently utilize the repetitive-task characteristics in modern precision industry, this paper concentrates on the comprehensive research of model-based data-driven learning adaptive robust control (LARC) strategy for precision mechatronic motion systems. The proposed LARC can achieve not only excellent transient/steady-state tracking performance but also adaptation ability and disturbance robustness. Specifically, the LARC strategy contains robust feedback term, adaptive model compensation term, and iterative learning term. Herein, the former two terms are designed based on the system dynamic model under parametric uncertainty and uncertain nonlinearity, and the data-driven iterative learning term is synthesized to generate optimal input to adjust the optimal reference. The whole controller design procedure and stability is presented, while the reason for the practically achievable performance of LARC is analyzed. Comparative experiments, among proportional-integral-differential, adaptive robust control, iterative learning control, and the proposed LARC, are conducted on a developed linear motor stage. The experimental results consistently validate that the proposed LARC scheme simultaneously achieves excellent transient/steady-state tracking performance, parametric adaptation ability, and disturbance robustness. The LARC strategy essentially provides an effective control technology with good potential in industrial applications.",
        "abstract": "In order to meet the rigorous motion accuracy requirement and efficiently utilize the repetitive-task characteristics in modern precision industry, this paper concentrates on the comprehensive research of model-based data-driven learning adaptive robust control (LARC) strategy for precision mechatronic motion systems. The proposed LARC can achieve not only excellent transient/steady-state tracking performance but also adaptation ability and disturbance robustness. Specifically, the LARC strategy contains robust feedback term, adaptive model compensation term, and iterative learning term. Herein, the former two terms are designed based on the system dynamic model under parametric uncertainty and uncertain nonlinearity, and the data-driven iterative learning term is synthesized to generate optimal input to adjust the optimal reference. The whole controller design procedure and stability is presented, while the reason for the practically achievable performance of LARC is analyzed. Comparative experiments, among proportional-integral-differential, adaptive robust control, iterative learning control, and the proposed LARC, are conducted on a developed linear motor stage. The experimental results consistently validate that the proposed LARC scheme simultaneously achieves excellent transient/steady-state tracking performance, parametric adaptation ability, and disturbance robustness. The LARC strategy essentially provides an effective control technology with good potential in industrial applications.",
        "authors": "Chuxiong Hu, Suqin He, Yu Zhu, Ze Wang, Zhipeng Hu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884947",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Function recognition is the preliminary step of the reverse analysis. It is the premise of binary reuse, generating control flow graphs, and performing semantic analysis. There are still many shortcomings in the application of current function identification tools, including cross-platform support, efficiency, and resource usage. To improve the accuracy and efficiency of function recognition in embedded system firmware, this paper proposes a new function recognition algorithm FRwithMBA based on the structure information. With the help of function call resolve, FRwithMBA identifies functions by determining the termination position of the function based on branches instruction and termination signatures of the function. In the experiment, we evaluated FRwithMBA against IDA Pro, radare2, and angr with the router binary Cisco IOS and found that FRwithMBA is more effective in identifying functions in the stripped binary under PPC and MIPS than the other tools. In terms of recognition effectiveness, FRwithMBA performs a little better than IDA Pro, but the results of radare2 and angr are poor. For a 14-MB binary, the parsing time of IDA Pro is 178 s, radare2 uses 376 s, angr uses 1896 s, and FRwithMBA uses 25 s. When processing 220-MB Executable and Linkable Format files, FRwithMBA can identify nearly 420 000 functions in about 240 s, IDA takes 2754 s, and both radare2 and angr get a fault. In the experiment, the recall of FRwithMBA reached 99.99% and the precision reached 99.7%. It is better than the other tools. In other words, FRwithMBA has more speed, higher accuracy, and less resource occupation.",
        "abstract": "Function recognition is the preliminary step of the reverse analysis. It is the premise of binary reuse, generating control flow graphs, and performing semantic analysis. There are still many shortcomings in the application of current function identification tools, including cross-platform support, efficiency, and resource usage. To improve the accuracy and efficiency of function recognition in embedded system firmware, this paper proposes a new function recognition algorithm FRwithMBA based on the structure information. With the help of function call resolve, FRwithMBA identifies functions by determining the termination position of the function based on branches instruction and termination signatures of the function. In the experiment, we evaluated FRwithMBA against IDA Pro, radare2, and angr with the router binary Cisco IOS and found that FRwithMBA is more effective in identifying functions in the stripped binary under PPC and MIPS than the other tools. In terms of recognition effectiveness, FRwithMBA performs a little better than IDA Pro, but the results of radare2 and angr are poor. For a 14-MB binary, the parsing time of IDA Pro is 178 s, radare2 uses 376 s, angr uses 1896 s, and FRwithMBA uses 25 s. When processing 220-MB Executable and Linkable Format files, FRwithMBA can identify nearly 420 000 functions in about 240 s, IDA takes 2754 s, and both radare2 and angr get a fault. In the experiment, the recall of FRwithMBA reached 99.99% and the precision reached 99.7%. It is better than the other tools. In other words, FRwithMBA has more speed, higher accuracy, and less resource occupation.",
        "authors": "Da Xiao, Long Liu, Shengli Liu, Xiaokang Yin",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883973",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper explores the design of a high-level mission planner and controller for managing unmanned aerial vehicles (UAVs) fighting a wildfire through the utilization of reactive synthesis and dynamic allocation of the UAVs as resources for the fire. The contribution of this paper is a study on the hierarchal integration of reactive synthesis, used for assuring desired system design traits, and dynamic allocation, used for making heuristic-based decisions. Reactive synthesis provides a formal means of guaranteeing the UAVs' transition to areas of fire, refill of water, and land as defined by the linear temporal logic specifications. Dynamic allocation coordinates the behavior of multiple UAVs through assignments to regions of fire based on a cost function that takes into consideration the fire locations relative to a UAV, distance to the domain edge, wind speed and direction, and the amount of suppressant already present. The use of receding horizons in the reactive synthesis formulation incorporates horizons defined only through spatial distance from a goal. Modifications to these horizon definitions guarantee that the scenario still maintains the overall realizability of the formal specifications after the inclusion of static obstacles. This paper shows the effectiveness of multiple UAV fleets in slowing down the progression of fires from reaching the domain edge through six fire scenarios. At last, our results and successful application demonstrate the utilization of reactive synthesis in larger task spaces and the implications of abstracting UAV transitions for use in formal methods.",
        "abstract": "This paper explores the design of a high-level mission planner and controller for managing unmanned aerial vehicles (UAVs) fighting a wildfire through the utilization of reactive synthesis and dynamic allocation of the UAVs as resources for the fire. The contribution of this paper is a study on the hierarchal integration of reactive synthesis, used for assuring desired system design traits, and dynamic allocation, used for making heuristic-based decisions. Reactive synthesis provides a formal means of guaranteeing the UAVs' transition to areas of fire, refill of water, and land as defined by the linear temporal logic specifications. Dynamic allocation coordinates the behavior of multiple UAVs through assignments to regions of fire based on a cost function that takes into consideration the fire locations relative to a UAV, distance to the domain edge, wind speed and direction, and the amount of suppressant already present. The use of receding horizons in the reactive synthesis formulation incorporates horizons defined only through spatial distance from a goal. Modifications to these horizon definitions guarantee that the scenario still maintains the overall realizability of the formal specifications after the inclusion of static obstacles. This paper shows the effectiveness of multiple UAV fleets in slowing down the progression of fires from reaching the domain edge through six fire scenarios. At last, our results and successful application demonstrate the utilization of reactive synthesis in larger task spaces and the implications of abstracting UAV transitions for use in formal methods.",
        "authors": "Estefany Carrillo, Huan Xu, Joshua A. Shaffer",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885455",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Biometric template aging is defined as an increase in recognition error rate with increased time since enrollment. It is believed that template aging does not occur for iris recognition. Several research groups, however, have recently reported experimental results showing that iris template aging does occur. This template aging effect manifests as a shift in the authentic distribution, resulting in an increased false non-match rate. Analyzing results from a three-year time-lapse data set, we find∼150%increase in the false non-match rate at a decision threshold representing a one in two million false match rate. We summarize several known elements of eye aging that could contribute to template aging, including age-related change in pupil dilation. Finally, we discuss various steps that can control the template aging effect in typical identity verification applications.",
        "abstract": "Biometric template aging is defined as an increase in recognition error rate with increased time since enrollment. It is believed that template aging does not occur for iris recognition. Several research groups, however, have recently reported experimental results showing that iris template aging does occur. This template aging effect manifests as a shift in the authentic distribution, resulting in an increased false non-match rate. Analyzing results from a three-year time-lapse data set, we find∼150%increase in the false non-match rate at a decision threshold representing a one in two million false match rate. We summarize several known elements of eye aging that could contribute to template aging, including age-related change in pupil dilation. Finally, we discuss various steps that can control the template aging effect in typical identity verification applications.",
        "authors": "Estefan Ortiz, Kevin W. Bowyer, Samuel P. Fenker",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2262916",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A functional brain network has attracted much attention due to its capability of characterizing the functional connectivity patterns of the brain. The existing methods for network construction usually rely on the conventional measures such as Pearson correlation to determine the pairwise similarity between each pair of brain regions, thus ignoring the global structure relationship and the information communication flow among different brain regions. To this end, this paper proposes a directional brain network construction method based on effective distance. Specifically, the effective distance can capture the hidden relevance pattern among all the brain regions and provide a directional functional network reflecting information propagation paths among the functional brain areas simultaneously. When estimating the probability of the direction and the strength of the connectivity between two regions, the structure information characterized by their neighbors is also considered in our method. The functional brain network produced by our method is more flexible in uncovering physiological mechanisms of the brain compared with the conventional undirected network. The experiments on two fMRI data analysis tasks, i.e., disease diagnosis and cognitive state detection, show that our method outperforms the conventional functional brain network approaches, including Pearson correlation, structural equation modeling, and sparse representation-based method.",
        "abstract": "A functional brain network has attracted much attention due to its capability of characterizing the functional connectivity patterns of the brain. The existing methods for network construction usually rely on the conventional measures such as Pearson correlation to determine the pairwise similarity between each pair of brain regions, thus ignoring the global structure relationship and the information communication flow among different brain regions. To this end, this paper proposes a directional brain network construction method based on effective distance. Specifically, the effective distance can capture the hidden relevance pattern among all the brain regions and provide a directional functional network reflecting information propagation paths among the functional brain areas simultaneously. When estimating the probability of the direction and the strength of the connectivity between two regions, the structure information characterized by their neighbors is also considered in our method. The functional brain network produced by our method is more flexible in uncovering physiological mechanisms of the brain compared with the conventional undirected network. The experiments on two fMRI data analysis tasks, i.e., disease diagnosis and cognitive state detection, show that our method outperforms the conventional functional brain network approaches, including Pearson correlation, structural equation modeling, and sparse representation-based method.",
        "authors": "Daoqiang Zhang, Jiuwen Zhu, Mingxia Liu, Qi Zhu, Xijia Xu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884739",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The application of big data in biology has been a hot trend in the research of bioinformatics. High throughput sequencing and large-scale biology network contribute massive valuable data for the complex biology analysis, in which genome-scale metabolic network (GSMN) has become an important tool for the systematic analysis of metabolism in various organisms. We reconstructed a GSMN model based on the high-throughput transcriptome sequencing of Eriocheir sinensis hepatopancreas. The model contains 1471 reactions, 1882 unigenes, and 1400 metabolites distributed in 101 pathways and 11 subsystems. Functional module analysis indicates that the metabolic modules in the network is in consistence with the functions of hepatopancreas for lipid metabolism, amino acid metabolism, and other related metabolic processes. Differential analysis of the hepatopancreas reveals that the eyestalk-ablation has the greatest effect on the lipid metabolism pathway, with a total of 66 related unigenes differentially expressed, which agrees with the functions of the hepatopancreas for nutrient absorption, storage, and utilization. The GSMN supplies a novel dataset of the metabolic information in E.sinensis to facilitate the further analysis of the metabolic mechanism of E.sinensis and other aquatic crustaceans. Besides, it also provides a valuable reference for the studies on regulation mechanism of eyestalk on hepatopancreas in E.sinensis.",
        "abstract": "The application of big data in biology has been a hot trend in the research of bioinformatics. High throughput sequencing and large-scale biology network contribute massive valuable data for the complex biology analysis, in which genome-scale metabolic network (GSMN) has become an important tool for the systematic analysis of metabolism in various organisms. We reconstructed a GSMN model based on the high-throughput transcriptome sequencing of Eriocheir sinensis hepatopancreas. The model contains 1471 reactions, 1882 unigenes, and 1400 metabolites distributed in 101 pathways and 11 subsystems. Functional module analysis indicates that the metabolic modules in the network is in consistence with the functions of hepatopancreas for lipid metabolism, amino acid metabolism, and other related metabolic processes. Differential analysis of the hepatopancreas reveals that the eyestalk-ablation has the greatest effect on the lipid metabolism pathway, with a total of 66 related unigenes differentially expressed, which agrees with the functions of the hepatopancreas for nutrient absorption, storage, and utilization. The GSMN supplies a novel dataset of the metabolic information in E.sinensis to facilitate the further analysis of the metabolic mechanism of E.sinensis and other aquatic crustaceans. Besides, it also provides a valuable reference for the studies on regulation mechanism of eyestalk on hepatopancreas in E.sinensis.",
        "authors": "Bin Wang, Jinsheng Sun, Lingxuan Zhao, Tong Hao, Xin Feng",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885005",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Recent years have witnessed significant advances in indoor positioning. Existing approaches either require cumbersome site surveys or customized hardware, thus hindering their popularity. In this paper, we propose Stationary peers-Assisted indoor Positioning (SAP), a practical indoor positioning solution that is highly scalable and easy-to-deploy. SAP greatly alleviates the pain of fingerprint collection by smartly leveraging the relatively stationary people, namely stationary peers that are largely available in common indoor environments to assist positioning. Once SPs’ locations and the relative distance between SPs and a target are obtained, SAP can apply trilateration to locate the target. SAP incorporates three key modules: a novel accelerometer-based filter that can accurately identify SP, an enhanced fingerprint-based positioning method that can accurately pinpoint SPs’ locations, and a robust acoustic ranging method. We implement a prototype of SAP on the Android platform and evaluate its performance in representative real-world environments. SAP achieves an 80% positioning error of 2.2 m, which is comparable to the most existing smartphone-assisted approaches.",
        "abstract": "Recent years have witnessed significant advances in indoor positioning. Existing approaches either require cumbersome site surveys or customized hardware, thus hindering their popularity. In this paper, we propose Stationary peers-Assisted indoor Positioning (SAP), a practical indoor positioning solution that is highly scalable and easy-to-deploy. SAP greatly alleviates the pain of fingerprint collection by smartly leveraging the relatively stationary people, namely stationary peers that are largely available in common indoor environments to assist positioning. Once SPs’ locations and the relative distance between SPs and a target are obtained, SAP can apply trilateration to locate the target. SAP incorporates three key modules: a novel accelerometer-based filter that can accurately identify SP, an enhanced fingerprint-based positioning method that can accurately pinpoint SPs’ locations, and a robust acoustic ranging method. We implement a prototype of SAP on the Android platform and evaluate its performance in representative real-world environments. SAP achieves an 80% positioning error of 2.2 m, which is comparable to the most existing smartphone-assisted approaches.",
        "authors": "Chao Cai, Jiangchuan Liu, Menglan Hu, Xiaoqiang Ma, Yang Yang, Zhetao Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883800",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper focuses on the state feedback stabilization problem for a class of discrete-time singular Markov jump systems with repeated scalar nonlinearities. First, on the basis of the implicit function theorem and the diagonally dominant Lyapunov approach, a sufficient condition is obtained, which ensures the regularity, causality, uniqueness of solution in the neighbourhood of the origin, and stochastic stability for the system under consideration. Moreover, by employing some lemmas and matrix inequalities, the sufficient condition is changed into a set of linear matrix inequalities. Then, the procedures of designing the state feedback controller are given. Eventually, three examples are presented to show the validness of the proposed approach.",
        "abstract": "This paper focuses on the state feedback stabilization problem for a class of discrete-time singular Markov jump systems with repeated scalar nonlinearities. First, on the basis of the implicit function theorem and the diagonally dominant Lyapunov approach, a sufficient condition is obtained, which ensures the regularity, causality, uniqueness of solution in the neighbourhood of the origin, and stochastic stability for the system under consideration. Moreover, by employing some lemmas and matrix inequalities, the sufficient condition is changed into a set of linear matrix inequalities. Then, the procedures of designing the state feedback controller are given. Eventually, three examples are presented to show the validness of the proposed approach.",
        "authors": "Jiaming Tian, Shuping Ma",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883980",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Indoor target tracking is a crucial application of indoor localization systems. Wi-Fi fingerprint-based target tracking/positioning has been extensively studied due to its deployability under pervasive indoor wireless local area networks. However, the majority of fingerprint-based schemes adopted the static radio map and did not make full use of continuous motion information of the target. Therefore, we build a ubiquitous indoor tracking/localization system that can maintain a dynamic fingerprint database at low cost in this paper. First, we propose an optimal target tracking algorithm exploiting the historical data of the target location information to help improve the accuracy, which adopts a modified particle filter algorithm to reduce the number of samples and computing overhead. Second, we present a database self-update method according to trajectory continuity, which is employed to update fingerprint database dynamically for keeping the system robust. Finally, we tested the optimal target tracking based on dynamic fingerprint algorithm (OTTDF) in a complex laboratory area with diverse target motion conditions and various obstacles, our experimental results indicate that the OTTDF scheme successfully handles complex indoor structure, including different target motion state, signal fingerprint changes caused by obstacles, simultaneously provides better performance in localization cost, and localization/tracking accuracy in indoor wireless network.",
        "abstract": "Indoor target tracking is a crucial application of indoor localization systems. Wi-Fi fingerprint-based target tracking/positioning has been extensively studied due to its deployability under pervasive indoor wireless local area networks. However, the majority of fingerprint-based schemes adopted the static radio map and did not make full use of continuous motion information of the target. Therefore, we build a ubiquitous indoor tracking/localization system that can maintain a dynamic fingerprint database at low cost in this paper. First, we propose an optimal target tracking algorithm exploiting the historical data of the target location information to help improve the accuracy, which adopts a modified particle filter algorithm to reduce the number of samples and computing overhead. Second, we present a database self-update method according to trajectory continuity, which is employed to update fingerprint database dynamically for keeping the system robust. Finally, we tested the optimal target tracking based on dynamic fingerprint algorithm (OTTDF) in a complex laboratory area with diverse target motion conditions and various obstacles, our experimental results indicate that the OTTDF scheme successfully handles complex indoor structure, including different target motion state, signal fingerprint changes caused by obstacles, simultaneously provides better performance in localization cost, and localization/tracking accuracy in indoor wireless network.",
        "authors": "Chun Wang, Juan Luo, Yanliu Zheng",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2880247",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, a novel variational Bayesian-based adaptive Kalman filter (VBAKF) is proposed to solve the problem of a linear state-space model with colored measurement noise and inaccurate noise covariance matrices. The filter problem of a linear state-space model with colored measurement noise and inaccurate noise covariance matrices is transformed into the filtering problem of a linear state-space model with white measurement noise and inaccurate noise covariance matrices using measurement differencing method and state augmentation approach. The augmentation state vector, corresponding predicted error covariance matrix and covariance matrix of white measurement noise are jointly estimated based on the variational Bayesian approach. The ability of addressing the colored measurement and inaccurate noise covariance matrices is demonstrated in the simulation of a target tracking example.",
        "abstract": "In this paper, a novel variational Bayesian-based adaptive Kalman filter (VBAKF) is proposed to solve the problem of a linear state-space model with colored measurement noise and inaccurate noise covariance matrices. The filter problem of a linear state-space model with colored measurement noise and inaccurate noise covariance matrices is transformed into the filtering problem of a linear state-space model with white measurement noise and inaccurate noise covariance matrices using measurement differencing method and state augmentation approach. The augmentation state vector, corresponding predicted error covariance matrix and covariance matrix of white measurement noise are jointly estimated based on the variational Bayesian approach. The ability of addressing the colored measurement and inaccurate noise covariance matrices is demonstrated in the simulation of a target tracking example.",
        "authors": "Guangle Jia, Mingming Bai, Ning Li, Yonggang Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883040",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, we propose an interactive artwork system with automultiscopic 3D and haptic paint brush in an immersive room. Our system consists of a 81-view automultiscopic display, a handheld haptic paint brush, and a large-scale color palette station in a CAVE-like cubic room filled with the visual scene of the artwork. The 81-view rendering and multiplexing technology is applied by setting up the virtual cameras in the off-axis layout. The haptic paint brush is designed and implemented using a 2D array of multiple piezoelectric actuators. It provides the tactile feedback of spatial distance information between a virtual brush and a distal 3D object displayed on the automultiscopic display for the precise control of the brush when it is interacting with automultiscopic 3D. We demonstrate a proof-of-concept system that integrates a classic artwork into an innovative interactive system using novel multimedia and interactive technologies and evaluate our system using the handheld haptic brush for its performance and usability to enhance the user experiences.",
        "abstract": "In this paper, we propose an interactive artwork system with automultiscopic 3D and haptic paint brush in an immersive room. Our system consists of a 81-view automultiscopic display, a handheld haptic paint brush, and a large-scale color palette station in a CAVE-like cubic room filled with the visual scene of the artwork. The 81-view rendering and multiplexing technology is applied by setting up the virtual cameras in the off-axis layout. The haptic paint brush is designed and implemented using a 2D array of multiple piezoelectric actuators. It provides the tactile feedback of spatial distance information between a virtual brush and a distal 3D object displayed on the automultiscopic display for the precise control of the brush when it is interacting with automultiscopic 3D. We demonstrate a proof-of-concept system that integrates a classic artwork into an innovative interactive system using novel multimedia and interactive technologies and evaluate our system using the handheld haptic brush for its performance and usability to enhance the user experiences.",
        "authors": "Hyungki Son, Jin Ryong Kim, Sang-Youn Kim, Seungho Choi, Seunghyup Shin",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883821",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Deregulation of electricity retail market enables competition among retailers and thereby enriches customers' choices. In this environment, retailers compete in the retail market mainly by bidding their retail prices. As bidding price of a retailer increases, customers would reduce their demands on this retailer, or even switch their demands to other retailers with lower bidding prices. These potential consequences will inevitably impact retailers' bidding strategies in the retail market. The impacts of consumers' switching behaviors and contract trading on strategic behaviors of retailers in the retail market is first investigated in this paper. A Bertrand-based game model for the retail market is proposed while considering customers' switching behaviors and retailers' contract trading. Specifically, in the proposed model, the market share function is introduced to describe customers' elasticity and switching behaviors. Also, the close-loop interaction of the wholesale market and retail market is considered. In addition, the existence and uniqueness of the Nash equilibrium for the game model are proved. The nonlinear complementarity approach is employed to find the Nash equilibrium outcomes. Effectiveness of the theoretical model is verified by numerical examples. Simulation results show that both customers' switching behaviors and retailers' contract trading can help mitigate market power abuse of retailers.",
        "abstract": "Deregulation of electricity retail market enables competition among retailers and thereby enriches customers' choices. In this environment, retailers compete in the retail market mainly by bidding their retail prices. As bidding price of a retailer increases, customers would reduce their demands on this retailer, or even switch their demands to other retailers with lower bidding prices. These potential consequences will inevitably impact retailers' bidding strategies in the retail market. The impacts of consumers' switching behaviors and contract trading on strategic behaviors of retailers in the retail market is first investigated in this paper. A Bertrand-based game model for the retail market is proposed while considering customers' switching behaviors and retailers' contract trading. Specifically, in the proposed model, the market share function is introduced to describe customers' elasticity and switching behaviors. Also, the close-loop interaction of the wholesale market and retail market is considered. In addition, the existence and uniqueness of the Nash equilibrium for the game model are proved. The nonlinear complementarity approach is employed to find the Nash equilibrium outcomes. Effectiveness of the theoretical model is verified by numerical examples. Simulation results show that both customers' switching behaviors and retailers' contract trading can help mitigate market power abuse of retailers.",
        "authors": "Chen Zhao, Lei Wu, Shaohua Zhang, Xian Wang, Xue Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883118",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "As the representative one of representation-based classification (RBC) methods, collaborative RBC (CRC) has drawn much attention in pattern recognition and machine learning recently. Moreover, the collaborative representation-based face recognition has been extensively studied because of the effective classification performance of CRC. CRC collaboratively represents each query sample as the linear combination of all the training samples and then classifies the query sample according to the categorical representation-based distances. However, most variants of CRC cannot fully consider the locality and discrimination of data and cannot well handle the noise data, which has negative effect on real-world classification problems, such as face recognition. In this paper, a new discriminative collaborative neighbor representation (DCNR) method for face recognition is proposed by integrating class discrimination and data locality. In the proposed method, the locality of data constrains collaborative representation of each query sample to find representative nearest samples of the query sample. Moreover, the class discrimination regularization is taken into account by employing the representation of each class for each query sample. Due to the existing noises, such as corruptions and occlusions in face recognition, we further propose robust DCNR (R-DCNR) for robust classification by using the ℓ1-norm representation fidelity. Extensive experiments on face databases demonstrate that the proposed methods achieve competitive classification performance, compared to the state-of-the-art representation-based classification methods.",
        "abstract": "As the representative one of representation-based classification (RBC) methods, collaborative RBC (CRC) has drawn much attention in pattern recognition and machine learning recently. Moreover, the collaborative representation-based face recognition has been extensively studied because of the effective classification performance of CRC. CRC collaboratively represents each query sample as the linear combination of all the training samples and then classifies the query sample according to the categorical representation-based distances. However, most variants of CRC cannot fully consider the locality and discrimination of data and cannot well handle the noise data, which has negative effect on real-world classification problems, such as face recognition. In this paper, a new discriminative collaborative neighbor representation (DCNR) method for face recognition is proposed by integrating class discrimination and data locality. In the proposed method, the locality of data constrains collaborative representation of each query sample to find representative nearest samples of the query sample. Moreover, the class discrimination regularization is taken into account by employing the representation of each class for each query sample. Due to the existing noises, such as corruptions and occlusions in face recognition, we further propose robust DCNR (R-DCNR) for robust classification by using the ℓ1-norm representation fidelity. Extensive experiments on face databases demonstrate that the proposed methods achieve competitive classification performance, compared to the state-of-the-art representation-based classification methods.",
        "authors": "Jiancheng Lv, Jianping Gou, Lei Wang, Qirong Mao, Yun-Hao Yuan, Zhang Yi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883527",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Collaborative decision making (CDM) with linguistic computational techniques has recently achieved significant advancements. Due to the widespread use of sophisticated linguistic constructions, such as generalized comparative linguistic expressions (GCLEs), additional information associated with subjective appraisals has been exploited with the aim of addressing accuracy improvements in multifarious CDM, given that partial information loss is almost inevitable while dealing with complex linguistic comprehension. This paper brings an innovative perspective into CDM from COmponent ANalysis with GCLEs (COANG) to formalize problems involved in making optimal choices, mainly in CDM problems with participants who are usually characterized by domain specificity. Consequently, the focus of this paper is on the domain-specific CDM (DSCDM) in which individual semantics should be built predominantly to model various implications of their decision appraisals with heterogeneity in the knowledgeable domain for the effort of computational reinforcements. The attitude orientation and strength are crucial decompositions to incorporate COANG into DSCDM to establish an elastic paradigm that puts forward individual perception comprehension ahead of exerting collective efforts. The DSCDM based on COANG model enables agents to turn complex challenges of sophisticated linguistic constructions into substantial opportunities by translating them into customized individual semantics (CIS), and CIS into useful insights for making better decisions and improving results. The potential advantages of the proposed COANG-based DSCDM framework are validated with a clinical psychological practice related to the severity assessment of symptoms of schizophrenia.",
        "abstract": "Collaborative decision making (CDM) with linguistic computational techniques has recently achieved significant advancements. Due to the widespread use of sophisticated linguistic constructions, such as generalized comparative linguistic expressions (GCLEs), additional information associated with subjective appraisals has been exploited with the aim of addressing accuracy improvements in multifarious CDM, given that partial information loss is almost inevitable while dealing with complex linguistic comprehension. This paper brings an innovative perspective into CDM from COmponent ANalysis with GCLEs (COANG) to formalize problems involved in making optimal choices, mainly in CDM problems with participants who are usually characterized by domain specificity. Consequently, the focus of this paper is on the domain-specific CDM (DSCDM) in which individual semantics should be built predominantly to model various implications of their decision appraisals with heterogeneity in the knowledgeable domain for the effort of computational reinforcements. The attitude orientation and strength are crucial decompositions to incorporate COANG into DSCDM to establish an elastic paradigm that puts forward individual perception comprehension ahead of exerting collective efforts. The DSCDM based on COANG model enables agents to turn complex challenges of sophisticated linguistic constructions into substantial opportunities by translating them into customized individual semantics (CIS), and CIS into useful insights for making better decisions and improving results. The potential advantages of the proposed COANG-based DSCDM framework are validated with a clinical psychological practice related to the severity assessment of symptoms of schizophrenia.",
        "authors": "Kwai-Sang Chin, Kwok-Leung Tsui, Luis Martínez, Mei Xu, Xian-Jia Wang, Zhen-Song Chen",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885342",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In multimedia forensics, many efforts have been made to detect whether an image is pristine or manipulated with high enough accuracies based on specially designed features and classifiers in the past decade. However, the important task for localizing the tampering regions in a fake image still faces more challenges compared with the manipulation detection and relatively a few algorithms attempt to tackle it. With this in mind, a technique that utilizes the dual-domain-based convolutional neural networks (D-CNNs) taking different kinds of input into consideration is proposed in this paper. In the proposed framework, two sub-networks, named the spatial-domain CNN model (Sub-SCNN) and the frequency-domain-based CNN model (Sub-FCNN), are designed and trained, respectively. With the well-trained parameters, a transfer policy is applied to the training process of the D-CNN. While CNNs are capable of learning classification features directly from data, in their standard form they tend to learn features related to the image's content. To overcome this issue in image forensics tasks, a new image pre-processing layer is proposed to jointly suppress image's content and adaptively learn manipulation detection and localization features. After investigating the properties of datasets, two post-processing operations are finally proposed and compared to obtain the final results of the pixel-wise manipulation region localization. The D-CNNs is trained and validated using 75 percent of images in the CASIA v2.0 and tested using the remaining images in the CASIA v2.0, all images in Columbia Uncompressed and Carvalho datasets. The extensive experiments show that the proposed post-processing operations optimize the final tamper probability map, and our framework with the combination of Sub-SCNN and Sub-FCNN significantly outperforms the state-of-art techniques with the best F1 scores on the datasets.",
        "abstract": "In multimedia forensics, many efforts have been made to detect whether an image is pristine or manipulated with high enough accuracies based on specially designed features and classifiers in the past decade. However, the important task for localizing the tampering regions in a fake image still faces more challenges compared with the manipulation detection and relatively a few algorithms attempt to tackle it. With this in mind, a technique that utilizes the dual-domain-based convolutional neural networks (D-CNNs) taking different kinds of input into consideration is proposed in this paper. In the proposed framework, two sub-networks, named the spatial-domain CNN model (Sub-SCNN) and the frequency-domain-based CNN model (Sub-FCNN), are designed and trained, respectively. With the well-trained parameters, a transfer policy is applied to the training process of the D-CNN. While CNNs are capable of learning classification features directly from data, in their standard form they tend to learn features related to the image's content. To overcome this issue in image forensics tasks, a new image pre-processing layer is proposed to jointly suppress image's content and adaptively learn manipulation detection and localization features. After investigating the properties of datasets, two post-processing operations are finally proposed and compared to obtain the final results of the pixel-wise manipulation region localization. The D-CNNs is trained and validated using 75 percent of images in the CASIA v2.0 and tested using the remaining images in the CASIA v2.0, all images in Columbia Uncompressed and Carvalho datasets. The extensive experiments show that the proposed post-processing operations optimize the final tamper probability map, and our framework with the combination of Sub-SCNN and Sub-FCNN significantly outperforms the state-of-art techniques with the best F1 scores on the datasets.",
        "authors": "Hui Kang, Xuanjing Shen, Yingda Lv, Zenan Shi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883588",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Cultivation of olive trees for the past few years has been widely spread across Mediterranean countries, including Spain, Greece, Italy, France, and Turkey. Among these countries, Spain is listed as the largest olive producing country with almost 45% of olive oil production per year. Dedicating land of over 2.4 million hectares for the olive cultivation, Spain is among the leading distributors of olives throughout the world. Due to its high significance in the country’s economy, the crop yield must be recorded. Manual collection of data over such expanded fields is humanly infeasible. Remote collection of such information can be made possible through the utilization of satellite imagery. This paper presents an automated olive tree counting method based on image processing of satellite imagery. The images are pre-processed using the unsharp masking followed by improved multi-level thresholding-based segmentation. Resulting circular blobs are detected through the circular Hough transform for identification. Validation has been performed by evaluating the proposed scheme for the dataset formed by acquiring images through the “El Sistema de Información Geográfica de Parcelas Agrícolas” viewer over the region of Spain. The proposed algorithm achieves an accuracy of 96% in detection. Computation time was recorded as 24 ms for an image size of300×300pixels. The less spectral information is used in our proposed methodology resulting in a competitive accuracy with low computational cost in comparison to the state-of-the-art technique.",
        "abstract": "Cultivation of olive trees for the past few years has been widely spread across Mediterranean countries, including Spain, Greece, Italy, France, and Turkey. Among these countries, Spain is listed as the largest olive producing country with almost 45% of olive oil production per year. Dedicating land of over 2.4 million hectares for the olive cultivation, Spain is among the leading distributors of olives throughout the world. Due to its high significance in the country’s economy, the crop yield must be recorded. Manual collection of data over such expanded fields is humanly infeasible. Remote collection of such information can be made possible through the utilization of satellite imagery. This paper presents an automated olive tree counting method based on image processing of satellite imagery. The images are pre-processed using the unsharp masking followed by improved multi-level thresholding-based segmentation. Resulting circular blobs are detected through the circular Hough transform for identification. Validation has been performed by evaluating the proposed scheme for the dataset formed by acquiring images through the “El Sistema de Información Geográfica de Parcelas Agrícolas” viewer over the region of Spain. The proposed algorithm achieves an accuracy of 96% in detection. Computation time was recorded as 24 ms for an image size of300×300pixels. The less spectral information is used in our proposed methodology resulting in a competitive accuracy with low computational cost in comparison to the state-of-the-art technique.",
        "authors": "Aftab Khan, Ashfaq Khan, Muhammad Waleed, Safdar Nawaz Khan Marwat, Tariq Kamal, Umair Khan",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884199",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "With the advent of the age of big data, people can collect rich and diverse data from a wide variety of collection devices, such as those of the Internet of Things. Knowledge hidden in large data is very useful and valuable. Frequent pattern mining, as a basic method of data mining, is applied to every aspect of society. However, the application of traditional frequent pattern mining methods to big data involves bottlenecks due to the large number of result sets. Such bottlenecks make it difficult to produce practical value in production and life. Therefore, mining representative pattern sets has been proposed. However, most existing algorithms select representative patterns after mining frequent pattern sets. This framework can make the runtime difficult to evaluate in large data environments. To solve the above-mentioned problems, this paper presents an online representative pattern-set parallel-mining algorithm. Within the parallel MapReduce framework, this algorithm uses horizontal segmentation to process the database and then applies the online mining algorithm to mine the locally represented pattern sets on each small database. Finally, several performance optimization strategies are proposed. As shown by numerous experiments on the actual dataset, the algorithm proposed in this paper improves the time efficiency by one order of magnitude. Several optimization strategies reduce the execution time to varying degrees.",
        "abstract": "With the advent of the age of big data, people can collect rich and diverse data from a wide variety of collection devices, such as those of the Internet of Things. Knowledge hidden in large data is very useful and valuable. Frequent pattern mining, as a basic method of data mining, is applied to every aspect of society. However, the application of traditional frequent pattern mining methods to big data involves bottlenecks due to the large number of result sets. Such bottlenecks make it difficult to produce practical value in production and life. Therefore, mining representative pattern sets has been proposed. However, most existing algorithms select representative patterns after mining frequent pattern sets. This framework can make the runtime difficult to evaluate in large data environments. To solve the above-mentioned problems, this paper presents an online representative pattern-set parallel-mining algorithm. Within the parallel MapReduce framework, this algorithm uses horizontal segmentation to process the database and then applies the online mining algorithm to mine the locally represented pattern sets on each small database. Finally, several performance optimization strategies are proposed. As shown by numerous experiments on the actual dataset, the algorithm proposed in this paper improves the time efficiency by one order of magnitude. Several optimization strategies reduce the execution time to varying degrees.",
        "authors": "Liu Bin, Wei Mingqi, Zhang Tianrui",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884888",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In most echo state networks (ESNs), the training error typically decreases as the network size increases, and thus the overfitting issue is widely existed. To solve this problem, an incremental ESN (IESN) is proposed by incorporating the leave-one-out cross-validation (LOO-CV) and the regularization method. First, the LOO-CV error is used to automatically identify the network architecture such that the overfitting problem is avoided to some extent. Second, the regularization technique is used to solve the ill-posed problem, and thus the IESN owns good robustness property. Third, the output weights are incrementally calculated by the fast SVD updating algorithm to reduce the ESN training time. Moreover, the stability and convergence of IESN are discussed to ensure its successful application. Simulation results demonstrate that the proposed IESN requires fewer reservoir nodes yet obtains much better performance than other existing ESNs.",
        "abstract": "In most echo state networks (ESNs), the training error typically decreases as the network size increases, and thus the overfitting issue is widely existed. To solve this problem, an incremental ESN (IESN) is proposed by incorporating the leave-one-out cross-validation (LOO-CV) and the regularization method. First, the LOO-CV error is used to automatically identify the network architecture such that the overfitting problem is avoided to some extent. Second, the regularization technique is used to solve the ill-posed problem, and thus the IESN owns good robustness property. Third, the output weights are incrementally calculated by the fast SVD updating algorithm to reduce the ESN training time. Moreover, the stability and convergence of IESN are discussed to ensure its successful application. Simulation results demonstrate that the proposed IESN requires fewer reservoir nodes yet obtains much better performance than other existing ESNs.",
        "authors": "Cuili Yang, Junfei Qiao, Lei Wang, Xinxin Zhu, Zohaib Ahmad",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883114",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Exploiting the parallelism in multiprocessor systems is a major challenge in modern computer science. Multicore programming demands a change in the way we design and use fundamental data structures. The standard collection of data structures and algorithms in C++11 is the sequential standard template library (STL). In this paper, we present their vision for the theory and practice for the design and implementation of a collection of highly concurrent fundamental data structures for multiprocessor application development with associated programming interface and advanced optimization support. Specifically, the proposed approach will provide a familiar, easy-to-use, and composable interface, similar to that of C++ STL. Each container type will be enhanced with internal support for nonblocking synchronization of its data access, thereby providing better safety and performance than traditional blocking synchronization by: 1) eliminating hazards such as deadlock, livelock, and priority inversion and 2) by being highly scalable in supporting large numbers of threads. The new library, lockless containers/data concurrency, will provide algorithms for handling fundamental computations in multithreaded contexts, and will incorporate these into libraries with familiar look and feel. The proposed approach will provide an immense boost in performance and software reuse, consequently productivity, for developers of scientific and systems applications, which are predominantly in C/C++. STL is widely used and a concurrent replacement library will have an immediate practical relevance and a significant impact on a variety of parallel programming domains including simulation, massive data mining, computational biology, financial engineering, and embedded control systems. As a proof-of-concept, this paper discusses the first design and implementation of a wait-free hash table.",
        "abstract": "Exploiting the parallelism in multiprocessor systems is a major challenge in modern computer science. Multicore programming demands a change in the way we design and use fundamental data structures. The standard collection of data structures and algorithms in C++11 is the sequential standard template library (STL). In this paper, we present their vision for the theory and practice for the design and implementation of a collection of highly concurrent fundamental data structures for multiprocessor application development with associated programming interface and advanced optimization support. Specifically, the proposed approach will provide a familiar, easy-to-use, and composable interface, similar to that of C++ STL. Each container type will be enhanced with internal support for nonblocking synchronization of its data access, thereby providing better safety and performance than traditional blocking synchronization by: 1) eliminating hazards such as deadlock, livelock, and priority inversion and 2) by being highly scalable in supporting large numbers of threads. The new library, lockless containers/data concurrency, will provide algorithms for handling fundamental computations in multithreaded contexts, and will incorporate these into libraries with familiar look and feel. The proposed approach will provide an immense boost in performance and software reuse, consequently productivity, for developers of scientific and systems applications, which are predominantly in C/C++. STL is widely used and a concurrent replacement library will have an immediate practical relevance and a significant impact on a variety of parallel programming domains including simulation, massive data mining, computational biology, financial engineering, and embedded control systems. As a proof-of-concept, this paper discusses the first design and implementation of a wait-free hash table.",
        "authors": "Damian Dechev, Pierre Laborde, Steven D. Feldman",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2282500",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Wi-Fi and magnetic field fingerprinting have been a hot topic in indoor positioning researches because of their ubiquity and location-related features. Wi-Fi signals can provide rough initial positions, and magnetic fields can further improve the positioning accuracies, therefore many researchers have tried to combine the two signals for high-accuracy indoor localization. Currently, state-of-the-art solutions design separate algorithms to process different indoor signals. Outputs of these algorithms are generally used as inputs of data fusion strategies. These methods rely on computationally expensive particle filters, labor-intensive feature analysis, and time-consuming parameter tuning to achieve better accuracies. Besides, particle filters need to estimate the moving directions of particles, limiting smartphone orientation to be stable, and aligned with the user's moving directions. In this paper, we adopted a convolutional neural network (CNN) to implement an accurate and orientation-free positioning system. Inspired by the state-of-the-art image classification methods, we design a novel hybrid location image using Wi-Fi and magnetic field fingerprints, and then a CNN is employed to classify the locations of the fingerprint images. In order to prevent the overfitting problem of the positioning CNN on limited training datasets, we also propose to divide the learning process into two steps to adopt proper learning strategies for different network branches. We show that the CNN solution is able to automatically learn location patterns, thus significantly lower the workforce burden of designing a localization system. Our experimental results convincingly reveal that the proposed positioning method achieves an accuracy of about 1 m under different smartphone orientations, users, and use patterns.",
        "abstract": "Wi-Fi and magnetic field fingerprinting have been a hot topic in indoor positioning researches because of their ubiquity and location-related features. Wi-Fi signals can provide rough initial positions, and magnetic fields can further improve the positioning accuracies, therefore many researchers have tried to combine the two signals for high-accuracy indoor localization. Currently, state-of-the-art solutions design separate algorithms to process different indoor signals. Outputs of these algorithms are generally used as inputs of data fusion strategies. These methods rely on computationally expensive particle filters, labor-intensive feature analysis, and time-consuming parameter tuning to achieve better accuracies. Besides, particle filters need to estimate the moving directions of particles, limiting smartphone orientation to be stable, and aligned with the user's moving directions. In this paper, we adopted a convolutional neural network (CNN) to implement an accurate and orientation-free positioning system. Inspired by the state-of-the-art image classification methods, we design a novel hybrid location image using Wi-Fi and magnetic field fingerprints, and then a CNN is employed to classify the locations of the fingerprint images. In order to prevent the overfitting problem of the positioning CNN on limited training datasets, we also propose to divide the learning process into two steps to adopt proper learning strategies for different network branches. We show that the CNN solution is able to automatically learn location patterns, thus significantly lower the workforce burden of designing a localization system. Our experimental results convincingly reveal that the proposed positioning method achieves an accuracy of about 1 m under different smartphone orientations, users, and use patterns.",
        "authors": "Antonino Crivello, Fang Zhao, Haiyong Luo, Wenhua Shao, Yan Ma, Zhongliang Zhao",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884193",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Through modeling human’s brainstorming process, the brain storm optimization (BSO) algorithm has become a promising population-based evolutionary algorithm. However, BSO is pointed out that it possesses a degenerated L-curve phenomenon, i.e., it often gets near optimum quickly but needs much more cost to improve the accuracy. To overcome this question in this paper, an excellent direct search-based local solver, the Nelder–Mead Simplex method is adopted in BSO. Through combining BSO’s exploration ability and NMS’s exploitation ability together, a simplex search-based BSO (Simplex-BSO) is developed via a better balance between global exploration and local exploitation. Simplex-BSO is shown to be able to eliminate the degenerated L-curve phenomenon on unimodal functions, and alleviate significantly this phenomenon on multimodal functions. Large number of experimental results shows that Simplex-BSO is a promising algorithm for global optimization problems.",
        "abstract": "Through modeling human’s brainstorming process, the brain storm optimization (BSO) algorithm has become a promising population-based evolutionary algorithm. However, BSO is pointed out that it possesses a degenerated L-curve phenomenon, i.e., it often gets near optimum quickly but needs much more cost to improve the accuracy. To overcome this question in this paper, an excellent direct search-based local solver, the Nelder–Mead Simplex method is adopted in BSO. Through combining BSO’s exploration ability and NMS’s exploitation ability together, a simplex search-based BSO (Simplex-BSO) is developed via a better balance between global exploration and local exploitation. Simplex-BSO is shown to be able to eliminate the degenerated L-curve phenomenon on unimodal functions, and alleviate significantly this phenomenon on multimodal functions. Large number of experimental results shows that Simplex-BSO is a promising algorithm for global optimization problems.",
        "authors": "Qunfeng Liu, Shi Cheng, Wei Chen, Yifei Sun, Yingying Cao, Yun Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883506",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "With the rapid development of modern teaching technology, the construction of smart campus has become the focus of modern college education reform. The application of technologies, such as the Internet of Things and big data, plays an important role in improving the teaching environment of colleges and universities, improving the utilization of teaching resources, and the flexibility of education. As an important part of campus activities, teaching performance evaluation scientifically and effectively utilizes teaching information and teacher and student interaction information to evaluate teachers' teaching performance, which helps to motivate teachers' work enthusiasm, improve teaching quality, and enhance school core competitiveness. This paper analyzes the salient features of smart campus from the perspectives of technology, business, and construction mode, and proposes a smart campus architecture model. According to the research content of teaching performance evaluation, the framework model of smart campus education data collection and storage platform is established, which provides a reference model for the construction of smart campus in colleges and universities. The evaluation of teaching performance in smart campus first analyzes the shortcomings of traditional evaluation methods and proposes the necessity of combining teaching performance evaluation with modern technology. Second, six principal components were determined using the PCA algorithm. Then, use the AHP to calculate the weights of each layer of the indicator set, avoiding the decision errors caused by subjective factors. Finally, the gray correlation degree is used to improve the TOPSIS algorithm for multi-objective decision analysis. The evaluation results of the AHP-TOPSIS teaching performance model are consistent with the actual situation. The application of the smart campus education data platform combined with the AHP and the gray correlation improvement TOPSIS algorithm is more targeted t...",
        "abstract": "With the rapid development of modern teaching technology, the construction of smart campus has become the focus of modern college education reform. The application of technologies, such as the Internet of Things and big data, plays an important role in improving the teaching environment of colleges and universities, improving the utilization of teaching resources, and the flexibility of education. As an important part of campus activities, teaching performance evaluation scientifically and effectively utilizes teaching information and teacher and student interaction information to evaluate teachers' teaching performance, which helps to motivate teachers' work enthusiasm, improve teaching quality, and enhance school core competitiveness. This paper analyzes the salient features of smart campus from the perspectives of technology, business, and construction mode, and proposes a smart campus architecture model. According to the research content of teaching performance evaluation, the framework model of smart campus education data collection and storage platform is established, which provides a reference model for the construction of smart campus in colleges and universities. The evaluation of teaching performance in smart campus first analyzes the shortcomings of traditional evaluation methods and proposes the necessity of combining teaching performance evaluation with modern technology. Second, six principal components were determined using the PCA algorithm. Then, use the AHP to calculate the weights of each layer of the indicator set, avoiding the decision errors caused by subjective factors. Finally, the gray correlation degree is used to improve the TOPSIS algorithm for multi-objective decision analysis. The evaluation results of the AHP-TOPSIS teaching performance model are consistent with the actual situation. The application of the smart campus education data platform combined with the AHP and the gray correlation improvement TOPSIS algorithm is more targeted t...",
        "authors": "Shujiang Yu, Xin Xu, Yunsheng Wang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884022",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "During the last two decades, several visible watermarking algorithms with different characteristics have been proposed. In the context of visible digital watermarking, watermark visibility and its non-obtrusiveness are essential issues. However, assessment metrics were not established to evaluate these two issues until now. Due to the lack of established assessment metrics, visibility is evaluated in a subjective manner and the obtrusiveness caused by the visible watermark is evaluated by well-known image quality metrics, such as peak signal noise ratio, comparing the host image and the watermarked image. In this paper, we propose four assessment metrics for visible watermarking algorithms, in which the global visibility, global obtrusiveness, local obtrusiveness in host edge region, and a global quality of watermarked image are evaluated using a couple of criteria based on the human visual system and a pixel-based just-noticeable distortion function. The evaluation results show the reliability of the proposed metrics used for measuring the above-mentioned aspects when several visible watermarking algorithms are employed as well as in several scenarios where an attempt to remove the watermark pattern from the watermarked content was made.",
        "abstract": "During the last two decades, several visible watermarking algorithms with different characteristics have been proposed. In the context of visible digital watermarking, watermark visibility and its non-obtrusiveness are essential issues. However, assessment metrics were not established to evaluate these two issues until now. Due to the lack of established assessment metrics, visibility is evaluated in a subjective manner and the obtrusiveness caused by the visible watermark is evaluated by well-known image quality metrics, such as peak signal noise ratio, comparing the host image and the watermarked image. In this paper, we propose four assessment metrics for visible watermarking algorithms, in which the global visibility, global obtrusiveness, local obtrusiveness in host edge region, and a global quality of watermarked image are evaluated using a couple of criteria based on the human visual system and a pixel-based just-noticeable distortion function. The evaluation results show the reliability of the proposed metrics used for measuring the above-mentioned aspects when several visible watermarking algorithms are employed as well as in several scenarios where an attempt to remove the watermark pattern from the watermarked content was made.",
        "authors": "Antonio Cedillo-Hernández, Eduardo Fragoso-Navarro, Héctor Manuel Pérez-Meana, Manuel Cedillo-Hernández, Mariko Nakano-Miyatake",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883322",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Posted road speed limits contribute to the safety of driving, yet when certain driving conditions occur, such as fog or severe darkness, they become less meaningful to the drivers. To overcome this limitation, there is a need for adaptive speed limits system to improve road safety under varying driving conditions. In that vein, a visibility range estimation algorithm for real-time adaptive speed limits control in intelligent transportation systems is proposed in this paper. The information required to specify the speed limit is captured via a road side unit that collects environmental data and captures road images, which are then analyzed locally or on the cloud. The proposed analysis is performed using two image processing algorithms, namely, the improved dark channel prior (DCP) and weighted image entropy (WIE), and the support vector machine (SVM) classifier is used to produce a visibility indicator in real-time. Results obtained from the analysis of various parts of highways in Canada, provided by the Ministry of Transportation of Ontario, show that the proposed technique can generate credible visibility indicators to motorists. The analytical results corroborated by extensive field measurements confirmed the advantage of the proposed system when compared to other visibility estimation methods such as the conventional DCP and WIE, where the proposed system results exhibit about 25% accuracy enhancement over the other considered techniques. Moreover, the proposed DCP is about 26% faster than the conventional DCP. The obtained promising results potentiate the integration of the proposed technique in real-life scenarios.",
        "abstract": "Posted road speed limits contribute to the safety of driving, yet when certain driving conditions occur, such as fog or severe darkness, they become less meaningful to the drivers. To overcome this limitation, there is a need for adaptive speed limits system to improve road safety under varying driving conditions. In that vein, a visibility range estimation algorithm for real-time adaptive speed limits control in intelligent transportation systems is proposed in this paper. The information required to specify the speed limit is captured via a road side unit that collects environmental data and captures road images, which are then analyzed locally or on the cloud. The proposed analysis is performed using two image processing algorithms, namely, the improved dark channel prior (DCP) and weighted image entropy (WIE), and the support vector machine (SVM) classifier is used to produce a visibility indicator in real-time. Results obtained from the analysis of various parts of highways in Canada, provided by the Ministry of Transportation of Ontario, show that the proposed technique can generate credible visibility indicators to motorists. The analytical results corroborated by extensive field measurements confirmed the advantage of the proposed system when compared to other visibility estimation methods such as the conventional DCP and WIE, where the proposed system results exhibit about 25% accuracy enhancement over the other considered techniques. Moreover, the proposed DCP is about 26% faster than the conventional DCP. The obtained promising results potentiate the integration of the proposed technique in real-life scenarios.",
        "authors": "Arafat Al-Dweik, Leontios J. Hadjileontiadis, Li Yang, Radu Muresan",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884225",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The original moth-flame optimization (MFO) algorithm neither generates high-performance flames nor utilizes the flames to offer enough effective search guidance for moths in solution spaces, causing the degeneration of the global search capability and convergence speed in confronting complicated problems. To overwhelm those imperfections, this paper proposes a double-evolutionary learning MFO algorithm (DELMFO), where two different evolutionary learning strategies, namely, the differential evolution flame generation (DEFG) and dynamic flame guidance (DFG) strategy, are presented to generate high-performance flames and dynamically guide the search of moths, respectively. By constructing the cascading collaboration between DEFG and DFG, the DELMFO offers a positive feedback channel that makes the personal best historical solutions (PBHSs), flames, and moths promote each other. This improves the global search capability and accelerates convergence speed. The DELMFO is compared with six MFO algorithms and nine popular stochastic optimization algorithms on the CEC2013 test suite. Furthermore, the DELMFO also is further compared with 10 stochastic optimization algorithms on the CEC2017 test suite. Experimental results show that the DELMFO obtains the competitive performance on the global search capability, convergence speed, and scalability among all the algorithms.",
        "abstract": "The original moth-flame optimization (MFO) algorithm neither generates high-performance flames nor utilizes the flames to offer enough effective search guidance for moths in solution spaces, causing the degeneration of the global search capability and convergence speed in confronting complicated problems. To overwhelm those imperfections, this paper proposes a double-evolutionary learning MFO algorithm (DELMFO), where two different evolutionary learning strategies, namely, the differential evolution flame generation (DEFG) and dynamic flame guidance (DFG) strategy, are presented to generate high-performance flames and dynamically guide the search of moths, respectively. By constructing the cascading collaboration between DEFG and DFG, the DELMFO offers a positive feedback channel that makes the personal best historical solutions (PBHSs), flames, and moths promote each other. This improves the global search capability and accelerates convergence speed. The DELMFO is compared with six MFO algorithms and nine popular stochastic optimization algorithms on the CEC2013 test suite. Furthermore, the DELMFO also is further compared with 10 stochastic optimization algorithms on the CEC2017 test suite. Experimental results show that the DELMFO obtains the competitive performance on the global search capability, convergence speed, and scalability among all the algorithms.",
        "authors": "Buxing Li, Chunquan Li, Jinghui Fan, Peter X. Liu, Zheng Niu, Zhenshou Song",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884130",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Aortic stenosis (AS) and aortic regurgitation (AR) were two types of aortic valve defects which had different hemodynamic characteristics. Ultimately, chronic pressure and volume overloads result in different geometrical and functional patterns. The expectation of our study was to compare the impacts of two different types of loading conditions on myocardial deformation in asymptomatic patients with normal ejection fraction in compensatory stage using three-dimensional speckle tracking echocardiography (3D-STE). Among severe aortic valve disease, 36 patients with AR, and 32 patients with AS were enrolled. All subjects underwent conventional two-dimensional echocardiography and 3D-STE. Clinical and echocardiography characteristics were compared between the groups. Compared with control group, left ventricular twist was increased, along with decreased longitudinal, radial, and circumferential strain in the AS group. And AR patients were characterized by a spherical expansion of ventricle, and reduced myocardial longitudinal and radial strain, along with a preserved rotational motion. In univariate linear regression analysis, age, diastolic blood pressure, e', sphericity index, end-diastolic volume index, stoke volume index, ejection fraction, and left ventricular mass index were independently associated with global longitudinal strain (GLS) in the AR group and relative wall thickness, stroke volume index (SVI), LV mass index, and peak velocity were independently associated with GLS in the AS group. In addition, multiple stepwise regression analysis showed that sphericity index, SVI, and left ventricular ejection fraction (LVEF) were independently associated with GLS in the AR group. And LVEF, peak velocity, and left atrial dimension were independently associated with GLS in the AS group. The affected myocardial layers of AS patients were worse than AR patients, but increased twist was the compensatory mechanism to maintain the effective stroke volume. Multidirectional...",
        "abstract": "Aortic stenosis (AS) and aortic regurgitation (AR) were two types of aortic valve defects which had different hemodynamic characteristics. Ultimately, chronic pressure and volume overloads result in different geometrical and functional patterns. The expectation of our study was to compare the impacts of two different types of loading conditions on myocardial deformation in asymptomatic patients with normal ejection fraction in compensatory stage using three-dimensional speckle tracking echocardiography (3D-STE). Among severe aortic valve disease, 36 patients with AR, and 32 patients with AS were enrolled. All subjects underwent conventional two-dimensional echocardiography and 3D-STE. Clinical and echocardiography characteristics were compared between the groups. Compared with control group, left ventricular twist was increased, along with decreased longitudinal, radial, and circumferential strain in the AS group. And AR patients were characterized by a spherical expansion of ventricle, and reduced myocardial longitudinal and radial strain, along with a preserved rotational motion. In univariate linear regression analysis, age, diastolic blood pressure, e', sphericity index, end-diastolic volume index, stoke volume index, ejection fraction, and left ventricular mass index were independently associated with global longitudinal strain (GLS) in the AR group and relative wall thickness, stroke volume index (SVI), LV mass index, and peak velocity were independently associated with GLS in the AS group. In addition, multiple stepwise regression analysis showed that sphericity index, SVI, and left ventricular ejection fraction (LVEF) were independently associated with GLS in the AR group. And LVEF, peak velocity, and left atrial dimension were independently associated with GLS in the AS group. The affected myocardial layers of AS patients were worse than AR patients, but increased twist was the compensatory mechanism to maintain the effective stroke volume. Multidirectional...",
        "authors": "Lei Wang, Mingxing Xie, Qingyu Zeng, Shuangshuang Kong, Yali Yang, Yuman Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2882438",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "During the working cycle of an external gear pump, hydraulic oil trapped between the pair of meshing gear and pinon teeth may exert high-pressure pulses due to periodic volumetric compression and expansion of the trapped volume. The pressure pulses cause excessive alternating radial loads on the gear shafts, partially responsible for the noise and vibration in the unit. To reduce the trapped pressure pulsation, a relief chamber that connected to the trapped volume to absorb the pressure pulsation is designed in place of the traditional relief grooves to avoid the reconciliation between cross port leakage and pressure pulsation reduction. A new lumped parameter model compatible 2D morphological approach is proposed based on the mapping of gear profile function from real coordinate space to a binary-valued matrix in the integer coordinate space, reducing the control area calculation problem from polygon segmentation and numerical integration to pixel counting, relaxing the computational difficulty in design evaluations. Initial results show that the proposed method can trace and calculate the area of the trapped region, and the relief chamber is capable of effectively attenuating the peak magnitude of trapped pressure pulses by 95%.",
        "abstract": "During the working cycle of an external gear pump, hydraulic oil trapped between the pair of meshing gear and pinon teeth may exert high-pressure pulses due to periodic volumetric compression and expansion of the trapped volume. The pressure pulses cause excessive alternating radial loads on the gear shafts, partially responsible for the noise and vibration in the unit. To reduce the trapped pressure pulsation, a relief chamber that connected to the trapped volume to absorb the pressure pulsation is designed in place of the traditional relief grooves to avoid the reconciliation between cross port leakage and pressure pulsation reduction. A new lumped parameter model compatible 2D morphological approach is proposed based on the mapping of gear profile function from real coordinate space to a binary-valued matrix in the integer coordinate space, reducing the control area calculation problem from polygon segmentation and numerical integration to pixel counting, relaxing the computational difficulty in design evaluations. Initial results show that the proposed method can trace and calculate the area of the trapped region, and the relief chamber is capable of effectively attenuating the peak magnitude of trapped pressure pulses by 95%.",
        "authors": "Hao Tian",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883332",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The Kinect system is arguably the most popular 3-D camera technology currently on the market. Its application domain is vast and has been deployed in scenarios where accurate geometric measurements are needed. Regarding the PrimeSense technology, a limited amount of work has been devoted to calibrating the Kinect, especially the depth data. The Kinect is, however, inevitably prone to distortions, as independently confirmed by numerous users. An effective method for improving the quality of the Kinect system is by modeling the sensor's systematic errors using bundle adjustment. In this paper, a method for modeling the intrinsic and extrinsic parameters of the infrared and colour cameras, and more importantly the distortions in the depth image, is presented. Through an integrated marker-and feature-based self-calibration, two Kinects were calibrated. A novel approach for modeling the depth systematic errors as a function of lens distortion and relative orientation parameters is shown to be effective. The results show improvements in geometric accuracy up to 53% compared with uncalibrated point clouds captured using the popular software RGBDemo. Systematic depth discontinuities were also reduced and in the check-plane analysis the noise of the Kinect point cloud was reduced by 17%.",
        "abstract": "The Kinect system is arguably the most popular 3-D camera technology currently on the market. Its application domain is vast and has been deployed in scenarios where accurate geometric measurements are needed. Regarding the PrimeSense technology, a limited amount of work has been devoted to calibrating the Kinect, especially the depth data. The Kinect is, however, inevitably prone to distortions, as independently confirmed by numerous users. An effective method for improving the quality of the Kinect system is by modeling the sensor's systematic errors using bundle adjustment. In this paper, a method for modeling the intrinsic and extrinsic parameters of the infrared and colour cameras, and more importantly the distortions in the depth image, is presented. Through an integrated marker-and feature-based self-calibration, two Kinects were calibrated. A novel approach for modeling the depth systematic errors as a function of lens distortion and relative orientation parameters is shown to be effective. The results show improvements in geometric accuracy up to 53% compared with uncalibrated point clouds captured using the popular software RGBDemo. Systematic depth discontinuities were also reduced and in the check-plane analysis the noise of the Kinect point cloud was reduced by 17%.",
        "authors": "Derek D. Lichti, Jacky C. K. Chow",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2271860",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Computed tomography (CT) image reconstruction using classical total variation (TV)-based methods or its variations inevitably suffers from a blocky effect when the sampling number is low, because of the piecewise assumption. A low-rank based method is an effective way to circumvent this side effect. Normally, a nuclear norm is used to impose the low rank constraint, and its numerical computation depends on the sum of singular values, which are calculated by singular value decomposition. However, as larger singular values mainly deliver the structural information, treating all the singular values equally may lead to imperfect preservation of edges and textures. To deal with this problem, we here propose to reconstruct the CT image by explicitly exploring the nonlocal similarity in the target image with nonlocal weighted nuclear norm minimization. First, a matrix is constructed by grouping nonlocal patches similar to the current patch. Then, the original nuclear norm minimization is replaced by a weighted version, which treats the singular values differently according to their magnitudes. By doing this, we can eliminate noise and streak artifacts without introducing any side effects. The corresponding numerical algorithm is given by an alternating optimization strategy. Experimental results demonstrate that our method outperforms several existing reconstruction methods in both qualitative and quantitative aspects, including filtered back projection, TV, and total generalized variation methods.",
        "abstract": "Computed tomography (CT) image reconstruction using classical total variation (TV)-based methods or its variations inevitably suffers from a blocky effect when the sampling number is low, because of the piecewise assumption. A low-rank based method is an effective way to circumvent this side effect. Normally, a nuclear norm is used to impose the low rank constraint, and its numerical computation depends on the sum of singular values, which are calculated by singular value decomposition. However, as larger singular values mainly deliver the structural information, treating all the singular values equally may lead to imperfect preservation of edges and textures. To deal with this problem, we here propose to reconstruct the CT image by explicitly exploring the nonlocal similarity in the target image with nonlocal weighted nuclear norm minimization. First, a matrix is constructed by grouping nonlocal patches similar to the current patch. Then, the original nuclear norm minimization is replaced by a weighted version, which treats the singular values differently according to their magnitudes. By doing this, we can eliminate noise and streak artifacts without introducing any side effects. The corresponding numerical algorithm is given by an alternating optimization strategy. Experimental results demonstrate that our method outperforms several existing reconstruction methods in both qualitative and quantitative aspects, including filtered back projection, TV, and total generalized variation methods.",
        "authors": "Jiliu Zhou, Kang Yang, Peng Bao, Wenjun Xia, Yi Zhang, Yining Zhu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2881966",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Multi-objective robot exploration constitutes one of the most challenging tasks for autonomous robots performing in various operations and different environments. However, the optimal exploration path depends heavily on the objectives and constraints that both these operations and environments introduce. Typical environment constraints include partially known or completely unknown workspaces, limited-bandwidth communications, and sparse or dense clattered spaces. In such environments, the exploration robots must satisfy additional operational constraints, including time-critical goals, kinematic modeling, and resource limitations. Finding the optimal exploration path under these multiple constraints and objectives constitutes a challenging non-convex optimization problem. In our approach, we model the environment constraints in cost functions and utilize the cognitive-based adaptive optimization algorithm to meet time-critical objectives. The exploration path produced is optimal in the sense of globally minimizing the required time as well as maximizing the explored area of a partially unknown workspace. Since obstacles are sensed during operation, initial paths are possible to be blocked leading to a robot entrapment. A supervisor is triggered to signal a blocked passage and subsequently escape from the basin of cost function local minimum. Extensive simulations and comparisons in typical scenarios are presented to show the efficiency of the proposed approach.",
        "abstract": "Multi-objective robot exploration constitutes one of the most challenging tasks for autonomous robots performing in various operations and different environments. However, the optimal exploration path depends heavily on the objectives and constraints that both these operations and environments introduce. Typical environment constraints include partially known or completely unknown workspaces, limited-bandwidth communications, and sparse or dense clattered spaces. In such environments, the exploration robots must satisfy additional operational constraints, including time-critical goals, kinematic modeling, and resource limitations. Finding the optimal exploration path under these multiple constraints and objectives constitutes a challenging non-convex optimization problem. In our approach, we model the environment constraints in cost functions and utilize the cognitive-based adaptive optimization algorithm to meet time-critical objectives. The exploration path produced is optimal in the sense of globally minimizing the required time as well as maximizing the explored area of a partially unknown workspace. Since obstacles are sensed during operation, initial paths are possible to be blocked leading to a robot entrapment. A supervisor is triggered to signal a blocked passage and subsequently escape from the basin of cost function local minimum. Extensive simulations and comparisons in typical scenarios are presented to show the efficiency of the proposed approach.",
        "authors": "Angelos A. Amanatiadis, Elias B. Kosmatopoulos, Konstantinos Charalampous, Lefteris Doitsidis, Phillipos Tsalides, Savvas A. Chatzichristofis",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2283031",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, a new ordinary differential equation numerical integration method is successfully applied to various mathematical branches such as partial differential equation (PDE) boundary problems, PDE initial-boundary problems, tough nonlinear equations, and so forth. The new method does not use Jacobian, so it can handle very large systems, say the dimension N=1 000 000, or even larger. In addition, we give a very simple accelerating convergence approach for the linear algebraic equations arising from linear PDE boundary problems. All the numerical results show that the new method is very promising for super large scale systems.",
        "abstract": "In this paper, a new ordinary differential equation numerical integration method is successfully applied to various mathematical branches such as partial differential equation (PDE) boundary problems, PDE initial-boundary problems, tough nonlinear equations, and so forth. The new method does not use Jacobian, so it can handle very large systems, say the dimension N=1 000 000, or even larger. In addition, we give a very simple accelerating convergence approach for the linear algebraic equations arising from linear PDE boundary problems. All the numerical results show that the new method is very promising for super large scale systems.",
        "authors": "Tianmin Han, Yuhuan Han",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2280244",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A critical problem associated with surgical simulation is balancing deformation accuracy with a real-time performance. Although the canonical surface mass-spring model (MSM) can provide an excellent real-time performance, it fails to provide effective shape restoration behavior when generating large deformations. This significantly influences its deformation accuracy. To address this problem, this paper proposes a modified surface MSM. In the proposed MSM, a new flexion spring is first developed to oppose bending based on the included angle between the initial position vector and the deformational position vector, improving the shape restoration performance, and enhance the deformational accuracy of MSM; then, a new type of surface triangular topological unit is developed for enhancing the computational efficiency and better adapting to the different topological soft tissue deformational models. In addition, to further improve the accuracy of deformational interactions between the soft tissue and surgical instruments, we also propose two new collision detection algorithms. One is the discrete collision detection with the volumetric structure (DCDVS), applying a volumetric structure to extend the effective range of collision detection and the other is the hybrid collision detection with the volumetric structure (HCDVS), introducing the interpolation techniques of the continuous collision detection to DCDVS. Experimental results show that the proposed MSM with the DCDVS or the HCDVS can achieve accurate and stable shape restoration and show the real-time interactive capability in the virtual artery vessel and heart compared with the canonical surface MSM and new volume MSM.",
        "abstract": "A critical problem associated with surgical simulation is balancing deformation accuracy with a real-time performance. Although the canonical surface mass-spring model (MSM) can provide an excellent real-time performance, it fails to provide effective shape restoration behavior when generating large deformations. This significantly influences its deformation accuracy. To address this problem, this paper proposes a modified surface MSM. In the proposed MSM, a new flexion spring is first developed to oppose bending based on the included angle between the initial position vector and the deformational position vector, improving the shape restoration performance, and enhance the deformational accuracy of MSM; then, a new type of surface triangular topological unit is developed for enhancing the computational efficiency and better adapting to the different topological soft tissue deformational models. In addition, to further improve the accuracy of deformational interactions between the soft tissue and surgical instruments, we also propose two new collision detection algorithms. One is the discrete collision detection with the volumetric structure (DCDVS), applying a volumetric structure to extend the effective range of collision detection and the other is the hybrid collision detection with the volumetric structure (HCDVS), introducing the interpolation techniques of the continuous collision detection to DCDVS. Experimental results show that the proposed MSM with the DCDVS or the HCDVS can achieve accurate and stable shape restoration and show the real-time interactive capability in the virtual artery vessel and heart compared with the canonical surface MSM and new volume MSM.",
        "authors": "Chunquan Li, Jiajun Ding, Peter X. Liu, Yucheng Pan, Zhichao Hong",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883679",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The operating conditions for industrial batch production often cover a wide range in order to produce different products. Inconsistent working conditions and recipes may change the data properties, but the generated batches may share similar mechanisms in terms of their qualitative and quantitative knowledge domains. In this paper, we propose a transfer learning framework for both domains to improve the efficiency of monitoring in similar batch scenarios. First, a statistical pattern clustering strategy is developed for assessing and separating similar conditions. Based on this strategy, the phase-based generalized Procrustes analysis and the ordinary Procrustes analysis are proposed to produce the nominal representations and also to transfer quantitative knowledge by accommodating batch-wise and recipe-wise discrepancies. Furthermore, a multiphase Bayesian network is constructed for qualitative knowledge transfer and statistical modeling with the nominal representations. Finally, a systematic monitoring flowchart is established for fault detection and isolation based on a just-in-time transfer strategy. Under this framework, the efforts required for similar process modeling can be reduced and the monitoring efficiency can be improved. The feasibility and effectiveness of the proposed diagram for industrial uses are validated on a fed-batch penicillin fermentation process.",
        "abstract": "The operating conditions for industrial batch production often cover a wide range in order to produce different products. Inconsistent working conditions and recipes may change the data properties, but the generated batches may share similar mechanisms in terms of their qualitative and quantitative knowledge domains. In this paper, we propose a transfer learning framework for both domains to improve the efficiency of monitoring in similar batch scenarios. First, a statistical pattern clustering strategy is developed for assessing and separating similar conditions. Based on this strategy, the phase-based generalized Procrustes analysis and the ordinary Procrustes analysis are proposed to produce the nominal representations and also to transfer quantitative knowledge by accommodating batch-wise and recipe-wise discrepancies. Furthermore, a multiphase Bayesian network is constructed for qualitative knowledge transfer and statistical modeling with the nominal representations. Finally, a systematic monitoring flowchart is established for fault detection and isolation based on a just-in-time transfer strategy. Under this framework, the efforts required for similar process modeling can be reduced and the monitoring efficiency can be improved. The feasibility and effectiveness of the proposed diagram for industrial uses are validated on a fed-batch penicillin fermentation process.",
        "authors": "Furong Gao, Jinlin Zhu, Yuan Yao",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884652",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper investigates the problem of event-triggered dynamic output feedback control for networked Takagi-Sugeno (T-S) fuzzy systems with asynchronous premise variables. Our attention is focused on the design of an event-triggered fuzzy dynamic output feedback controller that guarantees that the closed-loop system is asymptotically stable. First, with consideration of the limited bandwidth and restricted network resource, an event-triggered communication scheme is proposed to save the limited network resource. Second, different from the existing results in the literature, a fuzzy dynamic output feedback controller with asynchronous premise variables is constructed such that the premise membership functions are not necessarily the same as the networked T-S fuzzy systems. Then, by using the Wirtinger inequality and some slack matrices, a less conservative stability and stabilization conditions for the existence of fuzzy dynamic output feedback controller are derived in the form of linear matrix inequalities, which can be solved by the standard MATLAB toolbox. Finally, an example is given to demonstrate the effectiveness of the designed event-triggered fuzzy dynamic output feedback controller.",
        "abstract": "This paper investigates the problem of event-triggered dynamic output feedback control for networked Takagi-Sugeno (T-S) fuzzy systems with asynchronous premise variables. Our attention is focused on the design of an event-triggered fuzzy dynamic output feedback controller that guarantees that the closed-loop system is asymptotically stable. First, with consideration of the limited bandwidth and restricted network resource, an event-triggered communication scheme is proposed to save the limited network resource. Second, different from the existing results in the literature, a fuzzy dynamic output feedback controller with asynchronous premise variables is constructed such that the premise membership functions are not necessarily the same as the networked T-S fuzzy systems. Then, by using the Wirtinger inequality and some slack matrices, a less conservative stability and stabilization conditions for the existence of fuzzy dynamic output feedback controller are derived in the form of linear matrix inequalities, which can be solved by the standard MATLAB toolbox. Finally, an example is given to demonstrate the effectiveness of the designed event-triggered fuzzy dynamic output feedback controller.",
        "authors": "Feng-Xia Xu, Guang-Tao Ran, Jun-Xiao Lu, Zhong-Da Lu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885212",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Object tracking is a difficult work in complex situations including crowded environment, occlusion, out of view, and fast motion. Recently, many tracking strategies have been designed to handle the object tracking in complex conditions. However, most of the designed methods are inefficient to tackle the target aspect ratio variation and disappearance problems during the long-term tracking. Hence, it is most important to design a tracking algorithm that effectively reduce the drifting problem and recapture the target from the tracking failure. In this paper, we proposed a robust correlation filter-based moving object tracker with scale adaptation and online re-detection. First, we trained a translation filter using kernelized correlation filter with the multiple features for identifying the initial target location in each frame. Second, we used the high confidence score of the correlation output to reduce the model-drifting problem. Third, we introduced a new online re-detection strategy to relocate the target at the time of tracking failure. This re-detection component activated dynamically based on the present and historical confidence scores of the target. To tackle the aspect ratio and scale variation problems, we used detection proposal with the correlation filter method. The experimental evaluation on the several benchmark datasets proved that our results significantly better compared with the other methods.",
        "abstract": "Object tracking is a difficult work in complex situations including crowded environment, occlusion, out of view, and fast motion. Recently, many tracking strategies have been designed to handle the object tracking in complex conditions. However, most of the designed methods are inefficient to tackle the target aspect ratio variation and disappearance problems during the long-term tracking. Hence, it is most important to design a tracking algorithm that effectively reduce the drifting problem and recapture the target from the tracking failure. In this paper, we proposed a robust correlation filter-based moving object tracker with scale adaptation and online re-detection. First, we trained a translation filter using kernelized correlation filter with the multiple features for identifying the initial target location in each frame. Second, we used the high confidence score of the correlation output to reduce the model-drifting problem. Third, we introduced a new online re-detection strategy to relocate the target at the time of tracking failure. This re-detection component activated dynamically based on the present and historical confidence scores of the target. To tackle the aspect ratio and scale variation problems, we used detection proposal with the correlation filter method. The experimental evaluation on the several benchmark datasets proved that our results significantly better compared with the other methods.",
        "authors": "Chengzhi Lyu, Guoqing Hu, Md Mojahidul Islam, Qianbo Liu, Wang Dan",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883650",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The aim of this paper is to develop a novel consensus ranking method that uses a mixed choice strategy for multiple criteria decision analysis (MCDA) under complex uncertainty based on Pythagorean fuzzy (PF) sets. The majority of MCDA methods have focused almost exclusively on “criterion-specific”choice tasks that are the tasks in which all alternatives are decomposed into distinct components and evaluated on specific criteria. However, in certain MCDA problems in practical applications, category-based choices tend to be more holistic in nature, especially in affective-like aspects. Therefore, this paper incorporates a mixed choice strategy (i.e., a combination of a category-based strategy and a criterion-specific strategy) into the core structure of the developed MCDA method. Furthermore, this paper utilizes the theory of Pythagorean fuzziness to provide a powerful modeling tool for complex and varied decision-making environments. Employing the developed concepts of a PF precedence index based on PF information and a disagreement indicator based on distances between rankings, this paper proposes a novel consensus ranking method by means of a comprehensive disagreement-based assignment model for addressing a mixed-choice-strategy-based MCDA problem in the PF context. As an application of the proposed methodology, a real-world case study of a luxury car selection problem is investigated. The application results, along with a comparative analysis, demonstrate the practicality and effectiveness of the developed approach, which is capable of handling hybrid category-based and criterion-specific choice tasks and managing complex uncertainty in practical situations.",
        "abstract": "The aim of this paper is to develop a novel consensus ranking method that uses a mixed choice strategy for multiple criteria decision analysis (MCDA) under complex uncertainty based on Pythagorean fuzzy (PF) sets. The majority of MCDA methods have focused almost exclusively on “criterion-specific”choice tasks that are the tasks in which all alternatives are decomposed into distinct components and evaluated on specific criteria. However, in certain MCDA problems in practical applications, category-based choices tend to be more holistic in nature, especially in affective-like aspects. Therefore, this paper incorporates a mixed choice strategy (i.e., a combination of a category-based strategy and a criterion-specific strategy) into the core structure of the developed MCDA method. Furthermore, this paper utilizes the theory of Pythagorean fuzziness to provide a powerful modeling tool for complex and varied decision-making environments. Employing the developed concepts of a PF precedence index based on PF information and a disagreement indicator based on distances between rankings, this paper proposes a novel consensus ranking method by means of a comprehensive disagreement-based assignment model for addressing a mixed-choice-strategy-based MCDA problem in the PF context. As an application of the proposed methodology, a real-world case study of a luxury car selection problem is investigated. The application results, along with a comparative analysis, demonstrate the practicality and effectiveness of the developed approach, which is capable of handling hybrid category-based and criterion-specific choice tasks and managing complex uncertainty in practical situations.",
        "authors": "Ting-Yu Chen",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884895",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper introduces self-manipulation as a new formal design methodology for legged robots with varying ground interactions. The term denotes a set of modeling choices that permit a uniform and body-centric representation of the equations of motion—essentially a guide to the selection and configuration of coordinate frames. We present the hybrid system kinematics, dynamics, and transitions in the form of a consistently structured representation that simplifies and unites the account of these, otherwise bewilderingly diverse differential algebraic equations. Cleaving as closely as possible to the modeling strategies developed within the mature manipulation literature, self-manipulation models can leverage those insights and results where applicable, while clarifying the fundamental differences. Our primary motivation is not to facilitate numerical simulation but rather to promote design insight. We instantiate the abstract formalism for a simplified model of RHex, and illustrate its utility by applying a variety of analytical and computational techniques to derive new results bearing on behaviors, controllers, and platform design. For each example, we present empirical results documenting the specific benefits of the new insight into the robot's transitions from standing to moving in place and to leaping.",
        "abstract": "This paper introduces self-manipulation as a new formal design methodology for legged robots with varying ground interactions. The term denotes a set of modeling choices that permit a uniform and body-centric representation of the equations of motion—essentially a guide to the selection and configuration of coordinate frames. We present the hybrid system kinematics, dynamics, and transitions in the form of a consistently structured representation that simplifies and unites the account of these, otherwise bewilderingly diverse differential algebraic equations. Cleaving as closely as possible to the modeling strategies developed within the mature manipulation literature, self-manipulation models can leverage those insights and results where applicable, while clarifying the fundamental differences. Our primary motivation is not to facilitate numerical simulation but rather to promote design insight. We instantiate the abstract formalism for a simplified model of RHex, and illustrate its utility by applying a variety of analytical and computational techniques to derive new results bearing on behaviors, controllers, and platform design. For each example, we present empirical results documenting the specific benefits of the new insight into the robot's transitions from standing to moving in place and to leaping.",
        "authors": "Aaron M. Johnson, Daniel E. Koditschek",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2263192",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "One of the main features of adaptive systems is an oscillatory convergence that exacerbates with the speed of adaptation. Recently, it has been shown that closed-loop reference models (CRMs) can result in improved transient performance over their open-loop counterparts in model reference adaptive control. In this paper, we quantify both the transient performance in the classical adaptive systems and their improvement with CRMs. In addition to deriving bounds on L-2 norms of the derivatives of the adaptive parameters that are shown to be smaller, an optimal design of CRMs is proposed that minimizes an underlying peaking phenomenon. The analytical tools proposed are shown to be applicable for a range of adaptive control problems including direct control and composite control with observer feedback. The presence of CRMs in adaptive backstepping and adaptive robot control is also discussed. Simulation results are presented throughout this paper to support the theoretical derivations.",
        "abstract": "One of the main features of adaptive systems is an oscillatory convergence that exacerbates with the speed of adaptation. Recently, it has been shown that closed-loop reference models (CRMs) can result in improved transient performance over their open-loop counterparts in model reference adaptive control. In this paper, we quantify both the transient performance in the classical adaptive systems and their improvement with CRMs. In addition to deriving bounds on L-2 norms of the derivatives of the adaptive parameters that are shown to be smaller, an optimal design of CRMs is proposed that minimizes an underlying peaking phenomenon. The analytical tools proposed are shown to be applicable for a range of adaptive control problems including direct control and composite control with observer feedback. The presence of CRMs in adaptive backstepping and adaptive robot control is also discussed. Simulation results are presented throughout this paper to support the theoretical derivations.",
        "authors": "Anuradha M. Annaswamy, Eugene Lavretsky, Travis E. Gibson",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2284005",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, a resonant gate driver for silicon carbide power MOSFET is proposed. This resonant gate driver contains four N-MOSFETs, a resonant inductor, and a capacitor. The proposed gate driver recycles the energy which is stored in the gate capacitor of the SiC MOSFET. The gate drive losses of the resonant gate driver are reduced greatly, and the switching frequency can reach MHz. The design and the loss analysis are introduced in this paper. Finally, the simulation model and the experimental platform of the resonant gate driver are built to validate the theoretical analysis. Both the simulation and experiment results are provided to verify the feasibility and high performance of the proposed resonant gate driver.",
        "abstract": "In this paper, a resonant gate driver for silicon carbide power MOSFET is proposed. This resonant gate driver contains four N-MOSFETs, a resonant inductor, and a capacitor. The proposed gate driver recycles the energy which is stored in the gate capacitor of the SiC MOSFET. The gate drive losses of the resonant gate driver are reduced greatly, and the switching frequency can reach MHz. The design and the loss analysis are introduced in this paper. Finally, the simulation model and the experimental platform of the resonant gate driver are built to validate the theoretical analysis. Both the simulation and experiment results are provided to verify the feasibility and high performance of the proposed resonant gate driver.",
        "authors": "Haifu Wu, Jianzhong Zhang, Jin Zhao, Yaodong Zhu, Yaqian Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885023",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Internet of Things technology has brought new opportunities for rural informatization. The quality of rural information environment is directly related to the level of rural informatization. So accurately understanding the status and level of information environment in rural areas is a prerequisite for rural informatization. This paper assesses differences in rural information environment by developing a framework based on a fuzzy comprehensive evaluation method to measure information environment changes. The assessment focuses on three aspects: one is the evaluation of the overall rural information environment based on macro-factors in time dimension, the other is in 31 different provinces in space dimension, and the third is the evaluation of regional rural information environment based on macro-and subjective factors. The Probit model reveals the influence of various factors on the rural information environment, which constitutes the primary and secondary indicators of the evaluation system. The fuzzy comprehensive evaluation method is adopted to evaluate the rural information environment, and the cluster analysis is applied to the results. The results show that the differences in rural information environment exist objectively, which are the result of a variety of subjective and objective factors. In addition, the differences in rural information environment promote the diversified development path of rural information construction. These findings have clear policy implication for the rural ministry, since understanding the changes and differences in the rural information environment is important for their successful development. Results of the experimental applications of this evaluation system are given to illustrate the proposed method.",
        "abstract": "Internet of Things technology has brought new opportunities for rural informatization. The quality of rural information environment is directly related to the level of rural informatization. So accurately understanding the status and level of information environment in rural areas is a prerequisite for rural informatization. This paper assesses differences in rural information environment by developing a framework based on a fuzzy comprehensive evaluation method to measure information environment changes. The assessment focuses on three aspects: one is the evaluation of the overall rural information environment based on macro-factors in time dimension, the other is in 31 different provinces in space dimension, and the third is the evaluation of regional rural information environment based on macro-and subjective factors. The Probit model reveals the influence of various factors on the rural information environment, which constitutes the primary and secondary indicators of the evaluation system. The fuzzy comprehensive evaluation method is adopted to evaluate the rural information environment, and the cluster analysis is applied to the results. The results show that the differences in rural information environment exist objectively, which are the result of a variety of subjective and objective factors. In addition, the differences in rural information environment promote the diversified development path of rural information construction. These findings have clear policy implication for the rural ministry, since understanding the changes and differences in the rural information environment is important for their successful development. Results of the experimental applications of this evaluation system are given to illustrate the proposed method.",
        "authors": "Gang Li, Huifeng Zhang, Yanfeng Jin",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885069",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Mixed models provide a novel approach to the analysis of radar tracking residuals by considering randomness from different sources. Through properly considering randomness, mixed models can provide greater power to determine the statistical significance of various parameters needed for radar calibration. This paper applies a mixed models approach to the analysis of radar tracking residuals from calibration satellites observed by the Cobra Dane radar and finds a time dependent bias in the azimuth residuals.",
        "abstract": "Mixed models provide a novel approach to the analysis of radar tracking residuals by considering randomness from different sources. Through properly considering randomness, mixed models can provide greater power to determine the statistical significance of various parameters needed for radar calibration. This paper applies a mixed models approach to the analysis of radar tracking residuals from calibration satellites observed by the Cobra Dane radar and finds a time dependent bias in the azimuth residuals.",
        "authors": "Carl Clifford Gaither, Christopher R. Jackson, Dawn C. Foley Loper, Jasmina Pozderac",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2263191",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Feature selection is the preliminary step in machine learning and data mining. It identifies the most important and relevant features within a dataset by eliminating the redundant or irrelevant features. The substantial benefits may include an improved performance in terms of high prediction accuracy, reduced computational complexity, and simply interpretable underlying models. In this paper, we present a novel framework to investigate and understand the importance of Monte Carlo tree search (MCTS) in feature selection for very high-dimensional datasets. We construct a binary feature selection tree where each node represents one of the two feature states: a feature is selected or not. The search starts with an empty root node reflecting that no feature is selected. Then, the search tree is expanded by adding nodes in an incremental fashion through MCTS-based simulations. Following tree and default policy, every iteration generates an initial feature subset, where a filter is used to select the top k features forming the candidate feature subset. The classification accuracy is used as the goodness or reward of the candidate feature subset and propagated backward up to the root node following the active path. Finally, the candidate subset with highest reward is selected as the best feature subset. Experiments are performed on 30 real-world datasets, including 14 very high-dimensional microarray datasets, and results are also compared with state-of-the-art methods in the literature, which proves the efficacy, validity, and significance of the proposed method.",
        "abstract": "Feature selection is the preliminary step in machine learning and data mining. It identifies the most important and relevant features within a dataset by eliminating the redundant or irrelevant features. The substantial benefits may include an improved performance in terms of high prediction accuracy, reduced computational complexity, and simply interpretable underlying models. In this paper, we present a novel framework to investigate and understand the importance of Monte Carlo tree search (MCTS) in feature selection for very high-dimensional datasets. We construct a binary feature selection tree where each node represents one of the two feature states: a feature is selected or not. The search starts with an empty root node reflecting that no feature is selected. Then, the search tree is expanded by adding nodes in an incremental fashion through MCTS-based simulations. Following tree and default policy, every iteration generates an initial feature subset, where a filter is used to select the top k features forming the candidate feature subset. The classification accuracy is used as the goodness or reward of the candidate feature subset and propagated backward up to the root node following the active path. Finally, the candidate subset with highest reward is selected as the best feature subset. Experiments are performed on 30 real-world datasets, including 14 very high-dimensional microarray datasets, and results are also compared with state-of-the-art methods in the literature, which proves the efficacy, validity, and significance of the proposed method.",
        "authors": "Jee-Hyong Lee, Muhammad Umar Chaudhry",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883537",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In order to estimate the state-of-charge (SoC) for all cells in the battery pack, this paper proposed an average cell model to represent every cell in the pack. The average cell model consisted of a basic model and a bias function. First, the parameter identification of the basic model was conducted, and the inconsistencies between cells were calibrated by the uncertainties of the basic model parameters. Second, artificial neural networks were used to construct the response surface approximate model of the bias function. In order to make the average cell model more adaptable to different working conditions, a novel bias function considering the polarization voltage and the temperature was proposed to correct the basic model, and it was compared with other bias functions. Then, the extended Kalman filtering algorithm was used for SoC estimation based on the corrected model. Finally, a case study with six lithium-ion battery cells was performed for the verification and evaluation of the proposed method. The results indicated that the average model corrected by the proposed bias function showed good adaptability to different working conditions, and the maximum absolute SoC estimate errors of all cells in the battery pack were less than 2% at 25°C, and 3.5%at 10 °C or 40 °C.",
        "abstract": "In order to estimate the state-of-charge (SoC) for all cells in the battery pack, this paper proposed an average cell model to represent every cell in the pack. The average cell model consisted of a basic model and a bias function. First, the parameter identification of the basic model was conducted, and the inconsistencies between cells were calibrated by the uncertainties of the basic model parameters. Second, artificial neural networks were used to construct the response surface approximate model of the bias function. In order to make the average cell model more adaptable to different working conditions, a novel bias function considering the polarization voltage and the temperature was proposed to correct the basic model, and it was compared with other bias functions. Then, the extended Kalman filtering algorithm was used for SoC estimation based on the corrected model. Finally, a case study with six lithium-ion battery cells was performed for the verification and evaluation of the proposed method. The results indicated that the average model corrected by the proposed bias function showed good adaptability to different working conditions, and the maximum absolute SoC estimate errors of all cells in the battery pack were less than 2% at 25°C, and 3.5%at 10 °C or 40 °C.",
        "authors": "Hao Lei, Rui Xiong, Xiaokai Chen",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884844",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "We consider the channel estimation of millimeter wave (mmWave) multiple-input multiple-output systems, where both the transmitter and receiver adopt hybrid beamforming structure. Due to the spatial sparsity of the mmWave channel, it can be reconstructed by estimating the direction and gain of the paths. Leveraging this feature, we propose a channel estimation algorithm based on subspace fitting to estimate the path directions, and the path gains are obtained using the least squares method. However, similar to the most existing mmWave channel estimation schemes, the proposed algorithm requires a two-dimensional search in candidate angle space, which is very complicated. In order to reduce the computational complexity, we develop a low-complexity channel estimation algorithm using the orthogonal matching pursuit (OMP) method, which significantly reduces the computational complexity. However, when the paths are strongly correlated, the channel estimation accuracy will decrease. To overcome this defect, we further develop a low-complexity method based on subspace fitting. This algorithm makes a trade-off between the computational complexity and channel estimation accuracy. Furthermore, the pilot beam pattern for different hybrid beamforming structure is designed using infinitely and quantized phase shifters to improve the signal-to-noise ratio of the pilot signals. In addition, the proposed channel estimation methods can also be used in the multi-user scenario. Simulation results demonstrate that the proposed subspace fitting method outperforms the existing methods when the angular resolution is the same. Meanwhile, the low-complexity OMP and subspace fitting methods have a good performance in single path scenario when compared to subspace fitting method, even if the computation complexity is lower. Moreover, the performance of the low-complexity subspace fitting method is close to the subspace fitting method.",
        "abstract": "We consider the channel estimation of millimeter wave (mmWave) multiple-input multiple-output systems, where both the transmitter and receiver adopt hybrid beamforming structure. Due to the spatial sparsity of the mmWave channel, it can be reconstructed by estimating the direction and gain of the paths. Leveraging this feature, we propose a channel estimation algorithm based on subspace fitting to estimate the path directions, and the path gains are obtained using the least squares method. However, similar to the most existing mmWave channel estimation schemes, the proposed algorithm requires a two-dimensional search in candidate angle space, which is very complicated. In order to reduce the computational complexity, we develop a low-complexity channel estimation algorithm using the orthogonal matching pursuit (OMP) method, which significantly reduces the computational complexity. However, when the paths are strongly correlated, the channel estimation accuracy will decrease. To overcome this defect, we further develop a low-complexity method based on subspace fitting. This algorithm makes a trade-off between the computational complexity and channel estimation accuracy. Furthermore, the pilot beam pattern for different hybrid beamforming structure is designed using infinitely and quantized phase shifters to improve the signal-to-noise ratio of the pilot signals. In addition, the proposed channel estimation methods can also be used in the multi-user scenario. Simulation results demonstrate that the proposed subspace fitting method outperforms the existing methods when the angular resolution is the same. Meanwhile, the low-complexity OMP and subspace fitting methods have a good performance in single path scenario when compared to subspace fitting method, even if the computation complexity is lower. Moreover, the performance of the low-complexity subspace fitting method is close to the subspace fitting method.",
        "authors": "Didi Zhang, Wei Xiang, Yafeng Wang, Zhong Su",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884324",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, we have designed, simulated, fabricated, and characterized a clamped-clamped micro mechanical structure-based shunt capacitive RF MEMS switch. The clamped-clamped micromechanical structure is micromachined using a gold metal thickness of 500 nm. AlN is used as a dielectric material, and it is deposited using the dc sputtering PVD process. In the MEMS technology, particularly in devices fabrication, releasing the membrane is a difficult task, and here, we have presented a novel wet process to release the membrane. Primarily, the S1813 sacrificial layer is etched by using the piranha solution and cleaned with the IPA solution. Critical point drying is done after fabrication to reduce the stiction effect on the switch. Overall, the switch requires the pull-in voltage of 5.5 V for 1.8-μm displacement. In the process of optimization, primarily, the switch is designed and simulated using finite-element method tools. The reliability of the capacitive RF MEMS switches depends on the stiction problem caused by dielectric charging, and the proposed capacitive switch dielectric charging behavior is characterized using the CV curve method.",
        "abstract": "In this paper, we have designed, simulated, fabricated, and characterized a clamped-clamped micro mechanical structure-based shunt capacitive RF MEMS switch. The clamped-clamped micromechanical structure is micromachined using a gold metal thickness of 500 nm. AlN is used as a dielectric material, and it is deposited using the dc sputtering PVD process. In the MEMS technology, particularly in devices fabrication, releasing the membrane is a difficult task, and here, we have presented a novel wet process to release the membrane. Primarily, the S1813 sacrificial layer is etched by using the piranha solution and cleaned with the IPA solution. Critical point drying is done after fabrication to reduce the stiction effect on the switch. Overall, the switch requires the pull-in voltage of 5.5 V for 1.8-μm displacement. In the process of optimization, primarily, the switch is designed and simulated using finite-element method tools. The reliability of the capacitive RF MEMS switches depends on the stiction problem caused by dielectric charging, and the proposed capacitive switch dielectric charging behavior is characterized using the CV curve method.",
        "authors": "K. Girija Sravani, K. Srinivasa Rao, Koushik Guha, Lakshmi Narayana Thalluri",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883353",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Doris, the social robot girl, is under development to be employed in museums and trade fairs as a tour guide. External sensorial information must be inputted so that Doris moves around each new location by using landmark identification points that can improve the real localization of the robot in combination with an extended Kalman filter. Doris is equipped with a semantic map that contains several information points such as the building structure, sites that the robot must pass, features (obstacles) of the built environment, and landmark locations. Three additional sensors were installed on Doris: a laser range finder LMS-200, an omnidirectional Mobotix C25 camera, and an RFID system Speedway Revolution 220 by Impinj. The use of these sensors implies the use of different types of landmarks: 35-cm-high circular landmarks, placed on the ground and covered with a reflective laser-detectable material; markers similar to QR codes placed at 250 cm above the ground level that the omnidirectional camera can identify; and RFID detectable dogbone antennas. One contribution is to prove a simple methodology of localization by using sensor fusion with a semantic map, without mapping the whole environment by creating a point cloud map and without using the SLAM technique. Additionally, another contribution for the research is to define a good methodology for a precise sensors calibration. The initial results showed that each sensor functions efficiently, when using only the laser and the camera, due to the low accuracy of the RFID system alone. The final results show the behavior of the robot localization in the presence of people and different objects when both sensors are working at the same time. Occlusions may affect the reflective landmarks or visual markers. Therefore, the sensor fusion is implemented to achieve better robustness in the location estimation.",
        "abstract": "Doris, the social robot girl, is under development to be employed in museums and trade fairs as a tour guide. External sensorial information must be inputted so that Doris moves around each new location by using landmark identification points that can improve the real localization of the robot in combination with an extended Kalman filter. Doris is equipped with a semantic map that contains several information points such as the building structure, sites that the robot must pass, features (obstacles) of the built environment, and landmark locations. Three additional sensors were installed on Doris: a laser range finder LMS-200, an omnidirectional Mobotix C25 camera, and an RFID system Speedway Revolution 220 by Impinj. The use of these sensors implies the use of different types of landmarks: 35-cm-high circular landmarks, placed on the ground and covered with a reflective laser-detectable material; markers similar to QR codes placed at 250 cm above the ground level that the omnidirectional camera can identify; and RFID detectable dogbone antennas. One contribution is to prove a simple methodology of localization by using sensor fusion with a semantic map, without mapping the whole environment by creating a point cloud map and without using the SLAM technique. Additionally, another contribution for the research is to define a good methodology for a precise sensors calibration. The initial results showed that each sensor functions efficiently, when using only the laser and the camera, due to the low accuracy of the RFID system alone. The final results show the behavior of the robot localization in the presence of people and different objects when both sensors are working at the same time. Occlusions may affect the reflective landmarks or visual markers. Therefore, the sensor fusion is implemented to achieve better robustness in the location estimation.",
        "authors": "Biel Piero E. Alvarado Vasquez, Fernando Matia, Paloma De La Puente, Ruben Gonzalez",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885648",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, it is theoretically demonstrated that a quadrotor can track trajectories using a low-cost global positioning system with signal losses in unpredictable instants. The lack of a continuous altitude output and the signal loss are solved with certain limitations using two-hybrid linear observers. This type of observer is originally employed to compensate for data loss in computer networks. This was possible due to the sufficient conditions for uniform global asymptotic stability theorems of hybrid systems with persistent jumping. This concept is based on a dynamic hybrid system that guarantees a jumping condition with a Lyapunov function constant when the system flows and decreases during jumps. The quadrotor was modeled like a linear time-invariant system. The results are presented using simulations.",
        "abstract": "In this paper, it is theoretically demonstrated that a quadrotor can track trajectories using a low-cost global positioning system with signal losses in unpredictable instants. The lack of a continuous altitude output and the signal loss are solved with certain limitations using two-hybrid linear observers. This type of observer is originally employed to compensate for data loss in computer networks. This was possible due to the sufficient conditions for uniform global asymptotic stability theorems of hybrid systems with persistent jumping. This concept is based on a dynamic hybrid system that guarantees a jumping condition with a Lyapunov function constant when the system flows and decreases during jumps. The quadrotor was modeled like a linear time-invariant system. The results are presented using simulations.",
        "authors": "Elba Cinthya García-Estrada, Jesús Alberto Meda-Campaña, Jonathan Omega Escobedo-Alva, Luis Alberto Páramo-Carranza, Ricardo Tapia-Herrera",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883596",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Assessment of equipment trip is needed for proper estimation of interruption/disruption cost and voltage sag mitigation. The equipment trip depends on the severity of voltage sag and the tolerance of the equipment toward the sag. However, the occurrence of voltage tolerance of an equipment in between the two known bound levels is uncertain in nature. The existing evaluation methods for equipment trip analysis fail to properly assess this uncertain property of voltage tolerance curves. This paper presents a novel approach to assess the equipment trip by handling the uncertainties by using fuzzy probability and possibility distribution. A new method is proposed to transform a rigorously performed statistical data into a fuzzy possibility distribution function, which eliminates the ambiguity that comes with the non-standardized selection of membership function/possibility function. With the proposed method, the statistical data are used to extract the fuzzy probability distribution of voltage sag intensity, which is given by both the magnitude and time duration of voltage sags, while the concept of fuzzy probability is used to calculate the fuzzy trip probability or equipment failure probability. The proposed method is finally applied to estimate the number of trips for six different sensitive equipments connected to two practical Indian distribution systems.",
        "abstract": "Assessment of equipment trip is needed for proper estimation of interruption/disruption cost and voltage sag mitigation. The equipment trip depends on the severity of voltage sag and the tolerance of the equipment toward the sag. However, the occurrence of voltage tolerance of an equipment in between the two known bound levels is uncertain in nature. The existing evaluation methods for equipment trip analysis fail to properly assess this uncertain property of voltage tolerance curves. This paper presents a novel approach to assess the equipment trip by handling the uncertainties by using fuzzy probability and possibility distribution. A new method is proposed to transform a rigorously performed statistical data into a fuzzy possibility distribution function, which eliminates the ambiguity that comes with the non-standardized selection of membership function/possibility function. With the proposed method, the statistical data are used to extract the fuzzy probability distribution of voltage sag intensity, which is given by both the magnitude and time duration of voltage sags, while the concept of fuzzy probability is used to calculate the fuzzy trip probability or equipment failure probability. The proposed method is finally applied to estimate the number of trips for six different sensitive equipments connected to two practical Indian distribution systems.",
        "authors": "Arup Kumar Goswami, Chandra Prakash Gupta, Chinmaya Behera, Galiveeti Hemakumar Reddy, Girish Kumar Singh, Pranju Chakrapani",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884562",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Most of the link prediction algorithms for bipartite networks assume that the generation of link is based on a predefined prior assumption. However, for the real-world bipartite networks, the generation mechanism of link is still ambiguous due to their complexity. Consequently, these methods are not always obtaining satisfactory results in all the cases, as each network has its unique generation mechanisms of link. In this paper, by introducing the structure perturbation theory, we propose a non-parameter bipartite structural perturbation method (BiSPM) to predict unobserved links without making any assumption for link formation mechanism. We first make a small perturbation for the training set of a bipartite network. Then for the perturbed network, we use singular value decomposition to obtain the singular vectors and singular values. Finally, during the process of network reconstruction, we hypothesize that when the topological structure meets tiny disturbance in the bipartite network, the coordinate system (singular vectors) of the projection is invariant, but the scaling factors (singular values) will create a tiny change from the perspective of complex system stability. Thus, a new matrix is constructed for prediction by changing the singular values of the perturbed networks while fixing the singular vectors. Extensive experiments on a variety of real-world bipartite networks show that BiSPM achieves a more competitive and more robust performance in comparison with the state-of-art link prediction methods in bipartite networks.",
        "abstract": "Most of the link prediction algorithms for bipartite networks assume that the generation of link is based on a predefined prior assumption. However, for the real-world bipartite networks, the generation mechanism of link is still ambiguous due to their complexity. Consequently, these methods are not always obtaining satisfactory results in all the cases, as each network has its unique generation mechanisms of link. In this paper, by introducing the structure perturbation theory, we propose a non-parameter bipartite structural perturbation method (BiSPM) to predict unobserved links without making any assumption for link formation mechanism. We first make a small perturbation for the training set of a bipartite network. Then for the perturbed network, we use singular value decomposition to obtain the singular vectors and singular values. Finally, during the process of network reconstruction, we hypothesize that when the topological structure meets tiny disturbance in the bipartite network, the coordinate system (singular vectors) of the projection is invariant, but the scaling factors (singular values) will create a tiny change from the perspective of complex system stability. Thus, a new matrix is constructed for prediction by changing the singular values of the perturbed networks while fixing the singular vectors. Extensive experiments on a variety of real-world bipartite networks show that BiSPM achieves a more competitive and more robust performance in comparison with the state-of-art link prediction methods in bipartite networks.",
        "authors": "Pengfei Jiao, Wei Yu, Wenjun Wang, Xue Chen, Yueheng Sun",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883436",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A calibration framework for an ultra-wideband localization system employing inertial measurement units is presented. No external motion capture system or other sensors are required for the calibration procedure. Given a covariance function for the error in the range measurements, a range measurement model based on a Gaussian process is obtained by maximizing the joint likelihood of angular rate, acceleration, and range measurements. The framework is experimentally evaluated, and it is shown how the resulting measurement model, integrated in a standard Kalman filter, can be used for real-time localization on a platform with limited computational resources. The calibration significantly improves the localization accuracy for randomly generated trajectories and different localization system setups.",
        "abstract": "A calibration framework for an ultra-wideband localization system employing inertial measurement units is presented. No external motion capture system or other sensors are required for the calibration procedure. Given a covariance function for the error in the range measurements, a range measurement model based on a Gaussian process is obtained by maximizing the joint likelihood of angular rate, acceleration, and range measurements. The framework is experimentally evaluated, and it is shown how the resulting measurement model, integrated in a standard Kalman filter, can be used for real-time localization on a platform with limited computational resources. The calibration significantly improves the localization accuracy for randomly generated trajectories and different localization system setups.",
        "authors": "Anton Ledergerber, Raffaello D’andrea",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885195",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Two turn-key surface potential-based compact models are developed to simulate multigate transistors for integrated circuit (IC) designs. The BSIM-CMG (common-multigate) model is developed to simulate double-, triple-, and all-around-gate FinFETs and it is selected as the world's first industry-standard compact model for the FinFET. The BSIM-IMG (independent-multigate) model is developed for independent double-gate, ultrathin body (UTB) transistors, capturing the dynamic threshold voltage adjustment with back gate bias. Starting from long-channel devices, the basic models are first obtained using a Poisson-carrier transport approach. The basic models agree with the results of numerical two-dimensional device simulators. The real-device effects then augment the basic models. All the important real-device effects, such as short-channel effects (SCEs), quantum mechanical confinement effects, mobility degradation, and parasitics are included in the models. BSIM-CMG and BSIM-IMG have been validated with hardware silicon-based data from multiple technologies. The developed models also meet the stringent quality assurance tests expected of production level models.",
        "abstract": "Two turn-key surface potential-based compact models are developed to simulate multigate transistors for integrated circuit (IC) designs. The BSIM-CMG (common-multigate) model is developed to simulate double-, triple-, and all-around-gate FinFETs and it is selected as the world's first industry-standard compact model for the FinFET. The BSIM-IMG (independent-multigate) model is developed for independent double-gate, ultrathin body (UTB) transistors, capturing the dynamic threshold voltage adjustment with back gate bias. Starting from long-channel devices, the basic models are first obtained using a Poisson-carrier transport approach. The basic models agree with the results of numerical two-dimensional device simulators. The real-device effects then augment the basic models. All the important real-device effects, such as short-channel effects (SCEs), quantum mechanical confinement effects, mobility degradation, and parasitics are included in the models. BSIM-CMG and BSIM-IMG have been validated with hardware silicon-based data from multiple technologies. The developed models also meet the stringent quality assurance tests expected of production level models.",
        "authors": "Ali M. Niknejad, Juan Pablo Duarte, Navid Paydavosi, Sriramkumar Venugopalan, Srivatsava Jandhyala, Yogesh Singh Chauhan",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2260816",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The distributed representation of knowledge graphs (KGs), which embeds the structured graphs into low-dimensional embedding spaces, is widely used to facilitate various applications of AI, such as information retrieval and question answering. The primary elements of KGs, the entities viewed as nodes and the relations regarded as links between entities, naturally make up the local embedding context for each other, which is called the multi-restriction property of KGs. However, this property is not fully explored by previous models, where either only part of the multi-restriction is captured, or the capability of the embedding model is limited to a fixed function without clear interpretation. To address this issue, we propose TBNN, a triple-branch neural network to learn the embeddings of KGs. In particular, the embedding of any element of a KG is determined by its multi-restriction via an interaction layer followed by parallel branched layers. Thus, the entities and relations can be treated equivalently in spite of their seeming differences in the original KG. We define the loss function of TBNN based on the confidence score of the three elements of each triple. In addition, we propose using the log-sum-exp pairwise loss to smooth the hinge loss, which results in better performance. Empirically, we evaluate our model on the tasks of link prediction and triple classification with the subsets of WordNet and Freebase. Experiment results show that our model performs better than the baselines, especially providing stable performance for relations with different mapping properties.",
        "abstract": "The distributed representation of knowledge graphs (KGs), which embeds the structured graphs into low-dimensional embedding spaces, is widely used to facilitate various applications of AI, such as information retrieval and question answering. The primary elements of KGs, the entities viewed as nodes and the relations regarded as links between entities, naturally make up the local embedding context for each other, which is called the multi-restriction property of KGs. However, this property is not fully explored by previous models, where either only part of the multi-restriction is captured, or the capability of the embedding model is limited to a fixed function without clear interpretation. To address this issue, we propose TBNN, a triple-branch neural network to learn the embeddings of KGs. In particular, the embedding of any element of a KG is determined by its multi-restriction via an interaction layer followed by parallel branched layers. Thus, the entities and relations can be treated equivalently in spite of their seeming differences in the original KG. We define the loss function of TBNN based on the confidence score of the three elements of each triple. In addition, we propose using the log-sum-exp pairwise loss to smooth the hinge loss, which results in better performance. Empirically, we evaluate our model on the tasks of link prediction and triple classification with the subsets of WordNet and Freebase. Experiment results show that our model performs better than the baselines, especially providing stable performance for relations with different mapping properties.",
        "authors": "Chunhong Zhang, Tingting Sun, Xiao Han, Yang Ji, Zheng Hu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884012",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper addresses a local minima problem for multiple unmanned aerial vehicles (UAVs) in the process of collision avoidance by using the artificial potential field method, thereby enabling UAVs to avoid the obstacle effectively in 3-D space. The main contribution is to propose a collision avoidance control algorithm based on the virtual structure and the “leader-follower”control strategy in 3-D space that can avoid the obstacle effectively and then track the motion target. The three UAVs constitute the regular triangular formation as the control object, the virtual leader flight trajectory as the expected path, the obstacles as the simplified cylinders, and the artificial potential fields around them as approximately spherical surfaces. The attractive force of the artificial potential field can guide the virtual leader to track the target. At the same time, the follower tracks the leader to maintain the formation flight. The effect of the repulsive force can avoid the collision between the UAVs and arrange the followers such that they are evenly distributed on the spherical surface. Moreover, the follower's specific order and position are not required. The collision path of the UAV formation depends on the artificial potential field with the two composite vectors, and every UAV may choose the optimal path to avoid the obstacle and reconfigure the regular triangular formation flight after passing the obstacle. The effectiveness of the proposed collision avoidance control algorithm is fully proved by simulation tests. Meanwhile, we also provide a new concept for multi-UAV formation avoidance of an obstacle.",
        "abstract": "This paper addresses a local minima problem for multiple unmanned aerial vehicles (UAVs) in the process of collision avoidance by using the artificial potential field method, thereby enabling UAVs to avoid the obstacle effectively in 3-D space. The main contribution is to propose a collision avoidance control algorithm based on the virtual structure and the “leader-follower”control strategy in 3-D space that can avoid the obstacle effectively and then track the motion target. The three UAVs constitute the regular triangular formation as the control object, the virtual leader flight trajectory as the expected path, the obstacles as the simplified cylinders, and the artificial potential fields around them as approximately spherical surfaces. The attractive force of the artificial potential field can guide the virtual leader to track the target. At the same time, the follower tracks the leader to maintain the formation flight. The effect of the repulsive force can avoid the collision between the UAVs and arrange the followers such that they are evenly distributed on the spherical surface. Moreover, the follower's specific order and position are not required. The collision path of the UAV formation depends on the artificial potential field with the two composite vectors, and every UAV may choose the optimal path to avoid the obstacle and reconfigure the regular triangular formation flight after passing the obstacle. The effectiveness of the proposed collision avoidance control algorithm is fully proved by simulation tests. Meanwhile, we also provide a new concept for multi-UAV formation avoidance of an obstacle.",
        "authors": "Jialong Zhang, Jianguo Yan, Pu Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885003",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Location has become an essential part of the next-generation Internet of Things systems. This paper proposes a multi-sensor-based 3D indoor localization approach. Compared with the existing 3D localization methods, this paper presents a wireless received signal strength (RSS)-profile-based floor-detection approach to enhance RSS-based floor detection. The profile-based floor detection is further integrated with the barometer data to gain more reliable estimations of the height and the barometer bias. Furthermore, the data from inertial sensors, magnetometers, and a barometer are integrated with the RSS data through an extend Kalman filter. The proposed multi-sensor integration algorithm provided more robust and smoother floor detection and 3D localization solutions than the existing methods.",
        "abstract": "Location has become an essential part of the next-generation Internet of Things systems. This paper proposes a multi-sensor-based 3D indoor localization approach. Compared with the existing 3D localization methods, this paper presents a wireless received signal strength (RSS)-profile-based floor-detection approach to enhance RSS-based floor detection. The profile-based floor detection is further integrated with the barometer data to gain more reliable estimations of the height and the barometer bias. Furthermore, the data from inertial sensors, magnetometers, and a barometer are integrated with the RSS data through an extend Kalman filter. The proposed multi-sensor integration algorithm provided more robust and smoother floor detection and 3D localization solutions than the existing methods.",
        "authors": "Naser El-Sheimy, Peng Zhang, Ruizhi Chen, You Li, Zhe He, Zhouzheng Gao",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883869",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The converters and their control strategies play an important role in converting, transmitting, and improving the performance of high-voltage direct current (HVDC) system. There are different types of converter and their control strategies being employed in the HVDC system, such as line-commutated converter and voltage source converter (VSC). However, the existing converter controllers have still some limitations on certain deficiencies in certain aspects such as in weak AC grid or at high voltage and power level. Thus, an advanced converter control strategy is very much important in order to ensure optimal power transfer with minimal loss and stable voltage. This paper presents a comprehensive review of the advanced control strategies to address the problems and enhance the performance of the VSC-based HVDC (VSC-HVDC) transmission system. A detailed study on the overview of VSC-HVDC and their converter classifications are investigated. The main contribution of this paper is to carry out the different types of VSC-HVDC control strategies in controlling voltage, current, power, and the control parameters of the HVDC transmission system. This paper also highlights several factors, challenges, and problems of the conventional VSC-HVDC controllers. Furthermore, this paper provides some suggestions for the advanced control for the future research and development of the HVDC system.",
        "abstract": "The converters and their control strategies play an important role in converting, transmitting, and improving the performance of high-voltage direct current (HVDC) system. There are different types of converter and their control strategies being employed in the HVDC system, such as line-commutated converter and voltage source converter (VSC). However, the existing converter controllers have still some limitations on certain deficiencies in certain aspects such as in weak AC grid or at high voltage and power level. Thus, an advanced converter control strategy is very much important in order to ensure optimal power transfer with minimal loss and stable voltage. This paper presents a comprehensive review of the advanced control strategies to address the problems and enhance the performance of the VSC-based HVDC (VSC-HVDC) transmission system. A detailed study on the overview of VSC-HVDC and their converter classifications are investigated. The main contribution of this paper is to carry out the different types of VSC-HVDC control strategies in controlling voltage, current, power, and the control parameters of the HVDC transmission system. This paper also highlights several factors, challenges, and problems of the conventional VSC-HVDC controllers. Furthermore, this paper provides some suggestions for the advanced control for the future research and development of the HVDC system.",
        "authors": "A. Hussain, I. Hussin, M. A. Hannan, M. M. Hoque, M. S. Hossain Lipu, P. J. Ker",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885010",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Randomized response mechanisms for guaranteeing crowdsourcing data privacy have attracted scholarly attention; aggregators can ensure privacy by collecting only randomized data, and individuals can have plausible deniability regarding their responses. With these mechanisms, analysts employed by organizations can still make predictions and conduct analyses using the randomized data. Existing randomized response-based data collection solutions have severely restricted functionality and usability, resulting in impractical and inefficient systems. Therefore, we developed a randomized response-based privacy-preserving crowdsourcing data collection and analysis mechanism. We designed a complementary randomized response (C-RR) method to guarantee individuals' data privacy and to preserve features from the original data for analysis. We formalized a machine learning framework; our proposed method uses randomized data in the form of binary vectors to generate a learning network. Extensive experiments on real-world data sets demonstrated that our heavy-hitters estimation scheme, which applies C-RR and our data learning model, significantly outperformed existing estimation schemes in terms of data analysis.",
        "abstract": "Randomized response mechanisms for guaranteeing crowdsourcing data privacy have attracted scholarly attention; aggregators can ensure privacy by collecting only randomized data, and individuals can have plausible deniability regarding their responses. With these mechanisms, analysts employed by organizations can still make predictions and conduct analyses using the randomized data. Existing randomized response-based data collection solutions have severely restricted functionality and usability, resulting in impractical and inefficient systems. Therefore, we developed a randomized response-based privacy-preserving crowdsourcing data collection and analysis mechanism. We designed a complementary randomized response (C-RR) method to guarantee individuals' data privacy and to preserve features from the original data for analysis. We formalized a machine learning framework; our proposed method uses randomized data in the form of binary vectors to generate a learning network. Extensive experiments on real-world data sets demonstrated that our heavy-hitters estimation scheme, which applies C-RR and our data learning model, significantly outperformed existing estimation schemes in terms of data analysis.",
        "authors": "Bo-Cheng Lin, Yao-Tung Tsou",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884511",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The alternating current potential drop technique has been widely used to measure subsurface cracks in metal structures. However, the application of the technique to random cracks has, to date, been limited. By using a multidirectional alternating current potential drop technique, the angle between crack and exciting electrode wire changes from 0° – 90° to 67.5° – 90°, which considerably expanded the ranges of detection. Simulation and experiment results showed that this technique can accurately measure the depth of random cracks.",
        "abstract": "The alternating current potential drop technique has been widely used to measure subsurface cracks in metal structures. However, the application of the technique to random cracks has, to date, been limited. By using a multidirectional alternating current potential drop technique, the angle between crack and exciting electrode wire changes from 0° – 90° to 67.5° – 90°, which considerably expanded the ranges of detection. Simulation and experiment results showed that this technique can accurately measure the depth of random cracks.",
        "authors": "Fangji Gan, Shiping Zhao, Wenyang Li, Xiaoming He, Yongjie Zhou",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883757",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "B-double is one kind of vehicle combinations with three vehicle units structurally and widely applied in cargo transport, so its directional stability analysis is crucial but more complex than a car. Directional stability was related with vehicle operating conditions and structure parameters. A basic linear dynamic model with four degrees of freedom for B-double was developed with a special method for simplification in a calculation process. The method was verified from the simulation results with MATLAB. The step input is tractor's steering wheel angle, so the steady-state yaw rate gains with understeer gradient for three vehicle units were solved. Steady-state relative gain was also derived and has an impact on directional stability, which could be influenced by the structure parameters, such as body mass (cargo weight), tire cornering stiffness of rear axle, location of mass center, wheelbase, and location of articulated point. The results indicate that increasing body mass and tire cornering stiffness of rear axle appropriately, moving backward mass center, and moving forward articulated points could increase the understeer gradient, which results in the improvement of directional stability. It is theoretical basis for the stability tests and structure design of B-double.",
        "abstract": "B-double is one kind of vehicle combinations with three vehicle units structurally and widely applied in cargo transport, so its directional stability analysis is crucial but more complex than a car. Directional stability was related with vehicle operating conditions and structure parameters. A basic linear dynamic model with four degrees of freedom for B-double was developed with a special method for simplification in a calculation process. The method was verified from the simulation results with MATLAB. The step input is tractor's steering wheel angle, so the steady-state yaw rate gains with understeer gradient for three vehicle units were solved. Steady-state relative gain was also derived and has an impact on directional stability, which could be influenced by the structure parameters, such as body mass (cargo weight), tire cornering stiffness of rear axle, location of mass center, wheelbase, and location of articulated point. The results indicate that increasing body mass and tire cornering stiffness of rear axle appropriately, moving backward mass center, and moving forward articulated points could increase the understeer gradient, which results in the improvement of directional stability. It is theoretical basis for the stability tests and structure design of B-double.",
        "authors": "Guojun Wang, Hongfei Liu, Hongguo Xu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883675",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "For local homing navigation, a mobile robot is supposed to return home using snapshots of the surrounding environment. It basically follows the snapshot model, comparing the home snapshot and the current view to determine the homing direction. In this paper, we suggest a high-order moment potential to describe the landmark feature distribution for local homing navigation. The moment potential function calculates the sum of products of the feature and the distance of landmark particles as a holistic view, allowing a high order of the distance. It effectively combines the range sensor values of landmarks in the current view and the visual features. By analogy with the moment in physics, the center of the moment is estimated as the reference point, which is the unique convergence point in the convex moment potential from any view, and using the property, the gradient of moment potential at the current position and home location can derive the homing vector. We provide a proof of convergence for any moment potential with order greater than or equal to one. Also, we demonstrate homing performances with various moment models in real environments to validate our models. The suggested moment models combining both landmark distance and visual feature have better performances than the visual information alone, and high-order moment potentials can be searched to obtain a better description of landmark distribution for a given environment.",
        "abstract": "For local homing navigation, a mobile robot is supposed to return home using snapshots of the surrounding environment. It basically follows the snapshot model, comparing the home snapshot and the current view to determine the homing direction. In this paper, we suggest a high-order moment potential to describe the landmark feature distribution for local homing navigation. The moment potential function calculates the sum of products of the feature and the distance of landmark particles as a holistic view, allowing a high order of the distance. It effectively combines the range sensor values of landmarks in the current view and the visual features. By analogy with the moment in physics, the center of the moment is estimated as the reference point, which is the unique convergence point in the convex moment potential from any view, and using the property, the gradient of moment potential at the current position and home location can derive the homing vector. We provide a proof of convergence for any moment potential with order greater than or equal to one. Also, we demonstrate homing performances with various moment models in real environments to validate our models. The suggested moment models combining both landmark distance and visual feature have better performances than the visual information alone, and high-order moment potentials can be searched to obtain a better description of landmark distribution for a given environment.",
        "authors": "Changmin Lee, Daeeun Kim",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2882677",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper presents an experimental validation of the aerodynamic sectional modeling approach as a general methodology to model the aerodynamic effects on aerial vehicles. The introduced modeling scheme also takes into consideration important effects such as engine load torque and landing gear contact. The dynamic model is derived after Kirchhoff’s formulation expressed as a function of the extended momentum vector. The effective forces and torques are included as the addition of independent sources. The aerodynamic effects are computed as the summation of forces and torques over each body section on the airplane, and the engine thrust and load are included in simulation by means of experimental transfer functions. The experimental validation is conducted using a RC-controlled fixed-wing airplane PT-40 instrumented with the proper avionics to measure its pose (position and attitude) for trajectory tracking purposes. A comparative analysis shows that the analytical model and the experimental setup describe similar qualitative behavior in an open-loop maneuver and reveals that the propeller load torque produces significant aerodynamic effects on both longitudinal and lateral-directional stability of the vehicle.",
        "abstract": "This paper presents an experimental validation of the aerodynamic sectional modeling approach as a general methodology to model the aerodynamic effects on aerial vehicles. The introduced modeling scheme also takes into consideration important effects such as engine load torque and landing gear contact. The dynamic model is derived after Kirchhoff’s formulation expressed as a function of the extended momentum vector. The effective forces and torques are included as the addition of independent sources. The aerodynamic effects are computed as the summation of forces and torques over each body section on the airplane, and the engine thrust and load are included in simulation by means of experimental transfer functions. The experimental validation is conducted using a RC-controlled fixed-wing airplane PT-40 instrumented with the proper avionics to measure its pose (position and attitude) for trajectory tracking purposes. A comparative analysis shows that the analytical model and the experimental setup describe similar qualitative behavior in an open-loop maneuver and reveals that the propeller load torque produces significant aerodynamic effects on both longitudinal and lateral-directional stability of the vehicle.",
        "authors": "Angel Flores-Abad, Ernesto Olguín-Díaz, Manuel Nandayapa, Miguel Angel García Terán",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2880179",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Enabling users conveniently accessible to their most interesting jobs or candidates is an important and challenging task for professional network recommenders in professional social networks (PSNs). Nowadays, PSNs accommodate tremendous and diverse users and face continuous uploading and high data freshness. In this paper, an online mining and predicting system is proposed for personalized job or candidate recommendation with big-data support. It considers the users’ explicit information as context to achieve personalized recommendation. In addition, the system utilizes the reward extracted from online implicit information of the previous users with similar context information to relieve the cold start problem. Furthermore, a tree-based method is introduced to address large-volume items by effectively analyzing them in cluster level and thus reduce the computational load. Considering the dynamic property of PSNs, our model is adaptive for the expanding dataset, enabling it to real-timely make accurate recommendations for the incessant new arrivals. Moreover, theoretical analysis shows that our algorithm achieves sublinear regret over time. Finally, extensive experiments are conducted to test our algorithm based on the real dataset from ACM RecSys Challenge1and validate the outstanding performance of our algorithm when compared with other existing algorithms.1http://www.recsyschallenge.com/2017",
        "abstract": "Enabling users conveniently accessible to their most interesting jobs or candidates is an important and challenging task for professional network recommenders in professional social networks (PSNs). Nowadays, PSNs accommodate tremendous and diverse users and face continuous uploading and high data freshness. In this paper, an online mining and predicting system is proposed for personalized job or candidate recommendation with big-data support. It considers the users’ explicit information as context to achieve personalized recommendation. In addition, the system utilizes the reward extracted from online implicit information of the previous users with similar context information to relieve the cold start problem. Furthermore, a tree-based method is introduced to address large-volume items by effectively analyzing them in cluster level and thus reduce the computational load. Considering the dynamic property of PSNs, our model is adaptive for the expanding dataset, enabling it to real-timely make accurate recommendations for the incessant new arrivals. Moreover, theoretical analysis shows that our algorithm achieves sublinear regret over time. Finally, extensive experiments are conducted to test our algorithm based on the real dataset from ACM RecSys Challenge1and validate the outstanding performance of our algorithm when compared with other existing algorithms.1http://www.recsyschallenge.com/2017",
        "authors": "Kehao Wang, Menglan Hu, Pan Zhou, Shaokang Dong, Shimin Gong, Wenbo Chen",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883953",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "As one of the most important areas of public safety and security, intelligent video surveillance is an indispensable part of the urban Internet of Things infrastructure. Person re-identification (person re-ID), which aims to track and recognize a person in a multi-camera scene, is mostly viewed as an image retrieval problem, and this task has been greatly boosted by deep convolutional neural networks (CNNs) in recent years. In practice, person re-ID usually adopts automatic detectors to obtain cropped pedestrian images, and CNNs are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. We incorporate the deformable convolution module to the traditional baseline to enhance the transformation modeling capability without additional supervision. The new module can readily replace their plain counterparts in the existing CNNs and can be easily trained end-to-end by standard backpropagation. Experiments on two large-scale re-ID datasets confirm the performance of our approach. The experiments also show that learning dense spatial transformation in deep CNNs is effective for person re-ID task and has a bright future in the intelligent video surveillance area.",
        "abstract": "As one of the most important areas of public safety and security, intelligent video surveillance is an indispensable part of the urban Internet of Things infrastructure. Person re-identification (person re-ID), which aims to track and recognize a person in a multi-camera scene, is mostly viewed as an image retrieval problem, and this task has been greatly boosted by deep convolutional neural networks (CNNs) in recent years. In practice, person re-ID usually adopts automatic detectors to obtain cropped pedestrian images, and CNNs are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. We incorporate the deformable convolution module to the traditional baseline to enhance the transformation modeling capability without additional supervision. The new module can readily replace their plain counterparts in the existing CNNs and can be easily trained end-to-end by standard backpropagation. Experiments on two large-scale re-ID datasets confirm the performance of our approach. The experiments also show that learning dense spatial transformation in deep CNNs is effective for person re-ID task and has a bright future in the intelligent video surveillance area.",
        "authors": "Hangbin Yu, Shilin Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883560",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "For air-missile borne bistatic forward-looking beam-steering synthetic aperture radar (BFLBS-SAR), the receiver platform moves along a curve track due to the existence of accelerations. Besides, transmitting and receiving antenna beams rotate around a certain rotary center during the process of data collection to extend or shorten synthetic aperture time. These characteristics imply that the echo signal suffers serious range–azimuth coupling and aliasing problems of azimuth spectrum. Worse still, the echo process has problems of double-radical sign and high-order terms, which makes the rotary beam center unable to be obtained directly, and thus, the traditional polar formation algorithm (PFA) cannot be applied directly to BFLBS-SAR. To solve these problems, this paper proposes a generalized PFA (GPFA) for air-missile borne BFLBS-SAR. The algorithm uses the equivalent bistatic linear range model first to separate platform acceleration errors. By equating the bistatic mode to the monostatic one, an equivalent rotary center is obtained, and a new azimuth Deramp function is introduced to eliminate aliasing. Then, spectrum utilization and azimuth sampling rate are improved by spectral rotation and interpolation, and the image format is changed. In the azimuth focusing phase, azimuth aliasing is avoided by means of azimuth scaling, which makes azimuth focusing possible eventually. Numerous simulation experiments verify the effectiveness of the proposed GPFA.",
        "abstract": "For air-missile borne bistatic forward-looking beam-steering synthetic aperture radar (BFLBS-SAR), the receiver platform moves along a curve track due to the existence of accelerations. Besides, transmitting and receiving antenna beams rotate around a certain rotary center during the process of data collection to extend or shorten synthetic aperture time. These characteristics imply that the echo signal suffers serious range–azimuth coupling and aliasing problems of azimuth spectrum. Worse still, the echo process has problems of double-radical sign and high-order terms, which makes the rotary beam center unable to be obtained directly, and thus, the traditional polar formation algorithm (PFA) cannot be applied directly to BFLBS-SAR. To solve these problems, this paper proposes a generalized PFA (GPFA) for air-missile borne BFLBS-SAR. The algorithm uses the equivalent bistatic linear range model first to separate platform acceleration errors. By equating the bistatic mode to the monostatic one, an equivalent rotary center is obtained, and a new azimuth Deramp function is introduced to eliminate aliasing. Then, spectrum utilization and azimuth sampling rate are improved by spectral rotation and interpolation, and the image format is changed. In the azimuth focusing phase, azimuth aliasing is avoided by means of azimuth scaling, which makes azimuth focusing possible eventually. Numerous simulation experiments verify the effectiveness of the proposed GPFA.",
        "authors": "Huan Deng, Linrang Zhang, Ping Guo, Shiyang Tang, Yachao Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884345",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "With the development and penetration of information technology, the “fourth media”-the network media is coming, and the “grievance crisis” is increasingly happening. The effective monitoring of online public opinion information becomes a problem. For each university, how to understand the sensation of teachers and students in real time in the era of informationization entering the intelligent campus has become an urgent problem for all colleges and universities. This paper first analyzes the characteristics of Weibo’s communication and the process of microblogging’s public opinion formation, the mechanism of communication, and so on. For the acquisition of this information, the reptile method was used to obtain the required data, which was beneficial to the real-time sensation in the smart campus. Then, based on the improved single-pass algorithm to analyze the new characteristics of microblog propagation information and the public opinion sent by college students, and compared the improved single-pass algorithm with the effect of single-pass algorithm, the improved algorithm had the recall rate was high, the false detection rate was low, and the running time was short. Finally, taking the lyrics of college teachers and students as the research object, unified modeling language modeling was used to analyze the public opinion monitoring platform of the smart campus, and the campus public opinion of several major events was monitored and analyzed. This provided a reference for the effective, intelligent, and real-time detection of Weibo public opinion in colleges and universities.",
        "abstract": "With the development and penetration of information technology, the “fourth media”-the network media is coming, and the “grievance crisis” is increasingly happening. The effective monitoring of online public opinion information becomes a problem. For each university, how to understand the sensation of teachers and students in real time in the era of informationization entering the intelligent campus has become an urgent problem for all colleges and universities. This paper first analyzes the characteristics of Weibo’s communication and the process of microblogging’s public opinion formation, the mechanism of communication, and so on. For the acquisition of this information, the reptile method was used to obtain the required data, which was beneficial to the real-time sensation in the smart campus. Then, based on the improved single-pass algorithm to analyze the new characteristics of microblog propagation information and the public opinion sent by college students, and compared the improved single-pass algorithm with the effect of single-pass algorithm, the improved algorithm had the recall rate was high, the false detection rate was low, and the running time was short. Finally, taking the lyrics of college teachers and students as the research object, unified modeling language modeling was used to analyze the public opinion monitoring platform of the smart campus, and the campus public opinion of several major events was monitored and analyzed. This provided a reference for the effective, intelligent, and real-time detection of Weibo public opinion in colleges and universities.",
        "authors": "Feng Nan, Shuaijie Shan, Xueyong Jia, Yina Suo, Yuying Wu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883799",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Secure patterns provide a solution for the security requirement of the software. There are large number of secure patterns, and it is quite difficult to choose an appropriate pattern. Moreover, selection of these patterns needs security knowledge; generally, developers are not specialized in the domain of security knowledge. This paper can help in the selection of secure pattern on the basis of tradeoffs of the secure pattern using text categorization. A repository of secure design patterns is used as a data set and a repository of requirements artifacts in the form of software requirements specification (SRS) are used for this paper. A text categorization scheme, which begins with preprocessing, indexing of secure patterns, ends up by querying SRS features for retrieving secure design pattern using document retrieval model. For the evaluation of the proposed model, we have used three different domains’ SRS. These three SRS documents represent three different domains, i.e., e-commerce, social media, and desktop utility program. A traditional precision and recall method along with F-measure used for evaluation of information/document retrieval model is used to evaluate the results. F-measure for 17 different design problems shows around 81% accuracy with recall up to 0.69%.",
        "abstract": "Secure patterns provide a solution for the security requirement of the software. There are large number of secure patterns, and it is quite difficult to choose an appropriate pattern. Moreover, selection of these patterns needs security knowledge; generally, developers are not specialized in the domain of security knowledge. This paper can help in the selection of secure pattern on the basis of tradeoffs of the secure pattern using text categorization. A repository of secure design patterns is used as a data set and a repository of requirements artifacts in the form of software requirements specification (SRS) are used for this paper. A text categorization scheme, which begins with preprocessing, indexing of secure patterns, ends up by querying SRS features for retrieving secure design pattern using document retrieval model. For the evaluation of the proposed model, we have used three different domains’ SRS. These three SRS documents represent three different domains, i.e., e-commerce, social media, and desktop utility program. A traditional precision and recall method along with F-measure used for evaluation of information/document retrieval model is used to evaluate the results. F-measure for 17 different design problems shows around 81% accuracy with recall up to 0.69%.",
        "authors": "Adnan Khalid, Aziz Guergachi, Ishfaq Ali, Mariam Rehman, Muhammad Asif, Muhammad Shahbaz",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883077",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Box-office fraud in China is an increasingly highlighted problem of the movie market in recent years. It misleads consumers and investors and will inevitably hurt the developing motion picture industry and shadow movie market in China. More accurate supervision and auditing should be carried out to regulate the market. Nonfinancial measurement (NFM) is an important auditing method for assessing fraud risk and helping to detect financial fraud. Computational intelligence-based techniques and publicly available nonfinancial data could be used in NFM to prioritize exceptions and improve audit efficiency. In this paper, an NFM method is proposed for fraud risk assessment of China's box office. Movie-related data were collected from different movie websites by a web crawler. An evidence theory-based fraud risk assessment framework was established for iterative aggregation of different evidence. A machine learning method, i.e., ordered logistic regression, was used to calculate the basic probability assignment for evidence theory. The risk factor was put forward as the measurement of fraud risk in the proposed method for exception prioritization. Real case studies were carried out to validate the proposed method. The results show that the proposed method is effective in assessing the fraud risk of the box office and prioritizing exceptional box offices.",
        "abstract": "Box-office fraud in China is an increasingly highlighted problem of the movie market in recent years. It misleads consumers and investors and will inevitably hurt the developing motion picture industry and shadow movie market in China. More accurate supervision and auditing should be carried out to regulate the market. Nonfinancial measurement (NFM) is an important auditing method for assessing fraud risk and helping to detect financial fraud. Computational intelligence-based techniques and publicly available nonfinancial data could be used in NFM to prioritize exceptions and improve audit efficiency. In this paper, an NFM method is proposed for fraud risk assessment of China's box office. Movie-related data were collected from different movie websites by a web crawler. An evidence theory-based fraud risk assessment framework was established for iterative aggregation of different evidence. A machine learning method, i.e., ordered logistic regression, was used to calculate the basic probability assignment for evidence theory. The risk factor was put forward as the measurement of fraud risk in the proposed method for exception prioritization. Real case studies were carried out to validate the proposed method. The results show that the proposed method is effective in assessing the fraud risk of the box office and prioritizing exceptional box offices.",
        "authors": "Hong-Qu He, Shi Qiu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883487",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The light field can be applied to computational imaging methods, such as digital refocusing, depth reconstruction, and all-in-focus imaging. In this paper, the affine Radon transform of generating the focal stack by the light field is proposed. Then, we derive the inverse formula of the affine Radon transform for reconstructing the light field from the focal stack. We analyze the ill-posedness of the reconstruction problem by the inversion formula and the incompleteness of the focal stack data. The inversion formula reveals the instability of the solution. The focal stack can be regarded as the incomplete data for light field reconstruction in the spatial domain, while it corresponds to the limited support of light field in the Fourier domain. The numerical solution of light field reconstruction is realized by approximating the inverse of the affine Radon transform. Based on the approximated inverse affine Radon transform of light field reconstruction, the high-precision light field data reconstruction method and the computational imaging method can be established via the focal stack data. The experimental results show that the high-precision light field can be reconstructed from focal stack based on the approximated inverse affine Radon transform.",
        "abstract": "The light field can be applied to computational imaging methods, such as digital refocusing, depth reconstruction, and all-in-focus imaging. In this paper, the affine Radon transform of generating the focal stack by the light field is proposed. Then, we derive the inverse formula of the affine Radon transform for reconstructing the light field from the focal stack. We analyze the ill-posedness of the reconstruction problem by the inversion formula and the incompleteness of the focal stack data. The inversion formula reveals the instability of the solution. The focal stack can be regarded as the incomplete data for light field reconstruction in the spatial domain, while it corresponds to the limited support of light field in the Fourier domain. The numerical solution of light field reconstruction is realized by approximating the inverse of the affine Radon transform. Based on the approximated inverse affine Radon transform of light field reconstruction, the high-precision light field data reconstruction method and the computational imaging method can be established via the focal stack data. The experimental results show that the high-precision light field can be reconstructed from focal stack based on the approximated inverse affine Radon transform.",
        "authors": "Chang Liu, Jun Qiu, Qing Li, Xinkai Kang, Zhong Su",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883693",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper proposes a wearable hand rehabilitation robot for assisting patients to do rehabilitation training such as the flexion and extension of fingers. This robot prototype has a modularized structure with nine degrees of freedom for the independent control of the patient's fingers. To alleviate the weight applied on the patient's hand and arm, the entire control system is placed in the patient's backpack and the cable-driven approach is employed to achieve the long-distance power transmission. Because of the repetitive training manner and the existence of external disturbances, a controller combining the iterative learning control (ILC) and the active disturbance rejection control (ADRC) has been proposed for the control of the finger's joints. The contributions of this paper lie in the robot's modularized structure design and the proposed ``ILC + ADRC”controller. Experimental results have verified the function of the proposed robot and demonstrated the satisfactory control performance achieved by the proposed controller.",
        "abstract": "This paper proposes a wearable hand rehabilitation robot for assisting patients to do rehabilitation training such as the flexion and extension of fingers. This robot prototype has a modularized structure with nine degrees of freedom for the independent control of the patient's fingers. To alleviate the weight applied on the patient's hand and arm, the entire control system is placed in the patient's backpack and the cable-driven approach is employed to achieve the long-distance power transmission. Because of the repetitive training manner and the existence of external disturbances, a controller combining the iterative learning control (ILC) and the active disturbance rejection control (ADRC) has been proposed for the control of the finger's joints. The contributions of this paper lie in the robot's modularized structure design and the proposed ``ILC + ADRC”controller. Experimental results have verified the function of the proposed robot and demonstrated the satisfactory control performance achieved by the proposed controller.",
        "authors": "Long Cheng, Miao Chen, Zhengwei Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884451",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The original brain storm optimization (BSO) method does not rationally compromise global exploration and local exploitation capability, which results in the premature convergence when solving complicated optimization problems such as the shifted or shifted rotated functions. To address this problem, this paper develops a vector grouping learning BSO (VGLBSO) method. In VGLBSO, the individuals’ creation based on a VGL scheme is first developed to improve the population diversity and compromise the global exploration and local exploitation capability. Moreover, a hybrid individuals’ update scheme is established by reasonably combing two different individuals’ update schemes, which further compromises the global exploration and local exploitation capability. Finally, the random grouping scheme, instead of K-means grouping, is allowed to shrink the computational cost and maintain the diversity of the information exchange between different individuals. Twenty-eight popular benchmark functions are used to compare VGLBSO with 12 BSO and nine swarm intelligence methods. Experimental results present that VGLBSO achieves the best overall performance, including the global search ability, convergence speed, and scalability among all the compared algorithms.",
        "abstract": "The original brain storm optimization (BSO) method does not rationally compromise global exploration and local exploitation capability, which results in the premature convergence when solving complicated optimization problems such as the shifted or shifted rotated functions. To address this problem, this paper develops a vector grouping learning BSO (VGLBSO) method. In VGLBSO, the individuals’ creation based on a VGL scheme is first developed to improve the population diversity and compromise the global exploration and local exploitation capability. Moreover, a hybrid individuals’ update scheme is established by reasonably combing two different individuals’ update schemes, which further compromises the global exploration and local exploitation capability. Finally, the random grouping scheme, instead of K-means grouping, is allowed to shrink the computational cost and maintain the diversity of the information exchange between different individuals. Twenty-eight popular benchmark functions are used to compare VGLBSO with 12 BSO and nine swarm intelligence methods. Experimental results present that VGLBSO achieves the best overall performance, including the global search ability, convergence speed, and scalability among all the compared algorithms.",
        "authors": "Chunquan Li, Dujuan Hu, Feng Yang, Jinghui Fan, Zhenshou Song, Zu Luo",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884862",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A constant frequency double-integral sliding mode controller (DISMC) is proposed for the regulation of a four-quadrant continuous gain dc–dc converter based on quasi-Z-source (ZS) topology. No controller has yet been proposed for the control of this particular converter in the literature so far. Four-quadrant converters based on hybrid ZS networks or coupled inductors have been proposed in the past, but this converter is unique in the sense that it uses minimum number of passive devices and active switches to provide bidirectional current and bipolar output voltage. This makes the converter preferable to use in renewable energy or motor drive applications that require a four-quadrant operation. The proposed controller eliminates steady-state errors and provides robust control in the face of large input voltage or load variations. It enables the converter to provide a fast dynamical response over a wide operating range. Simulations of the proposed controller have been performed in MATLAB/Simulink, where the results of DISMC have also been compared with those of single integral sliding mode control.",
        "abstract": "A constant frequency double-integral sliding mode controller (DISMC) is proposed for the regulation of a four-quadrant continuous gain dc–dc converter based on quasi-Z-source (ZS) topology. No controller has yet been proposed for the control of this particular converter in the literature so far. Four-quadrant converters based on hybrid ZS networks or coupled inductors have been proposed in the past, but this converter is unique in the sense that it uses minimum number of passive devices and active switches to provide bidirectional current and bipolar output voltage. This makes the converter preferable to use in renewable energy or motor drive applications that require a four-quadrant operation. The proposed controller eliminates steady-state errors and provides robust control in the face of large input voltage or load variations. It enables the converter to provide a fast dynamical response over a wide operating range. Simulations of the proposed controller have been performed in MATLAB/Simulink, where the results of DISMC have also been compared with those of single integral sliding mode control.",
        "authors": "Iftikhar Ahmad, Muhammad Ahmad Qureshi, Muhammad Faizan Munir",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884092",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "At present, the sound source localization methods based on microphone arrays can be roughly classified into three categories: the controllable beamforming technology based on a maximum output power, the high-resolution spectrogram estimation technique, and the sound source localization technique based on time difference of sound. However, an existing localization technology in unstructured indoor environment lacks of localization accuracy and adaptability. In some practical situations, the location of sound source is limited to predefined areas. In this paper, we propose a research method of source region location system based on convolutional neural networks (CNNs). Based on the characteristics of weighted values of CNN, we realize the regional of indoor single sound sources transforming the sound source signals into grammar diagrams and then inputting them into the CNN. The whole process is based on the characteristics of weighted values of CNN. Finally, this paper completes the training and testing for CNN by using the Tensorflow framework. Simulation experiments on the test sets show the effectiveness of the proposed method.",
        "abstract": "At present, the sound source localization methods based on microphone arrays can be roughly classified into three categories: the controllable beamforming technology based on a maximum output power, the high-resolution spectrogram estimation technique, and the sound source localization technique based on time difference of sound. However, an existing localization technology in unstructured indoor environment lacks of localization accuracy and adaptability. In some practical situations, the location of sound source is limited to predefined areas. In this paper, we propose a research method of source region location system based on convolutional neural networks (CNNs). Based on the characteristics of weighted values of CNN, we realize the regional of indoor single sound sources transforming the sound source signals into grammar diagrams and then inputting them into the CNN. The whole process is based on the characteristics of weighted values of CNN. Finally, this paper completes the training and testing for CNN by using the Tensorflow framework. Simulation experiments on the test sets show the effectiveness of the proposed method.",
        "authors": "Hao Sun, Jing Xu, Shuopeng Wang, Xiaomeng Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883341",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Kinetic parameter identification in the dynamic metabolic model of Escherichia coli (E. coli) has become important and is needed to obtain appropriate metabolite and enzyme data that are valid under in vivo conditions. The dynamic metabolic model under study represents five metabolic pathways with more than 170 kinetic parameters at steady state with a 0.1 dilution rate. In this paper, identification is declared in two steps. The first step is to identify which kinetic parameters have a higher impact on the model response using local sensitivity analysis results upon increasing each kinetic parameter up to 2.0 by steps of 0.5, while the second step uses highly sensitive kinetic results to be identified and minimized the model simulation metabolite errors using real experimental data by adopting. However, this paper focuses on adopting segment particle swarm optimization (PSO) and PSO algorithms for large-scale kinetic parameters identification. Among the 170 kinetic parameters investigated, seven kinetic parameters were found to be the most effective kinetic parameters in the model response after finalizing the sensitivity. The seven sensitive kinetic parameters were used in both the algorithms to minimize the model response errors. The validation results proved the effectiveness of both the proposed methods, which identified the kinetics and minimized the model response errors perfectly.",
        "abstract": "Kinetic parameter identification in the dynamic metabolic model of Escherichia coli (E. coli) has become important and is needed to obtain appropriate metabolite and enzyme data that are valid under in vivo conditions. The dynamic metabolic model under study represents five metabolic pathways with more than 170 kinetic parameters at steady state with a 0.1 dilution rate. In this paper, identification is declared in two steps. The first step is to identify which kinetic parameters have a higher impact on the model response using local sensitivity analysis results upon increasing each kinetic parameter up to 2.0 by steps of 0.5, while the second step uses highly sensitive kinetic results to be identified and minimized the model simulation metabolite errors using real experimental data by adopting. However, this paper focuses on adopting segment particle swarm optimization (PSO) and PSO algorithms for large-scale kinetic parameters identification. Among the 170 kinetic parameters investigated, seven kinetic parameters were found to be the most effective kinetic parameters in the model response after finalizing the sensitivity. The seven sensitive kinetic parameters were used in both the algorithms to minimize the model response errors. The validation results proved the effectiveness of both the proposed methods, which identified the kinetics and minimized the model response errors perfectly.",
        "authors": "Aqeel S. Jaber, Mohammed Adam Kunna Azrag, Tuty Asmawaty Abdul Kadir",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885118",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In recent times, the trending modification of control chart is a result of its effective application in various aspects of life. In this paper, we propose a new charting scheme called mixed exponentially weighted moving average (EWMA) dual-cumulative sum (CUSUM) to monitor the location parameter. The driving goal, which inspired this scheme, was the enhancement in the sensitivity of the control scheme used by analyzers in the petro-chemical industry which identify different quantities of components present in different catalysts. The proposed scheme is the combination of a dual-CUSUM chart and a time-varying EWMA chart. We explore the scheme with robust estimators by evaluating their thresholds in the presence of disturbances and contaminations. We do this by introducing both location and variance contaminants into the process environment. The average and standard deviation of the run lengths were used as the benchmark for performance evaluation through the various estimators. We applied this scheme on real life datasets from refinery laboratory statistics which is the case study.",
        "abstract": "In recent times, the trending modification of control chart is a result of its effective application in various aspects of life. In this paper, we propose a new charting scheme called mixed exponentially weighted moving average (EWMA) dual-cumulative sum (CUSUM) to monitor the location parameter. The driving goal, which inspired this scheme, was the enhancement in the sensitivity of the control scheme used by analyzers in the petro-chemical industry which identify different quantities of components present in different catalysts. The proposed scheme is the combination of a dual-CUSUM chart and a time-varying EWMA chart. We explore the scheme with robust estimators by evaluating their thresholds in the presence of disturbances and contaminations. We do this by introducing both location and variance contaminants into the process environment. The average and standard deviation of the run lengths were used as the benchmark for performance evaluation through the various estimators. We applied this scheme on real life datasets from refinery laboratory statistics which is the case study.",
        "authors": "Ishaq A. Raji, Khalid Al-Ghamdi, Muhammad Riaz, Nasir Abbas",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885598",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, we propose a method that use the Gaussian mixture model (GMM) as a plug-and-play prior for image restoration in sensor network. The “plug-and-play\" concept as an image prior is extended to image restoration and just mentioned in recent years. Particularly, the plug-and-play alternating direction method of the multiplier is very fit for the GMM framework to regularize the sub-problems. In order to avoid error results caused by general minimum of mean square error criterion, we propose two spatial constraints: one applies the K-nearest-neighbor method based on an Euclidean distance to measure the similarity of image patches in clustering step; the other adopts the Gaussian weight based on the Mahalanobis distance to update the mean vector and covariance matrix. Finally, we compare our method with several recent state-of-the-art methods, and the results show that our proposed method has good performance in preserving details and eliminating the staircase effect.",
        "abstract": "In this paper, we propose a method that use the Gaussian mixture model (GMM) as a plug-and-play prior for image restoration in sensor network. The “plug-and-play\" concept as an image prior is extended to image restoration and just mentioned in recent years. Particularly, the plug-and-play alternating direction method of the multiplier is very fit for the GMM framework to regularize the sub-problems. In order to avoid error results caused by general minimum of mean square error criterion, we propose two spatial constraints: one applies the K-nearest-neighbor method based on an Euclidean distance to measure the similarity of image patches in clustering step; the other adopts the Gaussian weight based on the Mahalanobis distance to update the mean vector and covariance matrix. Finally, we compare our method with several recent state-of-the-art methods, and the results show that our proposed method has good performance in preserving details and eliminating the staircase effect.",
        "authors": "Liang Feng, Mingzhu Shi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884795",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Intuitive and robust multimodal robot control is the key toward human–robot collaboration (HRC) for manufacturing systems. Multimodal robot control methods were introduced in previous studies. The methods allow human operators to control robot intuitively without programming brand-specific code. However, most of the multimodal robot control methods are unreliable because the feature representations are not shared across multiple modalities. To target this problem, a deep learning-based multimodal fusion architecture is proposed in this paper for robust multimodal HRC manufacturing systems. The proposed architecture consists of three modalities: speech command, hand motion, and body motion. Three unimodal models are first trained to extract features, which are further fused for representation sharing. Experiments show that the proposed multimodal fusion model outperforms the three unimodal models. This paper indicates a great potential to apply the proposed multimodal fusion architecture to robust HRC manufacturing systems.",
        "abstract": "Intuitive and robust multimodal robot control is the key toward human–robot collaboration (HRC) for manufacturing systems. Multimodal robot control methods were introduced in previous studies. The methods allow human operators to control robot intuitively without programming brand-specific code. However, most of the multimodal robot control methods are unreliable because the feature representations are not shared across multiple modalities. To target this problem, a deep learning-based multimodal fusion architecture is proposed in this paper for robust multimodal HRC manufacturing systems. The proposed architecture consists of three modalities: speech command, hand motion, and body motion. Three unimodal models are first trained to extract features, which are further fused for representation sharing. Experiments show that the proposed multimodal fusion model outperforms the three unimodal models. This paper indicates a great potential to apply the proposed multimodal fusion architecture to robust HRC manufacturing systems.",
        "authors": "Hongyi Liu, Lihui Wang, Tianyu Zhou, Tongtong Fang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884793",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "U-type assembly lines have become a mainstream mode in manufacturing because of the higher flexibility and productivity compared with straight lines. Since the balancing problem of a large-scale U-type assembly line is known to be NP-hard, effective mathematical model and evolutionary algorithm are needed to solve this problem. This paper reviews the research status of the related literature in recent years and presents a hybrid evolutionary algorithm, namely, modified ant colony optimization inspired by the process of simulated annealing, to reduce the possibility of being trapped in a local optimum for the balancing problem of stochastic large-scale U-type assembly line. A modified mathematical model for this balancing problem considering stochastic properties is formulated. Furthermore, comparisons with genetic algorithm and imperialist competitive algorithm are conducted to evaluate this proposed method. The results indicate that this proposed algorithm outperforms prior methods in this balancing problem.",
        "abstract": "U-type assembly lines have become a mainstream mode in manufacturing because of the higher flexibility and productivity compared with straight lines. Since the balancing problem of a large-scale U-type assembly line is known to be NP-hard, effective mathematical model and evolutionary algorithm are needed to solve this problem. This paper reviews the research status of the related literature in recent years and presents a hybrid evolutionary algorithm, namely, modified ant colony optimization inspired by the process of simulated annealing, to reduce the possibility of being trapped in a local optimum for the balancing problem of stochastic large-scale U-type assembly line. A modified mathematical model for this balancing problem considering stochastic properties is formulated. Furthermore, comparisons with genetic algorithm and imperialist competitive algorithm are conducted to evaluate this proposed method. The results indicate that this proposed algorithm outperforms prior methods in this balancing problem.",
        "authors": "Chaoyong Zhang, Danqi Wang, Guangdong Tian, Honghao Zhang, Xu Liu, Yong Peng",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885030",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A restricted Boltzmann machine (RBM) is a generative model that can capture the probability distribution of data relevant to the problem domain and is usually deployed as the fundamental building block to form more complex deep architectures, such as deep belief network, deep Boltzmann machine, and deep stacked auto-encoder. In addition, the RBM itself can be used as a feature extractor to learn features from raw data. In addition, an RBM is a special type of energy-based model. This paper proposes a modified loss function as an example of an energy-based model defined by restricting the free energy value of the training data. This restriction punishes very low free energy value to reduce the model complexity, which is helpful to the training procedure of the RBM. We validate our method using the MNIST and MNIST-ROTATION datasets. Experiments reveal that the modified loss function behaves better in learning discriminative features as well as in providing better parameters when used to initialize deep feed-forward neural networks (DNN). The convergence speed of the DNN can improve by 44% on both datasets.",
        "abstract": "A restricted Boltzmann machine (RBM) is a generative model that can capture the probability distribution of data relevant to the problem domain and is usually deployed as the fundamental building block to form more complex deep architectures, such as deep belief network, deep Boltzmann machine, and deep stacked auto-encoder. In addition, the RBM itself can be used as a feature extractor to learn features from raw data. In addition, an RBM is a special type of energy-based model. This paper proposes a modified loss function as an example of an energy-based model defined by restricting the free energy value of the training data. This restriction punishes very low free energy value to reduce the model complexity, which is helpful to the training procedure of the RBM. We validate our method using the MNIST and MNIST-ROTATION datasets. Experiments reveal that the modified loss function behaves better in learning discriminative features as well as in providing better parameters when used to initialize deep feed-forward neural networks (DNN). The convergence speed of the DNN can improve by 44% on both datasets.",
        "authors": "Bin Wang, Changjun Zhou, Song Guo, Xuedong Zheng",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885071",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Technology-based learning systems enable enhanced student learning in higher-education institutions. This paper evaluates the factors affecting behavioral intention of students toward using e-learning systems in universities to augment classroom learning. Based on the technology acceptance model, this paper proposes six external factors that influence the behavioral intention of students toward use of e-learning. A quantitative approach involving structural equation modeling is adopted, and research data collected from 437 undergraduate students enrolled in three academic programs is used for analysis. Results indicate that subjective norm, perception of external control, system accessibility, enjoyment, and result demonstrability have a significant positive influence on perceived usefulness and on perceived ease of use of the e-learning system. This paper also examines the relevance of some previously used external variables, e.g., self-efficacy, experience, and computer anxiety, for present-world students who have been brought up as digital learners and have higher levels of computer literacy and experience.",
        "abstract": "Technology-based learning systems enable enhanced student learning in higher-education institutions. This paper evaluates the factors affecting behavioral intention of students toward using e-learning systems in universities to augment classroom learning. Based on the technology acceptance model, this paper proposes six external factors that influence the behavioral intention of students toward use of e-learning. A quantitative approach involving structural equation modeling is adopted, and research data collected from 437 undergraduate students enrolled in three academic programs is used for analysis. Results indicate that subjective norm, perception of external control, system accessibility, enjoyment, and result demonstrability have a significant positive influence on perceived usefulness and on perceived ease of use of the e-learning system. This paper also examines the relevance of some previously used external variables, e.g., self-efficacy, experience, and computer anxiety, for present-world students who have been brought up as digital learners and have higher levels of computer literacy and experience.",
        "authors": "Aamer Hanif, Faheem Qaisar Jamal, Muhammad Imran",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2881384",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Software refactoring is popular and thus most mainstream IDEs, e.g., Eclipse, provide a top level menu, especially for refactoring activities. The refactoring menu is designed to facilitate refactorings, and it has become one of the most commonly used menus. However, to support a large number of refactoring types, the refactoring menu contains a long list of menu items. As a result, it is tedious to select the intended menu item from the lengthy menu. To facilitate the menu selection, in this paper, we propose an approach to dynamic ranking of refactoring menu items for IDE. We put the most likely refactoring menu item on the top of the refactoring menu according to developers' source code selection and code smells associated with the selected source code. The ranking is dynamic because it changes frequently according to the context. First, we collect the refactoring history of the open source applications and detect the code smells. Based on the refactoring history, we design questionnaires and analyze the responses from developers to discover the source code selection patterns for different refactoring types. Subsequently, we analyze the relationship between code smells associated with the refactoring software entities and the corresponding refactoring types. Finally, based on the preceding analysis, we calculate the likelihood of different refactoring types to be applied when a specific part of source code is selected, and rank the menu items according to the resulting likelihood. We conduct a case study to evaluate the proposed approach. Evaluation results suggest that the proposed approach is accurate, and in most cases (95.69%), it can put the intended refactoring menu item on the top of the menu.",
        "abstract": "Software refactoring is popular and thus most mainstream IDEs, e.g., Eclipse, provide a top level menu, especially for refactoring activities. The refactoring menu is designed to facilitate refactorings, and it has become one of the most commonly used menus. However, to support a large number of refactoring types, the refactoring menu contains a long list of menu items. As a result, it is tedious to select the intended menu item from the lengthy menu. To facilitate the menu selection, in this paper, we propose an approach to dynamic ranking of refactoring menu items for IDE. We put the most likely refactoring menu item on the top of the refactoring menu according to developers' source code selection and code smells associated with the selected source code. The ranking is dynamic because it changes frequently according to the context. First, we collect the refactoring history of the open source applications and detect the code smells. Based on the refactoring history, we design questionnaires and analyze the responses from developers to discover the source code selection patterns for different refactoring types. Subsequently, we analyze the relationship between code smells associated with the refactoring software entities and the corresponding refactoring types. Finally, based on the preceding analysis, we calculate the likelihood of different refactoring types to be applied when a specific part of source code is selected, and rank the menu items according to the resulting likelihood. We conduct a case study to evaluate the proposed approach. Evaluation results suggest that the proposed approach is accurate, and in most cases (95.69%), it can put the intended refactoring menu item on the top of the menu.",
        "authors": "Bridget Nyirongo, Hui Liu, Thida Oo",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883769",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The lateral tire force is of crucial importance for the vehicle lateral dynamics modeling. Linear and nonlinear models have been proposed to describe the characteristic of the force. Due to its simplicity, the linear model is widely used in a vehicle controller and an observer design at present. However, this model has a limited application area since it is accurate only when tires operate in the linear region and fails to reflect the nonlinear behavior of the tire force. Contrary to the linear model, nonlinear models can represent the behavior entirely with complex nonlinear functions, which are difficult to be used for the controller or the observer design. Aiming at these problems, a piecewise affine (PWA) model for the lateral tire force is proposed in this paper. According to the relationship between the tire lateral force and the tire slip angle, the lateral tire force is approximated by a five-segment PWA model with different slopes. Model parameters are calculated by the tire analysis tool of the high-precision real-time simulation software of vehicle dynamics called veDYNA. Combining the PWA model with the lateral dynamic model, a novel two-degree-of-freedom model is obtained to describe the vehicle lateral dynamics. Based on veDYNA, simulation results with different maneuvers show that the proposed model is valid and has a higher precision.",
        "abstract": "The lateral tire force is of crucial importance for the vehicle lateral dynamics modeling. Linear and nonlinear models have been proposed to describe the characteristic of the force. Due to its simplicity, the linear model is widely used in a vehicle controller and an observer design at present. However, this model has a limited application area since it is accurate only when tires operate in the linear region and fails to reflect the nonlinear behavior of the tire force. Contrary to the linear model, nonlinear models can represent the behavior entirely with complex nonlinear functions, which are difficult to be used for the controller or the observer design. Aiming at these problems, a piecewise affine (PWA) model for the lateral tire force is proposed in this paper. According to the relationship between the tire lateral force and the tire slip angle, the lateral tire force is approximated by a five-segment PWA model with different slopes. Model parameters are calculated by the tire analysis tool of the high-precision real-time simulation software of vehicle dynamics called veDYNA. Combining the PWA model with the lateral dynamic model, a novel two-degree-of-freedom model is obtained to describe the vehicle lateral dynamics. Based on veDYNA, simulation results with different maneuvers show that the proposed model is valid and has a higher precision.",
        "authors": "Qian Zhang, Zhiyuan Liu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885050",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper addresses the constrained multi-objective optimization problem of sparse conformal arrays designing. The objective of array synthesis is to find an optimal element arrangement on a conformal surface and its associated excitation strategy, which generate the main radiation beam along a pre-selected spatial direction with maximum gain and, simultaneously, suppress sidelobe levels elsewhere. A hybrid algorithm particle swarm optimization (PSO)-second-order cone programming (SOCP), comprising of PSO and SOCP, each for a dedicated purpose, is proposed to fulfill this task in this paper. More specifically, the PSO algorithm is introduced to optimize sparse conformal array element positions, whereas the SOCP is applied to seek optimal excitation coefficients for each array layout obtained. After extensive simulation with the examples of sparse circular array and sparse conical arrays, we can find that our proposed method can synthesize better radiation patterns with regard to peak sidelobe levels, compared with those obtained through other traditional algorithms.",
        "abstract": "This paper addresses the constrained multi-objective optimization problem of sparse conformal arrays designing. The objective of array synthesis is to find an optimal element arrangement on a conformal surface and its associated excitation strategy, which generate the main radiation beam along a pre-selected spatial direction with maximum gain and, simultaneously, suppress sidelobe levels elsewhere. A hybrid algorithm particle swarm optimization (PSO)-second-order cone programming (SOCP), comprising of PSO and SOCP, each for a dedicated purpose, is proposed to fulfill this task in this paper. More specifically, the PSO algorithm is introduced to optimize sparse conformal array element positions, whereas the SOCP is applied to seek optimal excitation coefficients for each array layout obtained. After extensive simulation with the examples of sparse circular array and sparse conical arrays, we can find that our proposed method can synthesize better radiation patterns with regard to peak sidelobe levels, compared with those obtained through other traditional algorithms.",
        "authors": "Hailin Li, Jianjiang Zhou, Jing Tan, Yachao Jiang, Yuan Ding",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883042",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The process structures of manufacturing industry are efficiently modeled using linear profiles. Classical and Bayesian set-ups are two well-appreciated schemes for designing control charts for the monitoring of process structures. Mostly in profiles monitoring the independent variables along with the process parameters are assumed fixed. There are manufacturing processes where these conditions may not hold. The advancement in technology and day-to-day changes in process structures caused the parametric uncertainty along with variability in explanatory variables. This paper considered the case of random X and assumes different conjugate and non-conjugate priors to handle parametric uncertainty using double exponentially weighted moving average (DEWMA) control charts. Three univariate DEWMA charts are designed for the monitoring of V-intercepts, slopes, and error variances. The average run length criterion has been used to evaluate the proposed and competing charts. The wide spread relative study identifies that the proposed Bayesian DEWMA control charts are better than the competing charts based on early detection of out-of-control profiles, particularly for smaller value shifts. The Bayesian DEWMA charts using conjugate priors are the quickest in all as they take less sample points to show out-of-control profile. A case study has been considered to further justify the superiority of Bayesian DEWMA charts over competing charts.",
        "abstract": "The process structures of manufacturing industry are efficiently modeled using linear profiles. Classical and Bayesian set-ups are two well-appreciated schemes for designing control charts for the monitoring of process structures. Mostly in profiles monitoring the independent variables along with the process parameters are assumed fixed. There are manufacturing processes where these conditions may not hold. The advancement in technology and day-to-day changes in process structures caused the parametric uncertainty along with variability in explanatory variables. This paper considered the case of random X and assumes different conjugate and non-conjugate priors to handle parametric uncertainty using double exponentially weighted moving average (DEWMA) control charts. Three univariate DEWMA charts are designed for the monitoring of V-intercepts, slopes, and error variances. The average run length criterion has been used to evaluate the proposed and competing charts. The wide spread relative study identifies that the proposed Bayesian DEWMA control charts are better than the competing charts based on early detection of out-of-control profiles, particularly for smaller value shifts. The Bayesian DEWMA charts using conjugate priors are the quickest in all as they take less sample points to show out-of-control profile. A case study has been considered to further justify the superiority of Bayesian DEWMA charts over competing charts.",
        "authors": "Abdel-Salam Gomaa, Muhammad Riaz, Saddam Akber Abbasi, Tahir Abbas",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885014",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Predicting popularity of social media videos before they are published is a challenging task, mainly due to the complexity of content distribution network as well as the number of factors that play a part in this process. As solving this task provides tremendous help for media content creators, many successful methods were proposed to solve this problem with machine learning. In this work, we change the viewpoint and postulate that it is not only the predicted popularity that matters but also, maybe even more importantly, understanding of how individual parts influence the final popularity score. To that end, we propose to combine the Grad-CAM visualization method that allows to visualize spatial relevance to popularity with a soft self-attention mechanism to weight the relative importance of frames in time domain. Our preliminary results show that this approach allows for more intuitive interpretation of the content impact on video popularity while achieving competitive results in terms of prediction accuracy.",
        "abstract": "Predicting popularity of social media videos before they are published is a challenging task, mainly due to the complexity of content distribution network as well as the number of factors that play a part in this process. As solving this task provides tremendous help for media content creators, many successful methods were proposed to solve this problem with machine learning. In this work, we change the viewpoint and postulate that it is not only the predicted popularity that matters but also, maybe even more importantly, understanding of how individual parts influence the final popularity score. To that end, we propose to combine the Grad-CAM visualization method that allows to visualize spatial relevance to popularity with a soft self-attention mechanism to weight the relative importance of frames in time domain. Our preliminary results show that this approach allows for more intuitive interpretation of the content impact on video popularity while achieving competitive results in terms of prediction accuracy.",
        "authors": "Adam Bielski, Tomasz Trzcinski",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884831",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "When the on-time is constant during a line cycle, the input power factor (PF) of critical conduction mode buck-buck/boost power factor correction converter is low. A variable on-time control (VOTC) scheme is put forward in this paper to realize high PF. By utilizing the input and output voltage to modulate the on-time of buck and buck/boost switch, we can realize high-input PF with low-total harmonic distortion within the universal input voltage range. The operating principles and the comparative analysis of the converter with conventional constant on-time control (COTC) and VOTC are analyzed. The proposed control scheme achieves high PF and efficiency as compared with the COTC. A 100-W prototype was built up to verify the effectiveness of the proposed control strategy.",
        "abstract": "When the on-time is constant during a line cycle, the input power factor (PF) of critical conduction mode buck-buck/boost power factor correction converter is low. A variable on-time control (VOTC) scheme is put forward in this paper to realize high PF. By utilizing the input and output voltage to modulate the on-time of buck and buck/boost switch, we can realize high-input PF with low-total harmonic distortion within the universal input voltage range. The operating principles and the comparative analysis of the converter with conventional constant on-time control (COTC) and VOTC are analyzed. The proposed control scheme achieves high PF and efficiency as compared with the COTC. A 100-W prototype was built up to verify the effectiveness of the proposed control strategy.",
        "authors": "Abdul Hakeem Memon, Amir Mahmood Soomro, Anwar Ali Sahito, Mazhar Hussain Baloach, Zubair Ahmed Memon",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2879804",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Computers and other smart gadgets have become of a paramount importance in today’s transactions. Connected to the Internet, those devices offer the possibility to benefit from a myriad of electronic services, including social networking, banking, trade marketing, education and so on. Such activities are producing huge volume of information transiting with high velocity each day. Parallel to that, we have witnessed an epidemic increase in the number and the sophistication of cyberattacks, as they became more persistent and highly structured. In this context, modern intrusion detection systems are to be modeled so as to issue high detection rates in a tiny period of time in order to mitigate the risks. This paper is built on recurrent neural network with multilayered echo-state machine (ML-ESM) to model an intrusion detection. We assess our model on three publicly available data sets, namely, the DARPA KDD’99, NSL-KDD a reformed version of the latter, and UNSW NB 15. Performance metrics for both binary classification and multilabel classification are calculated and compared with those of some existing machine learning techniques and the recent state-of-the-art intrusion detection systems. Results indicate that the ML-ESM wins the challenge in both achieving a higher accuracy and considerably optimizing the processing time.",
        "abstract": "Computers and other smart gadgets have become of a paramount importance in today’s transactions. Connected to the Internet, those devices offer the possibility to benefit from a myriad of electronic services, including social networking, banking, trade marketing, education and so on. Such activities are producing huge volume of information transiting with high velocity each day. Parallel to that, we have witnessed an epidemic increase in the number and the sophistication of cyberattacks, as they became more persistent and highly structured. In this context, modern intrusion detection systems are to be modeled so as to issue high detection rates in a tiny period of time in order to mitigate the risks. This paper is built on recurrent neural network with multilayered echo-state machine (ML-ESM) to model an intrusion detection. We assess our model on three publicly available data sets, namely, the DARPA KDD’99, NSL-KDD a reformed version of the latter, and UNSW NB 15. Performance metrics for both binary classification and multilabel classification are calculated and compared with those of some existing machine learning techniques and the recent state-of-the-art intrusion detection systems. Results indicate that the ML-ESM wins the challenge in both achieving a higher accuracy and considerably optimizing the processing time.",
        "authors": "Mostafa Ezziyyani, Taha Ait Tchakoucht",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2867345",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "False data injection (FDI) attack is the most common data integrity attack, and it is also one of the most serious threats in industrial control systems (ICSs). Although many detection approaches are developed with burgeoning research interests, the technical capability of existing detection methods is still insufficient because the stealth FDI attacks have been proven to bypass bad data detector. In this paper, a novel data analytical algorithm is proposed to identify the stealth FDI attacks in ICSs according to the correlation analysis. First, we evaluate the correlation between measurements and control variables based on an improved grey relational analysis. Then, SVM is used to classify the FDI attack according to the values of correlation. Through a reliable semi-physical simulation testbed whose virtual plant corresponds to a 330 MW boiler-turbine unit, two FDI attacks that can bypass the detection system are studied. A dataset, which contains the normal data and attack data, is created from the testbed to verify the effectiveness of the proposed algorithm. In addition, the performance of the proposed algorithm is also studied based on the new gas pipeline dataset that is collected by the distributed analytics and security institute in Mississippi State University. Such a novel algorithm, which has better accuracy and reliability, is compared with the state of the art based on the data analysis.",
        "abstract": "False data injection (FDI) attack is the most common data integrity attack, and it is also one of the most serious threats in industrial control systems (ICSs). Although many detection approaches are developed with burgeoning research interests, the technical capability of existing detection methods is still insufficient because the stealth FDI attacks have been proven to bypass bad data detector. In this paper, a novel data analytical algorithm is proposed to identify the stealth FDI attacks in ICSs according to the correlation analysis. First, we evaluate the correlation between measurements and control variables based on an improved grey relational analysis. Then, SVM is used to classify the FDI attack according to the values of correlation. Through a reliable semi-physical simulation testbed whose virtual plant corresponds to a 330 MW boiler-turbine unit, two FDI attacks that can bypass the detection system are studied. A dataset, which contains the normal data and attack data, is created from the testbed to verify the effectiveness of the proposed algorithm. In addition, the performance of the proposed algorithm is also studied based on the new gas pipeline dataset that is collected by the distributed analytics and security institute in Mississippi State University. Such a novel algorithm, which has better accuracy and reliability, is compared with the state of the art based on the data analysis.",
        "authors": "Linbo Xie, Yunfei Wang, Zhengdao Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884504",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper addresses robust model predictive control for networked polytopic uncertain systems with packet loss. An infinite horizon performance cost in this paper only considers the sequence of the successful data transmissions. Two techniques are presented: one parameterizing the infinite horizon control moves into a single state feedback law and the other into a free control move followed by the single-state feedback law. Like the traditional approach, the performance cost is utilized as the Lyapunov function to prove closed-loop stability. Two simulation examples are given to illustrate the effectiveness of the proposed techniques.",
        "abstract": "This paper addresses robust model predictive control for networked polytopic uncertain systems with packet loss. An infinite horizon performance cost in this paper only considers the sequence of the successful data transmissions. Two techniques are presented: one parameterizing the infinite horizon control moves into a single state feedback law and the other into a free control move followed by the single-state feedback law. Like the traditional approach, the performance cost is utilized as the Lyapunov function to prove closed-loop stability. Two simulation examples are given to illustrate the effectiveness of the proposed techniques.",
        "authors": "Chai Yi, Fan Qian, Wei Shanbi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883366",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Rail area detection is essential in active obstacle perception system of the train. This paper presents an efficient rail area detection method based on the convolutional neural network (CNN). The proposed method is divided into two main parts: extraction of the rail area and further optimization. First, a CNN architecture is established to achieve accurate rail area detection, enabling the pixel-level classification of the rail area. It is notable that the main improvement of our architecture is dilated cascade connection and cascade sampling. Second, an improved polygon fitting method is applied to optimize the contour of the extracted rail area and, thus, obtains a more elegant outline of the rail region. As shown by the experimental results, the excellent accuracy is obtained by using our method, i.e., 98.46% mean intersection-over-union and 99.15% mean pixel accuracy on the BH-rail-dataset, and verified the applicability of our detection method in a large-scale traffic scene video frames of Beijing metro Yanfang line and Shanghai metro line 6.",
        "abstract": "Rail area detection is essential in active obstacle perception system of the train. This paper presents an efficient rail area detection method based on the convolutional neural network (CNN). The proposed method is divided into two main parts: extraction of the rail area and further optimization. First, a CNN architecture is established to achieve accurate rail area detection, enabling the pixel-level classification of the rail area. It is notable that the main improvement of our architecture is dilated cascade connection and cascade sampling. Second, an improved polygon fitting method is applied to optimize the contour of the extracted rail area and, thus, obtains a more elegant outline of the rail region. As shown by the experimental results, the excellent accuracy is obtained by using our method, i.e., 98.46% mean intersection-over-union and 99.15% mean pixel accuracy on the BH-rail-dataset, and verified the applicability of our detection method in a large-scale traffic scene video frames of Beijing metro Yanfang line and Shanghai metro line 6.",
        "authors": "Guizhen Yu, Mingxing Li, Xinkai Wu, Zhangyu Wang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883704",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Classifying traffic signs is an indispensable task for autonomous driving systems. Depending on the country, traffic signs possess a wide variability in their visual appearance making it harder for classification systems to succeed. Either the classifier should be fine-tuned or a bigger collection of images should be used. In this paper, we introduce a real-world European dataset for traffic sign classification. The dataset is composed of traffic sings from six European countries: Belgium, Croatia, France, Germany, The Netherlands, and Sweden. It gathers publically available datasets and complements French traffic signs with images acquired in Belfort with the equipped university autonomous vehicle. It is composed of more than 80000 images divided in 164 classes that at the same time belong to four main categories following the Vienna Convention of Road Signs. We analyzed the intra variability of classes and compared the classification performance of five convolutional neural network architectures.",
        "abstract": "Classifying traffic signs is an indispensable task for autonomous driving systems. Depending on the country, traffic signs possess a wide variability in their visual appearance making it harder for classification systems to succeed. Either the classifier should be fine-tuned or a bigger collection of images should be used. In this paper, we introduce a real-world European dataset for traffic sign classification. The dataset is composed of traffic sings from six European countries: Belgium, Croatia, France, Germany, The Netherlands, and Sweden. It gathers publically available datasets and complements French traffic signs with images acquired in Belfort with the equipped university autonomous vehicle. It is composed of more than 80000 images divided in 164 classes that at the same time belong to four main categories following the Vienna Convention of Road Signs. We analyzed the intra variability of classes and compared the classification performance of five convolutional neural network architectures.",
        "authors": "Citlalli Gámez Serna, Yassine Ruichek",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884826",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In a recent decade, deep neural networks have been applied for many research areas after achieving dramatic improvements of accuracy in solving complex problems in vision and computational linguistics area. However, some problems, such as environmental modeling, are still limited to benefit from the deep networks because of its difficulty in collecting sufficient data of learning process. In this paper, aside from the accuracy issue, we raise another property-stability-of the deep networks useful for even such data-limited problems, especially in time-series modeling. Recurrent neural networks with memory cell structures, a deep network, can be deemed as a more robust network structure for long-term forecasting under coarse data observation and associated uncertainties, including missing values and sampling/measurement errors. The stability in forecasting is induced from balancing impact of inputs over all time steps in the networks. To analyze this property in various problem conditions, we adapt the recurrent networks with memory structure to environmental time-series problems, such as forecasting water pollution, air pollution, and ozone alarm. In the results, the recurrent networks with memory showed better performance of forecasting in non-stationary environment and long-term time lags.",
        "abstract": "In a recent decade, deep neural networks have been applied for many research areas after achieving dramatic improvements of accuracy in solving complex problems in vision and computational linguistics area. However, some problems, such as environmental modeling, are still limited to benefit from the deep networks because of its difficulty in collecting sufficient data of learning process. In this paper, aside from the accuracy issue, we raise another property-stability-of the deep networks useful for even such data-limited problems, especially in time-series modeling. Recurrent neural networks with memory cell structures, a deep network, can be deemed as a more robust network structure for long-term forecasting under coarse data observation and associated uncertainties, including missing values and sampling/measurement errors. The stability in forecasting is induced from balancing impact of inputs over all time steps in the networks. To analyze this property in various problem conditions, we adapt the recurrent networks with memory structure to environmental time-series problems, such as forecasting water pollution, air pollution, and ozone alarm. In the results, the recurrent networks with memory showed better performance of forecasting in non-stationary environment and long-term time lags.",
        "authors": "Dong-Kyun Kim, Junhyug Noh, Kangil Kim, Minhyeok Kim",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884827",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, a novel remote sensing image registration method based on phase congruency (PC) and spatial constraint is proposed. PC can provide intrinsic and meaningful image features, even when there are complex intensity changes or noise. Image features will be well detected from the corresponding PC images by the SAR-SIFT operator. It means that the feature detection methods in the frequency domain (PC) and the spatial domain (SAR-SIFT operator) are combined. To further improve the result of registration, spatial constraints, including point and line constraint, are established by utilizing the position and orientation information. Then, one to more matches can be removed and the influence of adjacent point can be greatly eliminated. The experimental results demonstrate that our method can obtain a better registration performance with higher accuracy and more correct correspondences than the state-of-the-art methods, such as SIFT, SAR-SIFT, SURF, PSO-SIFT, RIFT, and GLPM.",
        "abstract": "In this paper, a novel remote sensing image registration method based on phase congruency (PC) and spatial constraint is proposed. PC can provide intrinsic and meaningful image features, even when there are complex intensity changes or noise. Image features will be well detected from the corresponding PC images by the SAR-SIFT operator. It means that the feature detection methods in the frequency domain (PC) and the spatial domain (SAR-SIFT operator) are combined. To further improve the result of registration, spatial constraints, including point and line constraint, are established by utilizing the position and orientation information. Then, one to more matches can be removed and the influence of adjacent point can be greatly eliminated. The experimental results demonstrate that our method can obtain a better registration performance with higher accuracy and more correct correspondences than the state-of-the-art methods, such as SIFT, SAR-SIFT, SURF, PSO-SIFT, RIFT, and GLPM.",
        "authors": "Qingxiu Su, Shaodi Liu, Wenping Ma, Yong Zhong, Yue Wu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883410",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In large-scale software-defined networking (SDN), the logically centralized controller usually uses multiple distributed controllers to manage and operate the network in a cooperative manner from the perspective of global network view. One of the major challenges with respect to deploying multiple controllers in SDN is how to synchronize the state among controllers to maintain information consistency. Aiming at solving this problem, we propose a periodically adaptive synchronization strategy of controllers. At first, the consistency level of controllers in the network is quantified based on the characteristics of SDN. Then, an adaptive synchronization strategy is proposed. By the use of this strategy, controllers are divided into three kinds of roles, and the synchronization period is adjusted dynamically by an adaptive function according to the current network state to attain a certain consistency level. The simulation results show that the proposed strategy has merits of reducing communication overhead of controllers as well as improving network availability compared with the other non-adaptive strategies. However, it lacks consideration of load balance among controllers, which will be our future research direction.",
        "abstract": "In large-scale software-defined networking (SDN), the logically centralized controller usually uses multiple distributed controllers to manage and operate the network in a cooperative manner from the perspective of global network view. One of the major challenges with respect to deploying multiple controllers in SDN is how to synchronize the state among controllers to maintain information consistency. Aiming at solving this problem, we propose a periodically adaptive synchronization strategy of controllers. At first, the consistency level of controllers in the network is quantified based on the characteristics of SDN. Then, an adaptive synchronization strategy is proposed. By the use of this strategy, controllers are divided into three kinds of roles, and the synchronization period is adjusted dynamically by an adaptive function according to the current network state to attain a certain consistency level. The simulation results show that the proposed strategy has merits of reducing communication overhead of controllers as well as improving network availability compared with the other non-adaptive strategies. However, it lacks consideration of load balance among controllers, which will be our future research direction.",
        "authors": "Bang Zhang, Min Huang, Xingwei Wang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885130",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper mainly introduces a pulse power supply for high-power semiconductor laser diode arrays with a novel micro-current pre-start control method. According to the operating characteristics of the semiconductor laser, the output current of the pulse power supply is required to be smooth, stable, and without overshoot. In the method of normal direct pulse start control, semiconductor laser is damaged easily by the overshoot and oscillation of pulse current caused by the switching threshold voltage of MOSFET. And the time delay between the current reference signal and the output pulse current makes the design of closed loop more complicated. Based on the above problems, the method of micro-current pre-start control is proposed. The method is simple to operate and it can solve the nonlinear problem of MOSFET caused by the switching threshold voltage. Therefore, it solve the problems of time delay between the current reference signal and the output pulse current and the overshoot and oscillation of output pulse current in the rising process. A 25.6-kW prototype is intended for verifying the effectiveness of the micro-current pre-start control.",
        "abstract": "This paper mainly introduces a pulse power supply for high-power semiconductor laser diode arrays with a novel micro-current pre-start control method. According to the operating characteristics of the semiconductor laser, the output current of the pulse power supply is required to be smooth, stable, and without overshoot. In the method of normal direct pulse start control, semiconductor laser is damaged easily by the overshoot and oscillation of pulse current caused by the switching threshold voltage of MOSFET. And the time delay between the current reference signal and the output pulse current makes the design of closed loop more complicated. Based on the above problems, the method of micro-current pre-start control is proposed. The method is simple to operate and it can solve the nonlinear problem of MOSFET caused by the switching threshold voltage. Therefore, it solve the problems of time delay between the current reference signal and the output pulse current and the overshoot and oscillation of output pulse current in the rising process. A 25.6-kW prototype is intended for verifying the effectiveness of the micro-current pre-start control.",
        "authors": "Deyu Wang, Jing Yuan, Qinglin Zhao, Ruru Cao, Shu Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883779",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Finding clusters in datasets with different distributions and sizes is challenging when clusters are of widely various shapes, sizes, and densities. Based on a similar-to-multiple-point clustering strategy, a novel and simple clustering algorithm named MulSim is presented to address these issues in this paper. MulSim first defines a new distance which can automatically adapt different densities when clustering. Then, the MulSim groups two points together if and only if one point is similar to another point and its similar neighbors. Our comprehensive experiments on both multi-dimensional and two dimensional datasets representing different clustering difficulties, show that the MulSim performs better than classical and state-of-the-art baselines in most cases. Besides, when increasing the size of datasets, MulSim can still ensure good clustering quality. In addition, the impact of the two MulSim parameters on clustering quality as well as the way of the parameter estimation are analyzed. In the end, the practicability and feasibility of the algorithm are tested through a face recognition example.",
        "abstract": "Finding clusters in datasets with different distributions and sizes is challenging when clusters are of widely various shapes, sizes, and densities. Based on a similar-to-multiple-point clustering strategy, a novel and simple clustering algorithm named MulSim is presented to address these issues in this paper. MulSim first defines a new distance which can automatically adapt different densities when clustering. Then, the MulSim groups two points together if and only if one point is similar to another point and its similar neighbors. Our comprehensive experiments on both multi-dimensional and two dimensional datasets representing different clustering difficulties, show that the MulSim performs better than classical and state-of-the-art baselines in most cases. Besides, when increasing the size of datasets, MulSim can still ensure good clustering quality. In addition, the impact of the two MulSim parameters on clustering quality as well as the way of the parameter estimation are analyzed. In the end, the practicability and feasibility of the algorithm are tested through a face recognition example.",
        "authors": "Mei Chen, Mei Zhang, Ming Li, Xiaofang Wen, Zhichong Yang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884902",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Alvis is a formal modeling language intended for developing systems consisting of concurrently operating units (real-time, embedded, and distributed systems). This paper describes the timed version of Alvis that is suitable for modeling discrete-time systems. This paper is the first relatively complete description of timed Alvis. We present formal definition and semantics of timed models and the algorithm of labeled transition system generation and introduce the computer software that we developed to support Alvis. All concepts are illustrated by examples.",
        "abstract": "Alvis is a formal modeling language intended for developing systems consisting of concurrently operating units (real-time, embedded, and distributed systems). This paper describes the timed version of Alvis that is suitable for modeling discrete-time systems. This paper is the first relatively complete description of timed Alvis. We present formal definition and semantics of timed models and the algorithm of labeled transition system generation and introduce the computer software that we developed to support Alvis. All concepts are illustrated by examples.",
        "authors": "Jerzy Biernacki, Marcin Szpyrka, Michał Wypych, Łukasz Podolski",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885249",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The design of a sensitivity improved dual diaphragm based vibrating wire transducer for sensing pneumatic pressure is proposed. The improvement achieved in sensitivity of a dual diaphragm structure over the single diaphragm category is verified through experiments. An experimental setup for efficiently assessing the dynamic performances of the pressure transducers is presented. Eventually, the improved sensitivity concedes improvement in precision and resolution of the instrument. While the vibrating wire transducer based pressure sensors always show better dynamic performances over others, the present dual diaphragm structure offers improved sensitivity, which adds to their dynamic performances and overall performance characteristics.",
        "abstract": "The design of a sensitivity improved dual diaphragm based vibrating wire transducer for sensing pneumatic pressure is proposed. The improvement achieved in sensitivity of a dual diaphragm structure over the single diaphragm category is verified through experiments. An experimental setup for efficiently assessing the dynamic performances of the pressure transducers is presented. Eventually, the improved sensitivity concedes improvement in precision and resolution of the instrument. While the vibrating wire transducer based pressure sensors always show better dynamic performances over others, the present dual diaphragm structure offers improved sensitivity, which adds to their dynamic performances and overall performance characteristics.",
        "authors": "Akin Cellatoglu, Karuppanan Balasubramanian2",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2262012",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The signal processing concept of signal-to-noise ratio (SNR), in its role as a performance measure, is recast within the more general context of information theory, leading to a series of useful insights. Establishing generalized SNR (GSNR) as a rigorous information theoretic measure inherent in any set of observations significantly strengthens its quantitative performance pedigree while simultaneously providing a specific definition under general conditions. In turn, this directly leads to consideration of the log likelihood ratio (LLR): first, as the simplest possible information-preserving transformation (i.e., signal processing algorithm) and subsequently, as an absolute, comparable measure of information for any specific observation exemplar. The information accounting methodology that results permits practical use of both GSNR and LLR as diagnostic scalar performance measurements, directly comparable across alternative system/algorithm designs, applicable at any tap point within any processing string, in a form that is also comparable with the inherent performance bounds due to information conservation.",
        "abstract": "The signal processing concept of signal-to-noise ratio (SNR), in its role as a performance measure, is recast within the more general context of information theory, leading to a series of useful insights. Establishing generalized SNR (GSNR) as a rigorous information theoretic measure inherent in any set of observations significantly strengthens its quantitative performance pedigree while simultaneously providing a specific definition under general conditions. In turn, this directly leads to consideration of the log likelihood ratio (LLR): first, as the simplest possible information-preserving transformation (i.e., signal processing algorithm) and subsequently, as an absolute, comparable measure of information for any specific observation exemplar. The information accounting methodology that results permits practical use of both GSNR and LLR as diagnostic scalar performance measurements, directly comparable across alternative system/algorithm designs, applicable at any tap point within any processing string, in a form that is also comparable with the inherent performance bounds due to information conservation.",
        "authors": "John Polcari",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2277930",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper explores the effective temporal surface-illuminated properties of two-component composites consisting of inclusions of regularly-and irregularly-shaped crystals in a matrix. Time-domain electromagnetic modeling using the finite integration technique is used to calculate scattering (S-) parameters, and from these, the effective relative permittivities are calculated. It is shown that the orientation of inclusions with high permittivity contrast affects the effective electrical permittivity of a composite mixture. For both low and high contrast inclusions, fields localize at edges and corners of the irregular inclusion in a manner not dependent on boundary conditions used in simulation.",
        "abstract": "This paper explores the effective temporal surface-illuminated properties of two-component composites consisting of inclusions of regularly-and irregularly-shaped crystals in a matrix. Time-domain electromagnetic modeling using the finite integration technique is used to calculate scattering (S-) parameters, and from these, the effective relative permittivities are calculated. It is shown that the orientation of inclusions with high permittivity contrast affects the effective electrical permittivity of a composite mixture. For both low and high contrast inclusions, fields localize at edges and corners of the irregular inclusion in a manner not dependent on boundary conditions used in simulation.",
        "authors": "Austin J. Pickles, Michael B. Steer",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2279356",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Simultaneously removing atmospheric turbulence-induced geometric distortion and blurry degradation is a challenging task. In this paper, we propose an effective method to remove or at least reduce turbulence effects in unified complex steerable pyramid (CSP) framework. The proposed method first decomposes the degraded image sequence by CSP. Then, the local motion and the energy information of the image sequence can be represented by multiscale and multidirectional phases and amplitudes. To mitigate turbulence-induced random oscillation, we use temporal average phase as the initial reference phase. Then, the reference phase is iteratively corrected, using the proposed phase correction method which is capable of correcting the large displacement. To reduce blurry degradation, optimal amplitude selection and fusion methods are proposed to reduce blur variation and CSP reconstruction errors. Finally, the corrected phase and fused amplitude can be synthesized to generate a reconstructed image. To further enhance the image quality, a blind deconvolution approach is adopted to deblur the reconstructed image. Through a variety of experiments on the simulated and real data, experimental results show that the proposed method can effectively alleviate the turbulence effects, recover image details, and significantenhance visual quality.",
        "abstract": "Simultaneously removing atmospheric turbulence-induced geometric distortion and blurry degradation is a challenging task. In this paper, we propose an effective method to remove or at least reduce turbulence effects in unified complex steerable pyramid (CSP) framework. The proposed method first decomposes the degraded image sequence by CSP. Then, the local motion and the energy information of the image sequence can be represented by multiscale and multidirectional phases and amplitudes. To mitigate turbulence-induced random oscillation, we use temporal average phase as the initial reference phase. Then, the reference phase is iteratively corrected, using the proposed phase correction method which is capable of correcting the large displacement. To reduce blurry degradation, optimal amplitude selection and fusion methods are proposed to reduce blur variation and CSP reconstruction errors. Finally, the corrected phase and fused amplitude can be synthesized to generate a reconstructed image. To further enhance the image quality, a blind deconvolution approach is adopted to deblur the reconstructed image. Through a variety of experiments on the simulated and real data, experimental results show that the proposed method can effectively alleviate the turbulence effects, recover image details, and significantenhance visual quality.",
        "authors": "Bindang Xue, Chao Zhang, Fugen Zhou, Wei Xiong",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883489",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Recently, swarm intelligence-based algorithms gained attention of the researchers due to their wide applicability and ease of implementation. However, much research has been made on the development of swarm intelligence algorithms but theoretical analysis of these algorithms is still a less explored area of the research. Theoretical analyses of trajectory and convergence of potential solutions toward the equilibrium point in the search space can help the researchers to understand the iteration-wise behavior of the algorithms which can further help in making them efficient. Artificial bee colony (ABC) optimization algorithm is swarm intelligence-based algorithm. This paper presents the convergence analysis of ABC algorithm using theory of dynamical system. Convergent boundaries for the parameters of ABC update equation have also been proposed. Also, the trajectory of potential solutions in the search space is analyzed by obtaining a partial differential equation corresponding to the position update equation of ABC algorithm. The analysis reveals that the ABC algorithm performs better when parameters of the update equation are in the convergent region and potential solutions movement follows 1-D advection equation.",
        "abstract": "Recently, swarm intelligence-based algorithms gained attention of the researchers due to their wide applicability and ease of implementation. However, much research has been made on the development of swarm intelligence algorithms but theoretical analysis of these algorithms is still a less explored area of the research. Theoretical analyses of trajectory and convergence of potential solutions toward the equilibrium point in the search space can help the researchers to understand the iteration-wise behavior of the algorithms which can further help in making them efficient. Artificial bee colony (ABC) optimization algorithm is swarm intelligence-based algorithm. This paper presents the convergence analysis of ABC algorithm using theory of dynamical system. Convergent boundaries for the parameters of ABC update equation have also been proposed. Also, the trajectory of potential solutions in the search space is analyzed by obtaining a partial differential equation corresponding to the position update equation of ABC algorithm. The analysis reveals that the ABC algorithm performs better when parameters of the update equation are in the convergent region and potential solutions movement follows 1-D advection equation.",
        "authors": "Anshul Gopal, Atulya K. Nagar, Jagdish Chand Bansal",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884255",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Fine-grained visual categorization aims at differentiating subcategories, such as different species of birds, models of cars, and variants of aircraft. It often suffers from small inter-variance and large intra-variance. To keep dissimilar images far apart and preserve large intra-variance simultaneously, we propose an adaptive triplet model. At first, images are batched as triplets and input to a general convolutional network, which extracts convolutional image features. Then, we combine adaptive triplet loss and classification loss for multi-task training. Adaptive triplet loss pulls the same-class embeddings together and pushes examples from different subcategories apart. It allocates different weights to hard and easy examples in an adaptive way in the training process. Unlike previous hard mining mechanisms that discard all non-hard triplets, it can benefit from all possible informative examples. Moreover, a second-order distance function is put forward to capture local pairwise interactions of embeddings, which is more discriminative in distance measure. Classification loss is used to provide more direct supervision for training embeddings with category specific concepts. Furthermore, it makes the prediction of category more convenient and more efficient in testing. Experiments demonstrate the state-of-the-art results on three popular fine-grained datasets, including CUB-200-2011, Stanford Cars, and FGVC-Aircraft. In addition, our network structure is relatively simple compared with previous methods, which often suffer from multiple sub-networks and complex training mechanisms. It is also applicable for most up-to-date backbone networks, while others might be restricted to specific convolutional networks.",
        "abstract": "Fine-grained visual categorization aims at differentiating subcategories, such as different species of birds, models of cars, and variants of aircraft. It often suffers from small inter-variance and large intra-variance. To keep dissimilar images far apart and preserve large intra-variance simultaneously, we propose an adaptive triplet model. At first, images are batched as triplets and input to a general convolutional network, which extracts convolutional image features. Then, we combine adaptive triplet loss and classification loss for multi-task training. Adaptive triplet loss pulls the same-class embeddings together and pushes examples from different subcategories apart. It allocates different weights to hard and easy examples in an adaptive way in the training process. Unlike previous hard mining mechanisms that discard all non-hard triplets, it can benefit from all possible informative examples. Moreover, a second-order distance function is put forward to capture local pairwise interactions of embeddings, which is more discriminative in distance measure. Classification loss is used to provide more direct supervision for training embeddings with category specific concepts. Furthermore, it makes the prediction of category more convenient and more efficient in testing. Experiments demonstrate the state-of-the-art results on three popular fine-grained datasets, including CUB-200-2011, Stanford Cars, and FGVC-Aircraft. In addition, our network structure is relatively simple compared with previous methods, which often suffer from multiple sub-networks and complex training mechanisms. It is also applicable for most up-to-date backbone networks, while others might be restricted to specific convolutional networks.",
        "authors": "Jingyun Liang, Jinlin Guo, Songyang Lao, Yanming Guo",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884695",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Searching source code is a common activity in many software engineering tasks. To some extent, the quality of the query determines the accuracy of query results. In practice, it is difficult for developers to provide a high-quality query, especially for the novice who just takes over the software project with a short time. What is more, existing code search techniques using queries expressed in natural language offer little support to help developers determine whether the search results are relevant or not. When a query preforms poorly, it has to be reformulated. In this paper, we present a novel approach, INQRES, to interactively reformulate the search query considering the relations between words in the source code to optimize the query quality. INQRES analyzes the keyword relations in the source code and builds AND and OR relations in an interactive way for developer to select suitable words for query reformulation. To evaluate the effectiveness of INQRES, we perform an empirical study on the jEdit project. Empirical results show that INQRES can effectively reformulate the search query, and the quality of the reformulated query of INQRES is better than that of the state-of-art technique, i.e., QReformu.",
        "abstract": "Searching source code is a common activity in many software engineering tasks. To some extent, the quality of the query determines the accuracy of query results. In practice, it is difficult for developers to provide a high-quality query, especially for the novice who just takes over the software project with a short time. What is more, existing code search techniques using queries expressed in natural language offer little support to help developers determine whether the search results are relevant or not. When a query preforms poorly, it has to be reformulated. In this paper, we present a novel approach, INQRES, to interactively reformulate the search query considering the relations between words in the source code to optimize the query quality. INQRES analyzes the keyword relations in the source code and builds AND and OR relations in an interactive way for developer to select suitable words for query reformulation. To evaluate the effectiveness of INQRES, we perform an empirical study on the jEdit project. Empirical results show that INQRES can effectively reformulate the search query, and the quality of the reformulated query of INQRES is better than that of the state-of-art technique, i.e., QReformu.",
        "authors": "Bin Li, Cheng Zhou, Jinting Lu, Wanzhi Wen, Xiaobing Sun, Ying Wei",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883963",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Graph embedding is a very useful dimensionality reduction technique in pattern recognition. In this paper, we develop a novel discriminative dimensionality reduction technique entitled sparsity and geometry preserving graph embedding (SGPGE). SGPGE can not only capture the sparse reconstructive relationships among training samples but also discover the intrinsic geometry and latent discrimination from high-dimensional data. In SGPGE, the novel ways of constructing global and local adjacent graphs are developed. The built graphs with discriminant and geometrical information are more informative in graph embedding. Integrating the sparse reconstruction with the designed local and global adjacent graph constructions, SGPGE newly characterizes with-class sparsity and geometry preserving scatter, between class sparsity and geometry preserving scatter and local sparsity and geometry preserving scatter to formulate the objective function of dimensionality reduction, and learns the discriminative transformation matrix using maximum margin criterion. Moreover, we also propose three variants of SGPGE. Experimental results on six public face datasets have demonstrated that the proposed methods are effective dimensionality reduction techniques with very good performance for classification in the embedding subspace.",
        "abstract": "Graph embedding is a very useful dimensionality reduction technique in pattern recognition. In this paper, we develop a novel discriminative dimensionality reduction technique entitled sparsity and geometry preserving graph embedding (SGPGE). SGPGE can not only capture the sparse reconstructive relationships among training samples but also discover the intrinsic geometry and latent discrimination from high-dimensional data. In SGPGE, the novel ways of constructing global and local adjacent graphs are developed. The built graphs with discriminant and geometrical information are more informative in graph embedding. Integrating the sparse reconstruction with the designed local and global adjacent graph constructions, SGPGE newly characterizes with-class sparsity and geometry preserving scatter, between class sparsity and geometry preserving scatter and local sparsity and geometry preserving scatter to formulate the objective function of dimensionality reduction, and learns the discriminative transformation matrix using maximum margin criterion. Moreover, we also propose three variants of SGPGE. Experimental results on six public face datasets have demonstrated that the proposed methods are effective dimensionality reduction techniques with very good performance for classification in the embedding subspace.",
        "authors": "David Zhang, Jianping Gou, Lan Du, Xiangjun Shen, Yongzhao Zhan, Zhang Yi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884027",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A resilience-constrained operation strategy is proposed in this paper using a battery energy storage system (BESS) as a resilience resource. A proactive operation scheme is proposed for the pre-disturbance phase by formulating resiliency cuts for the state of charge (SoC) of BESS units. These resiliency cuts can assure the survivability of critical loads for n intervals after the occurrence of an event. A resilience constraint index is formulated to access the feasibility of the operation model for each selected value of n. In the emergency mode, different possible scenarios are analyzed and a unified survivability enhancement strategy is proposed, which comprises of dynamic penalty costs and target SoC. Dynamic penalty costs can potentially avoid the unnecessary load shedding by shifting it toward the possible end of the scheduling horizon. Similarly, the inclusion of target SoC for the last interval of the scheduling horizon can mitigate the load shedding of critical loads on the following day. A two-step adaptive robust optimization scheme is adopted to incorporate the prevailing uncertainties in loads and renewables in the operation model. Finally, the impact of various decision parameters, involved in the problem formulation, is analyzed with reference to the proposed resilience-constrained operation strategy for hybrid microgrids.",
        "abstract": "A resilience-constrained operation strategy is proposed in this paper using a battery energy storage system (BESS) as a resilience resource. A proactive operation scheme is proposed for the pre-disturbance phase by formulating resiliency cuts for the state of charge (SoC) of BESS units. These resiliency cuts can assure the survivability of critical loads for n intervals after the occurrence of an event. A resilience constraint index is formulated to access the feasibility of the operation model for each selected value of n. In the emergency mode, different possible scenarios are analyzed and a unified survivability enhancement strategy is proposed, which comprises of dynamic penalty costs and target SoC. Dynamic penalty costs can potentially avoid the unnecessary load shedding by shifting it toward the possible end of the scheduling horizon. Similarly, the inclusion of target SoC for the last interval of the scheduling horizon can mitigate the load shedding of critical loads on the following day. A two-step adaptive robust optimization scheme is adopted to incorporate the prevailing uncertainties in loads and renewables in the operation model. Finally, the impact of various decision parameters, involved in the problem formulation, is analyzed with reference to the proposed resilience-constrained operation strategy for hybrid microgrids.",
        "authors": "Akhtar Hussain, Hak-Man Kim, Van-Hai Bui",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883418",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The influences of the intermediate band (IB) filling, the absorption coefficient constants, and the IB position on the efficiency of a quantum dot intermediate band solar cell (QD-IBSC) are investigated considering the spatial variation of subbandgap generation rates. A new definition of optimal intermediate band filling is proposed. A mathematical model is developed to optimize the intermediate band solar cell (IBSC) structure under idealized conditions, which calculates the optimal ratio of the subbandgap absorption coefficient constants and the optimal position of IB.",
        "abstract": "The influences of the intermediate band (IB) filling, the absorption coefficient constants, and the IB position on the efficiency of a quantum dot intermediate band solar cell (QD-IBSC) are investigated considering the spatial variation of subbandgap generation rates. A new definition of optimal intermediate band filling is proposed. A mathematical model is developed to optimize the intermediate band solar cell (IBSC) structure under idealized conditions, which calculates the optimal ratio of the subbandgap absorption coefficient constants and the optimal position of IB.",
        "authors": "Anisul Haque, Urmita Sikder",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2265094",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Prediction of quality of service (QoS) is a critical area of research for cloud service recommendation. The disadvantage of QoS values is that they are directly related to time series of service status and network condition and thus instantly vary over time. The main contribution of this paper is to consider service invocation time as a dynamic factor in the collaborative filtering model and recommend high-quality services for target user. In particular, this paper proposes a time-aware matrix factorization (TMF) model that integrates QoS time series to provide two-phase QoS predictions for cloud service recommendation. The TMF model uses an adaptive matrix factorization model on a sparse QoS dataset to predict the missing QoS values. A temporal smoothing method is then developed and applied to the predicted result to perform the time-varying QoS prediction that accounts for the dependence of QoS values at different time intervals. The numerical experiments presented are conducted to validate the accuracy of the proposed method on a public QoS dataset.",
        "abstract": "Prediction of quality of service (QoS) is a critical area of research for cloud service recommendation. The disadvantage of QoS values is that they are directly related to time series of service status and network condition and thus instantly vary over time. The main contribution of this paper is to consider service invocation time as a dynamic factor in the collaborative filtering model and recommend high-quality services for target user. In particular, this paper proposes a time-aware matrix factorization (TMF) model that integrates QoS time series to provide two-phase QoS predictions for cloud service recommendation. The TMF model uses an adaptive matrix factorization model on a sparse QoS dataset to predict the missing QoS values. A temporal smoothing method is then developed and applied to the predicted result to perform the time-varying QoS prediction that accounts for the dependence of QoS values at different time intervals. The numerical experiments presented are conducted to validate the accuracy of the proposed method on a public QoS dataset.",
        "authors": "Fengji Luo, Gianluca Ranzi, Junhao Wen, Shun Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883939",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Dimensionality reduction is commonly used to preprocess high-dimensional data, which is an essential step in machine learning and data mining. An outstanding low-dimensional feature can improve the efficiency of subsequent learning tasks. However, existing methods of dimensionality reduction mostly involve datasets with sufficient labels and fail to achieve effective feature vectors for datasets with insufficient labels. In this paper, an unsupervised multiple layered sparse autoencoder model is studied. Its advantage is that it reduces the reconstruction error as its optimization goal, with the resulting low-dimensional feature being reconstructed to the original dataset as much as possible. Therefore, the reduction of high-dimensional datasets to low-dimensional datasets is effective. First, the relationship among the reconstructed data, the number of iterations, and the number of hidden variables is explored. Second, the dimensionality reduction ability of the sparse autoencoder is proven. Several classical feature representation methods are compared with the sparse autoencoder on publicly available datasets, and the corresponding low-dimensional representations are placed into different supervised classifiers and the classification performances reported. Finally, by adjusting the parameters that might influence the classification performance, the parametric sensitivity of the sparse autoencoder is shown. The extensively low-dimensional feature classification experimental results demonstrated that the sparse autoencoder is more efficient and reliable than the other selected classical dimensional reduction algorithms.",
        "abstract": "Dimensionality reduction is commonly used to preprocess high-dimensional data, which is an essential step in machine learning and data mining. An outstanding low-dimensional feature can improve the efficiency of subsequent learning tasks. However, existing methods of dimensionality reduction mostly involve datasets with sufficient labels and fail to achieve effective feature vectors for datasets with insufficient labels. In this paper, an unsupervised multiple layered sparse autoencoder model is studied. Its advantage is that it reduces the reconstruction error as its optimization goal, with the resulting low-dimensional feature being reconstructed to the original dataset as much as possible. Therefore, the reduction of high-dimensional datasets to low-dimensional datasets is effective. First, the relationship among the reconstructed data, the number of iterations, and the number of hidden variables is explored. Second, the dimensionality reduction ability of the sparse autoencoder is proven. Several classical feature representation methods are compared with the sparse autoencoder on publicly available datasets, and the corresponding low-dimensional representations are placed into different supervised classifiers and the classification performances reported. Finally, by adjusting the parameters that might influence the classification performance, the parametric sensitivity of the sparse autoencoder is shown. The extensively low-dimensional feature classification experimental results demonstrated that the sparse autoencoder is more efficient and reliable than the other selected classical dimensional reduction algorithms.",
        "authors": "Chan Li, Jianran Liu, Wenyuan Yang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884697",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Hydropneumatic suspension has the characteristics of nonlinear stiffness, nonlinear friction damping, and nonlinear hydraulic damping, and therefore, it has been widely applied to heavy vehicles. A hydropneumatic suspension with a special structure was proposed and studied in this paper. The nonlinear quarter-vehicle model was built by using Newton’s laws according to its configuration. Then, this nonlinear mathematic model was linearized through the statistical linearization method on the basis of random vibration theory. Next, the transfer functions of the suspension system subjected to random road excitation were built according to the statistical linearization model and the power density spectrum approach. In addition, the responses of vehicle body acceleration, tire relative dynamic load, and suspension deflection with respect to the wideband random excitation due to road roughness were obtained according to the James formula. Next, the influences of the equivalent damping ratio and the equivalent frequency ratio on vehicle riding comfort, riding safety, handing stability, and suspension reliability were analyzed through simulations. These results provided the basic principles for selecting the reasonable hydropneumatic suspension parameters.",
        "abstract": "Hydropneumatic suspension has the characteristics of nonlinear stiffness, nonlinear friction damping, and nonlinear hydraulic damping, and therefore, it has been widely applied to heavy vehicles. A hydropneumatic suspension with a special structure was proposed and studied in this paper. The nonlinear quarter-vehicle model was built by using Newton’s laws according to its configuration. Then, this nonlinear mathematic model was linearized through the statistical linearization method on the basis of random vibration theory. Next, the transfer functions of the suspension system subjected to random road excitation were built according to the statistical linearization model and the power density spectrum approach. In addition, the responses of vehicle body acceleration, tire relative dynamic load, and suspension deflection with respect to the wideband random excitation due to road roughness were obtained according to the James formula. Next, the influences of the equivalent damping ratio and the equivalent frequency ratio on vehicle riding comfort, riding safety, handing stability, and suspension reliability were analyzed through simulations. These results provided the basic principles for selecting the reasonable hydropneumatic suspension parameters.",
        "authors": "Chunhong Ruan, Shuping Cao, Zuti Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884245",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper presents a logarithmic detection scheme for reducing the bandwidth of the data acquisition (DAQ) in a Brillouin optical time domain reflectometer (BOTDR). From the analysis of signal features in frequency-scanning BOTDR, the reduction effect of the proposed detection scheme on the bandwidth of DAQ is investigated theoretically. We implement the scheme and evaluate its influence on the performance of BOTDR over a ~10-km sensing fiber by employing a digitalizer with bandwidth of only 50 MHz. The experimental results show that the spatial resolution of 1.02 m is achieved even though the bandwidth of DAQ is only 50 MHz. At 100-m end of sensing fiber, the root-mean-square error (RMSE) of Brillouin frequency shift (BFS) is 0.67 MHz corresponding to the strain error of 13.4 με and temperature error of 0.66°C. As a comparison, the BFS is likewise acquired by a 200-MHz-bandwidth digitalizer, the RMSE of which is 0.66 MHz corresponding to the strain error of 13.2 με and temperature error of 0.65°C, nearly consistent with the former. It is confirmed that the logarithmic detection scheme can be used in BOTDR for reducing bandwidth request of DAQ meanwhile without excessive deterioration of spatial resolution or measurement accuracy.",
        "abstract": "This paper presents a logarithmic detection scheme for reducing the bandwidth of the data acquisition (DAQ) in a Brillouin optical time domain reflectometer (BOTDR). From the analysis of signal features in frequency-scanning BOTDR, the reduction effect of the proposed detection scheme on the bandwidth of DAQ is investigated theoretically. We implement the scheme and evaluate its influence on the performance of BOTDR over a ~10-km sensing fiber by employing a digitalizer with bandwidth of only 50 MHz. The experimental results show that the spatial resolution of 1.02 m is achieved even though the bandwidth of DAQ is only 50 MHz. At 100-m end of sensing fiber, the root-mean-square error (RMSE) of Brillouin frequency shift (BFS) is 0.67 MHz corresponding to the strain error of 13.4 με and temperature error of 0.66°C. As a comparison, the BFS is likewise acquired by a 200-MHz-bandwidth digitalizer, the RMSE of which is 0.66 MHz corresponding to the strain error of 13.2 με and temperature error of 0.65°C, nearly consistent with the former. It is confirmed that the logarithmic detection scheme can be used in BOTDR for reducing bandwidth request of DAQ meanwhile without excessive deterioration of spatial resolution or measurement accuracy.",
        "authors": "Dong Wang, Mingjiang Zhang, Qing Bai, Xin Liu, Xuan Zheng, Yu Wang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883412",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Change point detection is essential to understand the time-evolving structure of dynamic networks. Recent research shows that a latent semantic indexing (LSI)-based algorithm effectively detects the change points of a dynamic network. The LSI-based method involves a singular value decomposition (SVD) on the data matrix. In a dynamic scenario, recomputing the SVD of a large matrix each time new data arrives is prohibitively expensive and impractical. A more efficient approach is to incrementally update the decomposition. However, in the classical incremental SVD (incSVD) algorithm, the information of the newly added columns is not fully considered in updating the right singular space, resulting in an approximation error which cannot be ignored. This paper proposes an enhanced incSVD (EincSVD) algorithm, in which the right singular matrix is calculated in an alternative way. An adaptive EincSVD (AEincSVD) algorithm is also proposed to further reduce the computational complexity. Theoretical analysis proves that the approximation error of the EincSVD is smaller than that of the incSVD. Simulation results demonstrate that the EincSVD and the AEincSVD perform much better than the incSVD on change point detection, and the performance of the EincSVD is comparable to the batch SVD algorithm.",
        "abstract": "Change point detection is essential to understand the time-evolving structure of dynamic networks. Recent research shows that a latent semantic indexing (LSI)-based algorithm effectively detects the change points of a dynamic network. The LSI-based method involves a singular value decomposition (SVD) on the data matrix. In a dynamic scenario, recomputing the SVD of a large matrix each time new data arrives is prohibitively expensive and impractical. A more efficient approach is to incrementally update the decomposition. However, in the classical incremental SVD (incSVD) algorithm, the information of the newly added columns is not fully considered in updating the right singular space, resulting in an approximation error which cannot be ignored. This paper proposes an enhanced incSVD (EincSVD) algorithm, in which the right singular matrix is calculated in an alternative way. An adaptive EincSVD (AEincSVD) algorithm is also proposed to further reduce the computational complexity. Theoretical analysis proves that the approximation error of the EincSVD is smaller than that of the incSVD. Simulation results demonstrate that the EincSVD and the AEincSVD perform much better than the incSVD on change point detection, and the performance of the EincSVD is comparable to the batch SVD algorithm.",
        "authors": "Jiang Zhu, Xiaokang Lin, Yongsheng Cheng",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883647",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, we propose a novel technique to effectively design a channel adaptive collaborative constellation called the layered adaptive collaborative constellation (LACC) for multiple-input multiple-output visible light communications. Recently, a channel-adaptive space-collaborative constellation (CASCC) that designs the constellation with the consideration of channel into the model to improve the receiver mobility and enhance the error performance. However, this scheme only considers into optimization four constellation points that form a diamond shape in the receiver space. Our proposed scheme effectively exploits the layered structure of collaborative constellation and overcomes the aforementioned drawbacks of CASCC. Furthermore, an improved low-complexity maximum likelihood (ML)-based detector is derived. Simulation results show that the LACC can achieve some performance gain, compared with previous constellation design schemes for different scenarios, especially really high-correlation channels and under imperfect channel state information. Moreover, the proposed detector drastically reduces computational complexity while maintaining the same performance as a traditional ML detector.",
        "abstract": "In this paper, we propose a novel technique to effectively design a channel adaptive collaborative constellation called the layered adaptive collaborative constellation (LACC) for multiple-input multiple-output visible light communications. Recently, a channel-adaptive space-collaborative constellation (CASCC) that designs the constellation with the consideration of channel into the model to improve the receiver mobility and enhance the error performance. However, this scheme only considers into optimization four constellation points that form a diamond shape in the receiver space. Our proposed scheme effectively exploits the layered structure of collaborative constellation and overcomes the aforementioned drawbacks of CASCC. Furthermore, an improved low-complexity maximum likelihood (ML)-based detector is derived. Simulation results show that the LACC can achieve some performance gain, compared with previous constellation design schemes for different scenarios, especially really high-correlation channels and under imperfect channel state information. Moreover, the proposed detector drastically reduces computational complexity while maintaining the same performance as a traditional ML detector.",
        "authors": "Manh Le Tran, Sunghwan Kim",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883346",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "At the request of the Domestic Nuclear Detection Office (DNDO), a Study Committee comprised of representatives from the American Physical Society, Panel on Public Affairs, the IEEE, and Nuclear and Plasma Sciences Society performed a technical review of the DNDO Transformational and Applied Research Directorate (TARD) R&D program. TARD's principal objective is to address gaps in the Global Nuclear Detection Architecture (GNDA) through improvements in the performance, cost, and operational burden of detectors and systems. The charge to the Study Committee was to investigate the existing TARD R&D plan and portfolio, recommend changes to the existing plan, and recommend possible new R&D areas and opportunities. This report is the result of an independent, detailed analysis of the current R&D plan and includes, for each application area, observations, and recommendations to focus future investments within the context of the TARD mission.",
        "abstract": "At the request of the Domestic Nuclear Detection Office (DNDO), a Study Committee comprised of representatives from the American Physical Society, Panel on Public Affairs, the IEEE, and Nuclear and Plasma Sciences Society performed a technical review of the DNDO Transformational and Applied Research Directorate (TARD) R&D program. TARD's principal objective is to address gaps in the Global Nuclear Detection Architecture (GNDA) through improvements in the performance, cost, and operational burden of detectors and systems. The charge to the Study Committee was to investigate the existing TARD R&D plan and portfolio, recommend changes to the existing plan, and recommend possible new R&D areas and opportunities. This report is the result of an independent, detailed analysis of the current R&D plan and includes, for each application area, observations, and recommendations to focus future investments within the context of the TARD mission.",
        "authors": "Adam Isles, Anthony D. Lavietes, James Trebes, Jill Dahlburg, John Donnelly, Robert Borchers",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2281203",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Code injection attacks (CIAs) exploit security vulnerabilities and computer bugs that are caused by processing invalid codes. CIA is a problem which hackers attempt to introduce to any new method, their objective being to bypass the protection system. In this paper, we present a tool called GMSA, developed to detect a variety of CIAs, for example, cross-site scripting (XSS) attack, SQL injection attack, shell injection attack (command injection attack), and file inclusion attack. The latter consists of local file inclusion and remote file inclusion. Our empirical analysis reveals that compared with existing research, gathering multiple signatures approach (GMSA) executes a precision performance (accuracy of the proposed algorithm is 99.45%). The false positive rate (FPR) of GMSA is 0.59%, which is low compared with what other research has reported. The low FPR is the most important factor. Ideally, the defense algorithm should balance between the FPR and true positive rate (TPR) because with existing methodologies, security experts can defend against a broad range of CIAs with uncomplicated security software. Typical protection methods yield a high FPR. Our method results in high TPR while minimizing the resources needed to address the false positive. GMSA can detect four types of CIA. This is more comprehensive than other research techniques that are restricted to only two major types of CIA, namely, SQL injection and XSS attacks.",
        "abstract": "Code injection attacks (CIAs) exploit security vulnerabilities and computer bugs that are caused by processing invalid codes. CIA is a problem which hackers attempt to introduce to any new method, their objective being to bypass the protection system. In this paper, we present a tool called GMSA, developed to detect a variety of CIAs, for example, cross-site scripting (XSS) attack, SQL injection attack, shell injection attack (command injection attack), and file inclusion attack. The latter consists of local file inclusion and remote file inclusion. Our empirical analysis reveals that compared with existing research, gathering multiple signatures approach (GMSA) executes a precision performance (accuracy of the proposed algorithm is 99.45%). The false positive rate (FPR) of GMSA is 0.59%, which is low compared with what other research has reported. The low FPR is the most important factor. Ideally, the defense algorithm should balance between the FPR and true positive rate (TPR) because with existing methodologies, security experts can defend against a broad range of CIAs with uncomplicated security software. Typical protection methods yield a high FPR. Our method results in high TPR while minimizing the resources needed to address the false positive. GMSA can detect four types of CIA. This is more comprehensive than other research techniques that are restricted to only two major types of CIA, namely, SQL injection and XSS attacks.",
        "authors": "Hussein Alnabulsi, Majharul Talukder, Rafiqul Islam",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884201",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The intermittent characteristics and high penetration of renewables (such as photovoltaic and wind powers) cause voltage and stability problems in power systems. The static synchronous compensator (STATCOM) has recently acquired attention on account of its ability to regulate system voltages and improve system stability. This paper presents a novel interval type-II fuzzy logic system (IT2 FLS)-based controller, which consists of current and voltage regulators, applied to the STATCOM, to mitigate bus voltage variations caused by large disturbances (such as intermittent generation of photovoltaic arrays). The current regulator is used to produce the phase angle at the voltage-source converter in the STATCOM while the voltage regulator outputs the current reference on the quadrature axis for the STATCOM. The parameters of the upper/lower membership functions and control gains are optimized by particle swarm optimization. A realistic 10-bus distribution system is used to demonstrate the effectiveness of the proposed method. Comparative studies reveal that the proposed method outperforms traditional PI and type-I FLS-based methods.",
        "abstract": "The intermittent characteristics and high penetration of renewables (such as photovoltaic and wind powers) cause voltage and stability problems in power systems. The static synchronous compensator (STATCOM) has recently acquired attention on account of its ability to regulate system voltages and improve system stability. This paper presents a novel interval type-II fuzzy logic system (IT2 FLS)-based controller, which consists of current and voltage regulators, applied to the STATCOM, to mitigate bus voltage variations caused by large disturbances (such as intermittent generation of photovoltaic arrays). The current regulator is used to produce the phase angle at the voltage-source converter in the STATCOM while the voltage regulator outputs the current reference on the quadrature axis for the STATCOM. The parameters of the upper/lower membership functions and control gains are optimized by particle swarm optimization. A realistic 10-bus distribution system is used to demonstrate the effectiveness of the proposed method. Comparative studies reveal that the proposed method outperforms traditional PI and type-I FLS-based methods.",
        "authors": "Meng-Ju Liu, Ying-Yi Hong",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885198",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "To study the fundamental dynamics of the nonequatorial space elevator, a universal modeling strategy is presented, which applies to any anchored latitude. The equations of motion are established on the basis of the lumped mass model of the tether coupled with an interactional elevator. The relationship between the maximum anchored latitude and the design stress is studied by solving the equilibrium equations. The linearized equations of motion are formulated on the basis of the static equilibrium positions to analyze the modal of the tether. Numerical simulation is performed to analyze the properties of the system with a moving elevator. The results show that a positive correlation arises between the maximum anchored latitudes and the design stress. The fundamental modal and the fifth-order modal are associated with the latitudes considering the frequency and modal shape, respectively. Furthermore, the asymptotically stable characteristics are verified. The amplitude of the residual oscillation is magnified with an increase in the anchored latitude or the elevator speed.",
        "abstract": "To study the fundamental dynamics of the nonequatorial space elevator, a universal modeling strategy is presented, which applies to any anchored latitude. The equations of motion are established on the basis of the lumped mass model of the tether coupled with an interactional elevator. The relationship between the maximum anchored latitude and the design stress is studied by solving the equilibrium equations. The linearized equations of motion are formulated on the basis of the static equilibrium positions to analyze the modal of the tether. Numerical simulation is performed to analyze the properties of the system with a moving elevator. The results show that a positive correlation arises between the maximum anchored latitudes and the design stress. The fundamental modal and the fifth-order modal are associated with the latitudes considering the frequency and modal shape, respectively. Furthermore, the asymptotically stable characteristics are verified. The amplitude of the residual oscillation is magnified with an increase in the anchored latitude or the elevator speed.",
        "authors": "Dun Liu, Naigang Cui, Youhua Fan, Zhenkun Wang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883363",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Dimension reduction (DR) is an essential preprocessing for hyperspectral image (HSI) classification. Recently, nonnegative matrix factorization (NMF) has been shown as an effective tool for the DR of hyperspectral data given the fact that it provides interpretable results. However, the basic NMF ignores the geometric structure information of the HSI data, thus limiting its performance. To this end, a novel regularized NMF method, termed NMF with adaptive graph regularizer (NMFAGR), is proposed for the spectral-spatial dimension reduction of hyperspectral data in this paper. Specifically, to enhance the preservation ability of the geometric structure information, the NMFAGR performs the dimension reduction and graph learning simultaneously. Regarding the mutual correlation between these two tasks, a graph regularizer is added as an interaction. Moreover, to effectively utilize complementary information among spectral-spatial features, the NMFAGR allocates feature weight factors automatically without requiring any additional parameters. An efficient algorithm is utilized to solve the optimization problem. The effectiveness of the proposed method is demonstrated on three benchmark hyperspectral data sets through experimentation.",
        "abstract": "Dimension reduction (DR) is an essential preprocessing for hyperspectral image (HSI) classification. Recently, nonnegative matrix factorization (NMF) has been shown as an effective tool for the DR of hyperspectral data given the fact that it provides interpretable results. However, the basic NMF ignores the geometric structure information of the HSI data, thus limiting its performance. To this end, a novel regularized NMF method, termed NMF with adaptive graph regularizer (NMFAGR), is proposed for the spectral-spatial dimension reduction of hyperspectral data in this paper. Specifically, to enhance the preservation ability of the geometric structure information, the NMFAGR performs the dimension reduction and graph learning simultaneously. Regarding the mutual correlation between these two tasks, a graph regularizer is added as an interaction. Moreover, to effectively utilize complementary information among spectral-spatial features, the NMFAGR allocates feature weight factors automatically without requiring any additional parameters. An efficient algorithm is utilized to solve the optimization problem. The effectiveness of the proposed method is demonstrated on three benchmark hyperspectral data sets through experimentation.",
        "authors": "Bob Zhang, Wei Yan, Zuyuan Yang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884501",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper proposes a framework to improve the existing distortion functions designed for JPEG steganography, which results in a better capability of countering steganalysis. Different from the existing steganography approach that minimizes image distortion, we minimize the feature distortion caused by data embedding. Given a JPEG image, we construct a reference image close to the image before JPEG compression. Guided by both the reference image and the feature distortion minimization, the state-of-the-art distortion functions designed for syndrome trellis coding embedding are improved by distinguishing the embedding costs for +1 versus -1 embedding. This paper has three contributions. First, the proposed framework outperforms the traditional, since we use the constructed reference image and the public steganalytic knowledge for data embedding. Second, the proposed framework is universal for improving distortion functions that were designed for JPEG steganography. Finally, experimental results also prove that the proposed approach has a better undetectability when examined by modern steganalytic tools.",
        "abstract": "This paper proposes a framework to improve the existing distortion functions designed for JPEG steganography, which results in a better capability of countering steganalysis. Different from the existing steganography approach that minimizes image distortion, we minimize the feature distortion caused by data embedding. Given a JPEG image, we construct a reference image close to the image before JPEG compression. Guided by both the reference image and the feature distortion minimization, the state-of-the-art distortion functions designed for syndrome trellis coding embedding are improved by distinguishing the embedding costs for +1 versus -1 embedding. This paper has three contributions. First, the proposed framework outperforms the traditional, since we use the constructed reference image and the public steganalytic knowledge for data embedding. Second, the proposed framework is universal for improving distortion functions that were designed for JPEG steganography. Finally, experimental results also prove that the proposed approach has a better undetectability when examined by modern steganalytic tools.",
        "authors": "Dengpan Ye, Min Yang, Xinpeng Zhang, Zhenxing Qian, Zichi Wang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884198",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper investigates the problem of identifying the individual imaging device source of the same model from a natural image in RAW format. We propose an enhanced Poissonian-Gaussian model describing the distribution of pixels from a RAW image. The parameters of this statistical noise model are considered as unique fingerprints and, hence, used to identify the source camera device. The source camera identification problem is cast within the framework of the hypothesis testing theory. In an ideal context, where all model parameters are perfectly known, the Likelihood Ratio Test (LRT) is presented, and its performance is theoretically established. The statistical performance of this optimal LRT serves as an upper bound for the detection power. For a practical use, when the nuisance parameters are unknown, a generalized LRT based on estimation of those parameters is designed to deal with unknown expectation of pixels (roughly, the image content). More importantly, by combining multiple sub classifiers together with the voting strategy, a multi-classifier is proposed to identify multiple individual devices. Numerical results on simulated data and on various large dataset of real natural images highlight the relevance of our proposed approach.",
        "abstract": "This paper investigates the problem of identifying the individual imaging device source of the same model from a natural image in RAW format. We propose an enhanced Poissonian-Gaussian model describing the distribution of pixels from a RAW image. The parameters of this statistical noise model are considered as unique fingerprints and, hence, used to identify the source camera device. The source camera identification problem is cast within the framework of the hypothesis testing theory. In an ideal context, where all model parameters are perfectly known, the Likelihood Ratio Test (LRT) is presented, and its performance is theoretically established. The statistical performance of this optimal LRT serves as an upper bound for the detection power. For a practical use, when the nuisance parameters are unknown, a generalized LRT based on estimation of those parameters is designed to deal with unknown expectation of pixels (roughly, the image content). More importantly, by combining multiple sub classifiers together with the voting strategy, a multi-classifier is proposed to identify multiple individual devices. Numerical results on simulated data and on various large dataset of real natural images highlight the relevance of our proposed approach.",
        "authors": "Florent Retraint, Tong Qiao",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884710",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Currently, majority of persons were immobilized and need aid from caretakers due to disability. To reduce and overcome such problem, there was a need for developing human-computer interface (HCI) with the help of biosignals. In this paper, we propose a two-channel elecctrooculograpy (EOG)-based HCI to encourage the contact ability as well as value of life for paralyzed persons who cannot speak or shift their extremity by using 20 subjects with the help of ADT26 Bio amplifier. EOG signals were collected for 11 tasks from both vertical and horizontal eye movement by using gold-platted electrodes. The extracted EOG signals were processed with convolution and Plancherel theorem to obtain the features. Layered recurrent neural network (LRNN) was implemented to analyze the extracted features and then converted into a sequence of commands to control the HCI. A graphical user interface was developed using MATLAB to help a user to convey their thoughts. This paper shows an average classification accuracy of 90.72% for convolution features and 91.28% for Plancherel features. Off-line single trail analysis was also performed to analyze the recognition accuracy of the proposed HCI system. The off-line analysis displayed that Plancherel features using LRNN were high compared to convolution features using LRNN. From this paper, we found that LRNN architecture using Plancherel features was more suitable for developing EOG-based HCI. Single trail analysis was conducted to identify the recognizing accuracy in offline. The off-line results indicated that in comparison with other EOG-based HCI systems, our system was user friendly and needs minimum training to operate.",
        "abstract": "Currently, majority of persons were immobilized and need aid from caretakers due to disability. To reduce and overcome such problem, there was a need for developing human-computer interface (HCI) with the help of biosignals. In this paper, we propose a two-channel elecctrooculograpy (EOG)-based HCI to encourage the contact ability as well as value of life for paralyzed persons who cannot speak or shift their extremity by using 20 subjects with the help of ADT26 Bio amplifier. EOG signals were collected for 11 tasks from both vertical and horizontal eye movement by using gold-platted electrodes. The extracted EOG signals were processed with convolution and Plancherel theorem to obtain the features. Layered recurrent neural network (LRNN) was implemented to analyze the extracted features and then converted into a sequence of commands to control the HCI. A graphical user interface was developed using MATLAB to help a user to convey their thoughts. This paper shows an average classification accuracy of 90.72% for convolution features and 91.28% for Plancherel features. Off-line single trail analysis was also performed to analyze the recognition accuracy of the proposed HCI system. The off-line analysis displayed that Plancherel features using LRNN were high compared to convolution features using LRNN. From this paper, we found that LRNN architecture using Plancherel features was more suitable for developing EOG-based HCI. Single trail analysis was conducted to identify the recognizing accuracy in offline. The off-line results indicated that in comparison with other EOG-based HCI systems, our system was user friendly and needs minimum training to operate.",
        "authors": "G. Emayavaramban, Gu Jialu, M. Pallikonda Rajasekaran, M. Thilagaraj, S. Ramkumar, V. Muneeswaran",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884411",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Prediction accuracy (true positives, false positives, and so on) is the usual way for evaluating disk-failure prediction models. Realistically however, we aim not only to correctly predict failures, but also to protect data against failure, i.e., we need to take appropriate action after a failure prediction. In the context of storage systems, protecting data requires that we migrate at-risk data, but this consumes network and disk bandwidth, which is particularly problematic for large-scale and cloud systems. This paper consolidates and builds on Li et al. (2016), where we propose using two new metrics, migration rate (MR) and mismigration rate (MMR), to measure the quality of disk failure prediction: MR measures how much at-risk data is migrated (and therefore protected) as a result of correct failure predictions, while MMR measures how much data is migrated needlessly as a result of incorrect failure predictions. In this paper, we additionally propose measuring quality in terms of migration time and mismigration time, which measure the time spent migrating at-risk disks, and the time spent mismigrating healthy disks caused by false alarms, respectively. To demonstrate these metrics' usefulness, we use them to compare disk-failure prediction methods: we compare: 1) a classification tree (CT) model against a state-of-the-art recurrent neural network (RNN) model and 2) a gradient-boosted regression tree (GBRT) model (which predicts residual life) against RNN. We observe that while RNN performs best in the prediction accuracy experiments, the CT and GBRT models sometimes outperform RNN in the resource-dependent migrationrate experiments. We conclude that prediction accuracy is sometimes misleading: correct predictions do not necessarily imply protected data. We additionally present an improved GBRT model (GBRT+), which offers a practical improvement in disk residual-life prediction accordingly to the newly proposed metrics.",
        "abstract": "Prediction accuracy (true positives, false positives, and so on) is the usual way for evaluating disk-failure prediction models. Realistically however, we aim not only to correctly predict failures, but also to protect data against failure, i.e., we need to take appropriate action after a failure prediction. In the context of storage systems, protecting data requires that we migrate at-risk data, but this consumes network and disk bandwidth, which is particularly problematic for large-scale and cloud systems. This paper consolidates and builds on Li et al. (2016), where we propose using two new metrics, migration rate (MR) and mismigration rate (MMR), to measure the quality of disk failure prediction: MR measures how much at-risk data is migrated (and therefore protected) as a result of correct failure predictions, while MMR measures how much data is migrated needlessly as a result of incorrect failure predictions. In this paper, we additionally propose measuring quality in terms of migration time and mismigration time, which measure the time spent migrating at-risk disks, and the time spent mismigrating healthy disks caused by false alarms, respectively. To demonstrate these metrics' usefulness, we use them to compare disk-failure prediction methods: we compare: 1) a classification tree (CT) model against a state-of-the-art recurrent neural network (RNN) model and 2) a gradient-boosted regression tree (GBRT) model (which predicts residual life) against RNN. We observe that while RNN performs best in the prediction accuracy experiments, the CT and GBRT models sometimes outperform RNN in the resource-dependent migrationrate experiments. We conclude that prediction accuracy is sometimes misleading: correct predictions do not necessarily imply protected data. We additionally present an improved GBRT model (GBRT+), which offers a practical improvement in disk residual-life prediction accordingly to the newly proposed metrics.",
        "authors": "Gang Wang, Jianli Ding, Jing Li, Rebecca J. Stones, Xiaoguang Liu, Zhongwei Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884004",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The acquisition of aeroacoustics signals of wind turbines is of great significance in environmental noise assessment and fault monitoring of blades. The single acoustic sensor is simpler and more flexible than the acoustic sensors array but it lacks spatial analysis capability of the acoustic pressure field, and it is difficult to get pure aeroacoustics signals directly. This paper proposes a single channel blind source separation (SCBSS) method based on variational mode decomposition (VMD) which is applied to the separation of the wind turbine aeroacoustics signals acquired by the single acoustic sensors. The variational mode decomposition of the nonlinear and nonstationary signals based on the data itself completely is adaptive. In addition, the problem of mode mixing and “endpoint effect” has been improved. A novel approach combined correlation criterion with an overall index of orthogonality criterion is proposed in this paper to determine the optimal number of decomposition layers of VMD. We transform single channel underdetermined blind source separation to the non-underdetermined problem by establishing virtual multi-channel signals of the observation signals base on VMD, and separate the signals by joint approximate diagonalization of eigen-matrices (JADE) of fourth-order cumulant matrices. The method proposed in this paper has an excellent separation performance for wind turbine aeroacoustics signals, and the analysis of simulation signals indicates it has a 92.23% average recognition proportion, which is better than BSS based on EMD and EEMD, and the method has an extremely shorter computing time than EEMDBSS. The analysis of actual signals shows that the suggested method is adaptive and robust for noise.",
        "abstract": "The acquisition of aeroacoustics signals of wind turbines is of great significance in environmental noise assessment and fault monitoring of blades. The single acoustic sensor is simpler and more flexible than the acoustic sensors array but it lacks spatial analysis capability of the acoustic pressure field, and it is difficult to get pure aeroacoustics signals directly. This paper proposes a single channel blind source separation (SCBSS) method based on variational mode decomposition (VMD) which is applied to the separation of the wind turbine aeroacoustics signals acquired by the single acoustic sensors. The variational mode decomposition of the nonlinear and nonstationary signals based on the data itself completely is adaptive. In addition, the problem of mode mixing and “endpoint effect” has been improved. A novel approach combined correlation criterion with an overall index of orthogonality criterion is proposed in this paper to determine the optimal number of decomposition layers of VMD. We transform single channel underdetermined blind source separation to the non-underdetermined problem by establishing virtual multi-channel signals of the observation signals base on VMD, and separate the signals by joint approximate diagonalization of eigen-matrices (JADE) of fourth-order cumulant matrices. The method proposed in this paper has an excellent separation performance for wind turbine aeroacoustics signals, and the analysis of simulation signals indicates it has a 92.23% average recognition proportion, which is better than BSS based on EMD and EEMD, and the method has an extremely shorter computing time than EEMDBSS. The analysis of actual signals shows that the suggested method is adaptive and robust for noise.",
        "authors": "Lin Zhou, Shengbo Qi, Ya’nan Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884035",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper presents a potential game-based method for the non-myopic planning of mobile sensor networks in the context of target tracking. The planning objective is to select the sequence of sensing points over more than one future time step to maximize information about the target states. This multistep lookahead scheme aims to overcome getting trapped at local information maximum when there are gaps in the sensing coverage due to constraints of the sensor platform mobility or limitations in sensing capabilities. However, long-term planning becomes computationally intractable as the length of planning horizon increases. This paper develops a game-theoretic approach to address the computational challenges. The main contributions of this paper are twofold: 1) to formulate a non-myopic planning problem for tracking multiple targets in a potential game, the size of which increases linearly as the number of planning steps and 2) to design a learning algorithm exploiting the joint strategy fictitious play and dynamic programming, which overcomes the gaps in sensing coverage. The numerical examples of multi-target tracking demonstrate that the proposed method gives a better estimation performance than myopic planning and is computationally tractable.",
        "abstract": "This paper presents a potential game-based method for the non-myopic planning of mobile sensor networks in the context of target tracking. The planning objective is to select the sequence of sensing points over more than one future time step to maximize information about the target states. This multistep lookahead scheme aims to overcome getting trapped at local information maximum when there are gaps in the sensing coverage due to constraints of the sensor platform mobility or limitations in sensing capabilities. However, long-term planning becomes computationally intractable as the length of planning horizon increases. This paper develops a game-theoretic approach to address the computational challenges. The main contributions of this paper are twofold: 1) to formulate a non-myopic planning problem for tracking multiple targets in a potential game, the size of which increases linearly as the number of planning steps and 2) to design a learning algorithm exploiting the joint strategy fictitious play and dynamic programming, which overcomes the gaps in sensing coverage. The numerical examples of multi-target tracking demonstrate that the proposed method gives a better estimation performance than myopic planning and is computationally tractable.",
        "authors": "Han-Lim Choi, Soon-Seo Park, Su-Jin Lee",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885027",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The main task of cross-modal analysis is to learn discriminative representation shared across different modalities. In order to pursue aligned representation, conventional approaches tend to construct and optimize a linear projection or train a complex architecture of deep layers, yet it is difficult to compromise between accuracy and efficiency on modeling multimodal data. This paper proposes a novel graph-embedding learning framework implemented by neural networks. The learned embedding directly approximates the cross-modal aligned representation to perform cross-modal retrieval and image classification combining text information. Proposed framework extracts learned representation from a graph model and, simultaneously, trains a classifier under semi-supervised settings. For optimization, unlike previous methods based on the graph Laplacian regularization, a sampling strategy is adopted to generate training pairs to fully explore the inter-modal and intra-modal similarity relationship. Experimental results on various datasets show that the proposed framework outperforms other state-of-the-art methods on crossmodal retrieval. The framework also demonstrates convincing improvements on the new issue of image classification combining text information on Wiki dataset.",
        "abstract": "The main task of cross-modal analysis is to learn discriminative representation shared across different modalities. In order to pursue aligned representation, conventional approaches tend to construct and optimize a linear projection or train a complex architecture of deep layers, yet it is difficult to compromise between accuracy and efficiency on modeling multimodal data. This paper proposes a novel graph-embedding learning framework implemented by neural networks. The learned embedding directly approximates the cross-modal aligned representation to perform cross-modal retrieval and image classification combining text information. Proposed framework extracts learned representation from a graph model and, simultaneously, trains a classifier under semi-supervised settings. For optimization, unlike previous methods based on the graph Laplacian regularization, a sampling strategy is adopted to generate training pairs to fully explore the inter-modal and intra-modal similarity relationship. Experimental results on various datasets show that the proposed framework outperforms other state-of-the-art methods on crossmodal retrieval. The framework also demonstrates convincing improvements on the new issue of image classification combining text information on Wiki dataset.",
        "authors": "Jiayan Cao, Xiaodong Gu, Youcai Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2881997",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The performance of most face recognition systems (FRSs) in unconstrained environments is widely noted to be sub-optimal. One reason for this poor performance may be the lack of highly effective image pre-processing approaches, which are typically required before the feature extraction and classification stages. Furthermore, it is noted that only minimal face recognition issues are typically considered in most FRSs, thus limiting the wide applicability of most FRSs in real-life scenarios. Therefore, it is envisaged that installing more effective pre-processing techniques, in addition to selecting the right features for classification, will significantly improve the performance of FRSs. Hence, in this paper, we propose an FRS, which comprises an effective image enhancement technique for face image pre-processing, alongside a new set of hybrid features. Our image enhancement technique adopts the use of a metaheuristic optimization algorithm for effective face image enhancement, irrespective of the conditions in the unconstrained environment. This results in adding more features to the face image so that there is an increase in recognition performance as compared with the original image. The new hybrid feature is introduced in our FRS to improve the classification performance of the state-of-the-art convolutional neural network architectures. Experiments on standard face databases have been carried out to confirm the improvement in the performance of the face recognition system that considers all the constraints in the face database.",
        "abstract": "The performance of most face recognition systems (FRSs) in unconstrained environments is widely noted to be sub-optimal. One reason for this poor performance may be the lack of highly effective image pre-processing approaches, which are typically required before the feature extraction and classification stages. Furthermore, it is noted that only minimal face recognition issues are typically considered in most FRSs, thus limiting the wide applicability of most FRSs in real-life scenarios. Therefore, it is envisaged that installing more effective pre-processing techniques, in addition to selecting the right features for classification, will significantly improve the performance of FRSs. Hence, in this paper, we propose an FRS, which comprises an effective image enhancement technique for face image pre-processing, alongside a new set of hybrid features. Our image enhancement technique adopts the use of a metaheuristic optimization algorithm for effective face image enhancement, irrespective of the conditions in the unconstrained environment. This results in adding more features to the face image so that there is an increase in recognition performance as compared with the original image. The new hybrid feature is introduced in our FRS to improve the classification performance of the state-of-the-art convolutional neural network architectures. Experiments on standard face databases have been carried out to confirm the improvement in the performance of the face recognition system that considers all the constraints in the face database.",
        "authors": "Gerhard P. Hancke, Herman C. Myburgh, Muhtahir O. Oloyede",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883748",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Electromagnetically induced absorption (EIA) is a quantum phenomenon which occurs when detuned resonant laser fields interfere via atomic transition pathways. The transmission spectrum of a material experiencing EIA consists of an enhanced narrowband absorption line in between the two laser resonances. In this paper, we implement near-field interference of two microstrip radiators to produce a similar absorption mechanism. We propose a practical sensing application to detect foliage moisture by detecting the resonance shifts when sample leaves are made to perturb the near-field radiations. For the sensing, we exploit the anomalous phase signature that accompanies the EIA effect, instead of the amplitude signatures traditionally used in contemporary microwave sensors. The sensing using phase spectrum performs better than the amplitude-based sensing in harsh environments affected by noise and external interferences. Since the proposed EIA-based detector exploits multiple antenna interference in the near field, resonant sensing over distance is also possible. We demonstrate practical moisture detection using actual foliage samples with different moisture levels. We also develop a numerical dielectric model to estimate foliage moisture using full-wave electromagnetic simulations. We anticipate, from this paper, a way to produce low-cost and non-invasive microwave sensors that have reasonable sensitivity and which can be used in remote areas subjected to extreme weather environments.",
        "abstract": "Electromagnetically induced absorption (EIA) is a quantum phenomenon which occurs when detuned resonant laser fields interfere via atomic transition pathways. The transmission spectrum of a material experiencing EIA consists of an enhanced narrowband absorption line in between the two laser resonances. In this paper, we implement near-field interference of two microstrip radiators to produce a similar absorption mechanism. We propose a practical sensing application to detect foliage moisture by detecting the resonance shifts when sample leaves are made to perturb the near-field radiations. For the sensing, we exploit the anomalous phase signature that accompanies the EIA effect, instead of the amplitude signatures traditionally used in contemporary microwave sensors. The sensing using phase spectrum performs better than the amplitude-based sensing in harsh environments affected by noise and external interferences. Since the proposed EIA-based detector exploits multiple antenna interference in the near field, resonant sensing over distance is also possible. We demonstrate practical moisture detection using actual foliage samples with different moisture levels. We also develop a numerical dielectric model to estimate foliage moisture using full-wave electromagnetic simulations. We anticipate, from this paper, a way to produce low-cost and non-invasive microwave sensors that have reasonable sensitivity and which can be used in remote areas subjected to extreme weather environments.",
        "authors": "Muhammad Amin, Muhammad Omar, Nabil Bastaki, Omar F. Siddiqui, Rashad Ramzan, Taoufik Saleh Ksiksi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884224",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, we propose a novel controller design technique which is applicable to nonlinear systems. Based on the classical power shaping methodology, we establish some sufficient conditions which are able to guarantee the stability of the resulting closed-loop system. By relaxing some preliminary conditions, the particular expression of the nonlinear controller is given and the asymptotic stability of desired equilibria can be proved according to the Lyapunov theory. To motivate the application of the new proposed controller design technique and illustrate its feasibility, we verify the proposed methodology with a magnetic levitated system. Moreover, simulation results are presented to show the performance of the designed controller.",
        "abstract": "In this paper, we propose a novel controller design technique which is applicable to nonlinear systems. Based on the classical power shaping methodology, we establish some sufficient conditions which are able to guarantee the stability of the resulting closed-loop system. By relaxing some preliminary conditions, the particular expression of the nonlinear controller is given and the asymptotic stability of desired equilibria can be proved according to the Lyapunov theory. To motivate the application of the new proposed controller design technique and illustrate its feasibility, we verify the proposed methodology with a magnetic levitated system. Moreover, simulation results are presented to show the performance of the designed controller.",
        "authors": "Antai Han, Congli Mei, Jianping Cai, Jun Wan, Luwei Qi, Yunxia Luo",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885135",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Semantic segmentation has become one of the core tasks for scene understanding and many high-level works heavily rely on its performance. In the past decades, much progress has been achieved. However, some problems still need to be settled. One problem is about the challenging classification of various objects, which are with diverse viewpoints, illumination, appearance, and cluttered backgrounds, in a unified framework. The other one is focusing on the unbalanced distribution of semantic labels, where long-tail phenomenon exists and the trained model tends to be biased toward the majority classes when testing. And this problem can be regarded as the small-sample learning problem in semantic segmentation for the number of training samples upon the minority classes are small. For tackling these problems, a small-sample learning method via adversary is proposed and three contributions are claimed: 1) discriminatory modeling for semantic segmentation: two submodels are simultaneously built based on the attribute of semantic classs; 2) hierarchical contextual information consideration: both local and global contextual relationships are equally modeled under a hierarchical probabilistic graphical method and neighborhood relationship in label space are also considered; and 3) adversary learning for small-sample modeling: according to the structural relationships between small samples and the others, semantic classes are adversely modeled through computing the weighted costs. Experimental results on three benchmarks have verified the superiority of the proposed method compared with the state-of-the-arts.",
        "abstract": "Semantic segmentation has become one of the core tasks for scene understanding and many high-level works heavily rely on its performance. In the past decades, much progress has been achieved. However, some problems still need to be settled. One problem is about the challenging classification of various objects, which are with diverse viewpoints, illumination, appearance, and cluttered backgrounds, in a unified framework. The other one is focusing on the unbalanced distribution of semantic labels, where long-tail phenomenon exists and the trained model tends to be biased toward the majority classes when testing. And this problem can be regarded as the small-sample learning problem in semantic segmentation for the number of training samples upon the minority classes are small. For tackling these problems, a small-sample learning method via adversary is proposed and three contributions are claimed: 1) discriminatory modeling for semantic segmentation: two submodels are simultaneously built based on the attribute of semantic classs; 2) hierarchical contextual information consideration: both local and global contextual relationships are equally modeled under a hierarchical probabilistic graphical method and neighborhood relationship in label space are also considered; and 3) adversary learning for small-sample modeling: according to the structural relationships between small samples and the others, semantic classes are adversely modeled through computing the weighted costs. Experimental results on three benchmarks have verified the superiority of the proposed method compared with the state-of-the-arts.",
        "authors": "Qi Wang, Yuan Yuan, Zhiyu Jiang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884502",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Real datasets are often distributed nonlinearly. Although many least squares support vector machine (LS-SVM) methods have successfully modeled this kind of data using a divide-and-conquer strategy, they are often ineffective when nonlinear data are subject to noise due to a lack of robustness within each sub-model. In this paper, a robust clustered LS-SVM is proposed to model this type of data. First, the clustering method is used to divide the sample data into several sub-datasets. A local robust LS-SVM model is then developed to capture the local dynamics of the corresponding sub-dataset and to be robust to noise. Subsequently, a global regularization is constructed to intelligently coordinate all local models. These new features ensure that the global model is smooth and continuous and has a good generalization and robustness. Through the use of both artificial and real cases, the effectiveness of the proposed robust clustered LS-SVM is demonstrated.",
        "abstract": "Real datasets are often distributed nonlinearly. Although many least squares support vector machine (LS-SVM) methods have successfully modeled this kind of data using a divide-and-conquer strategy, they are often ineffective when nonlinear data are subject to noise due to a lack of robustness within each sub-model. In this paper, a robust clustered LS-SVM is proposed to model this type of data. First, the clustering method is used to divide the sample data into several sub-datasets. A local robust LS-SVM model is then developed to capture the local dynamics of the corresponding sub-dataset and to be robust to noise. Subsequently, a global regularization is constructed to intelligently coordinate all local models. These new features ensure that the global model is smooth and continuous and has a good generalization and robustness. Through the use of both artificial and real cases, the effectiveness of the proposed robust clustered LS-SVM is demonstrated.",
        "authors": "Bin Fan, Te-Te Hu, Xin-Jiang Lu, Yi Zhang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883433",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, a deep belief network (DBN) is proposed as an effective method to predict the mortality of patients within the first 72 h of visiting an emergency room (ER), i.e., 72-h mortality. Previously, physicians used the quick sequential organ failure assessment (qSOFA) and systemic inflammatory response syndrome (SIRS) to determine the survival probability of patients. Although such prediction methods are convenient, there is room for improvement in their accuracy. To demonstrate the effectiveness of our proposed method, original data from a hospital was used. A single-center retrospective study was performed regarding adult ER patients who were admitted between January 2007 and December 2013 with an infection. The qSOFA and SIRS scores were calculated using primary vital signs and laboratory data. A DBN was then used to predict patients’ survival rates and to choose 65 clinical variables as an input. Utilizing the DBN, the joint probability distribution of the 65 clinical variables was calculated, and valid solutions of the decreased features were achieved. Results indicate that the DBN can predict the 72-h mortality of ER septic patients more accurately than the qSOFA or SIRS. This paper aims to design an effective prediction system to assist clinicians in their diagnosis using a DBN for early risk stratification and intervention.",
        "abstract": "In this paper, a deep belief network (DBN) is proposed as an effective method to predict the mortality of patients within the first 72 h of visiting an emergency room (ER), i.e., 72-h mortality. Previously, physicians used the quick sequential organ failure assessment (qSOFA) and systemic inflammatory response syndrome (SIRS) to determine the survival probability of patients. Although such prediction methods are convenient, there is room for improvement in their accuracy. To demonstrate the effectiveness of our proposed method, original data from a hospital was used. A single-center retrospective study was performed regarding adult ER patients who were admitted between January 2007 and December 2013 with an infection. The qSOFA and SIRS scores were calculated using primary vital signs and laboratory data. A DBN was then used to predict patients’ survival rates and to choose 65 clinical variables as an input. Utilizing the DBN, the joint probability distribution of the 65 clinical variables was calculated, and valid solutions of the decreased features were achieved. Results indicate that the DBN can predict the 72-h mortality of ER septic patients more accurately than the qSOFA or SIRS. This paper aims to design an effective prediction system to assist clinicians in their diagnosis using a DBN for early risk stratification and intervention.",
        "authors": "Chih-Min Su, I-Hsi Kao, Jau-Woei Perng, Shih-Chiang Hung, Yen-Wei Chen, Yi-Horng Lai",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884509",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The success of designing software intensive systems (SISs) may be improved by incorporating experimentation to be part of the design process. This paper presents a scientific approach to experimentation on objects that are units of designers' behavior and is aimed at solving project tasks in conceptual design. The proposed approach is based on specifying the behavior units as precedents and pseudo-code programming of experiments' plans. Reasoning used by designers in the experiments is registered in a question-answer form. Experimenting is supported by a specialized toolkit.",
        "abstract": "The success of designing software intensive systems (SISs) may be improved by incorporating experimentation to be part of the design process. This paper presents a scientific approach to experimentation on objects that are units of designers' behavior and is aimed at solving project tasks in conceptual design. The proposed approach is based on specifying the behavior units as precedents and pseudo-code programming of experiments' plans. Reasoning used by designers in the experiments is registered in a question-answer form. Experimenting is supported by a specialized toolkit.",
        "authors": "Petr Sosnin",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2274896",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Most code clone detection approaches identify clones via static source code analysis. Such approaches are effective and efficient in detecting lexically similar clones. However, they are less effective in detecting semantic clones that are similar in functionality but different in implementation. As an initial try to detect semantic clones, in this paper, we propose a test-based approach to detecting methods that are semantically equivalent to API methods. For a given method m, we generate its test cases automatically and search for semantically equivalent API methods by running the generated test cases. If two methods generate the same output on each of the test cases, they are taken as semantically equivalent methods. One of the weakness of test-based clone detection is that it is often time consuming. To reduce the time complexity, we take the following measures. First, we focus on methods instead of arbitrary fragments. Second, for a given method, we only compare it against such API methods whose signatures are highly similar to that of the given method. We evaluate the proposed approach on 10 well-known applications. Evaluation results suggest that it is efficient and accurate, and its precision is up to 98%.",
        "abstract": "Most code clone detection approaches identify clones via static source code analysis. Such approaches are effective and efficient in detecting lexically similar clones. However, they are less effective in detecting semantic clones that are similar in functionality but different in implementation. As an initial try to detect semantic clones, in this paper, we propose a test-based approach to detecting methods that are semantically equivalent to API methods. For a given method m, we generate its test cases automatically and search for semantically equivalent API methods by running the generated test cases. If two methods generate the same output on each of the test cases, they are taken as semantically equivalent methods. One of the weakness of test-based clone detection is that it is often time consuming. To reduce the time complexity, we take the following measures. First, we focus on methods instead of arbitrary fragments. Second, for a given method, we only compare it against such API methods whose signatures are highly similar to that of the given method. We evaluate the proposed approach on 10 well-known applications. Evaluation results suggest that it is efficient and accurate, and its precision is up to 98%.",
        "authors": "Guangjie Li, Hui Liu, Jiahao Jin, Yanjie Jiang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883699",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Low-dose computed tomography (LDCT) plays a critical role in the early detection of lung cancer. Despite the life-saving benefit of early detection by LDCT, there are many limitations of this imaging modality including high rates of detection of indeterminate pulmonary nodules. Radiomics is the process of extracting and analyzing image-based, quantitative features from a region-of-interest which then can be analyzed to develop decision support tools that can improve lung cancer screening. Although prior published research has shown that delta radiomics (i.e., changes in features over time) have utility in predicting treatment response, limited work has been conducted using delta radiomics in lung cancer screening. As such, we conducted analyses to assess the performance of incorporating delta with conventional (non delta) features using machine learning to predict lung nodule malignancy. We found the best improved area under the receiver operating characteristic curve (AUC) was 0.822 when delta features were combined with conventional features versus an AUC 0.773 for conventional features only. Overall, this paper demonstrates the important utility of combining delta radiomics features with conventional radiomics features to improve performance of models in the lung cancer screening setting.",
        "abstract": "Low-dose computed tomography (LDCT) plays a critical role in the early detection of lung cancer. Despite the life-saving benefit of early detection by LDCT, there are many limitations of this imaging modality including high rates of detection of indeterminate pulmonary nodules. Radiomics is the process of extracting and analyzing image-based, quantitative features from a region-of-interest which then can be analyzed to develop decision support tools that can improve lung cancer screening. Although prior published research has shown that delta radiomics (i.e., changes in features over time) have utility in predicting treatment response, limited work has been conducted using delta radiomics in lung cancer screening. As such, we conducted analyses to assess the performance of incorporating delta with conventional (non delta) features using machine learning to predict lung nodule malignancy. We found the best improved area under the receiver operating characteristic curve (AUC) was 0.822 when delta features were combined with conventional features versus an AUC 0.773 for conventional features only. Overall, this paper demonstrates the important utility of combining delta radiomics features with conventional radiomics features to improve performance of models in the lung cancer screening setting.",
        "authors": "Dmitry B. Goldgof, Dmitry Cherezov, Lawrence O. Hall, Matthew B. Schabath, Robert J. Gillies, Saeed S. Alahmari",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884126",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Recognizing or retrieve a face based on its sketch-photo similarity has important applications in law enforcement and public security. While many existing methods focus on recognizing facial sketch from image-based queries, this setting has the major limitations in practice, since the abstract sketch only captures sparse global structure of the human face. To address this issue, in this paper, we propose to bridge the gap between the sketch-photo pair by “translating”the abstract visual sketch into a photorealistic face with the help of descriptive attributes. Specifically, we propose an improved multi-modal conditional generative adversarial network (MMC-GAN) to jointly utilize the complementary information of visual sketches and semantic facial attributes to reduce the uncertainties of the facial image generation. A fusion network is introduced to better leverage the information from different modalities (visual sketch and semantic attributes). In order to improve the details of the generated facial images, we adopt a two-path generator structure in which the global feature and the local feature of human faces are learned in parallel. An identitypreserving constraint is further introduced to enhance the identity consistency between the sketches and facial images. Extensive experiments demonstrate that we can effectively manipulate the face image generation by varying the input facial attributes. Besides, the generated photorealistic face image is validated to improve the sketch-photo face recognition and retrieval.",
        "abstract": "Recognizing or retrieve a face based on its sketch-photo similarity has important applications in law enforcement and public security. While many existing methods focus on recognizing facial sketch from image-based queries, this setting has the major limitations in practice, since the abstract sketch only captures sparse global structure of the human face. To address this issue, in this paper, we propose to bridge the gap between the sketch-photo pair by “translating”the abstract visual sketch into a photorealistic face with the help of descriptive attributes. Specifically, we propose an improved multi-modal conditional generative adversarial network (MMC-GAN) to jointly utilize the complementary information of visual sketches and semantic facial attributes to reduce the uncertainties of the facial image generation. A fusion network is introduced to better leverage the information from different modalities (visual sketch and semantic attributes). In order to improve the details of the generated facial images, we adopt a two-path generator structure in which the global feature and the local feature of human faces are learned in parallel. An identitypreserving constraint is further introduced to enhance the identity consistency between the sketches and facial images. Extensive experiments demonstrate that we can effectively manipulate the face image generation by varying the input facial attributes. Besides, the generated photorealistic face image is validated to improve the sketch-photo face recognition and retrieval.",
        "authors": "Hang Su, Qin Zhou, Shibao Zheng, Xiao Yang, Xinzhe Li",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883463",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Next-app prediction is the task of predicting the next app that a user will choose to use on the smartphone. It helps to establish a variety of intelligent personalized services, such as fast-launch UI app, intelligent user-phone interactions, and so on. Since app names only provide limited semantic information, the intrinsic relation among apps cannot be fully exploited. Meanwhile, next-app to be used is largely determined by a sequence of apps that a user used recently. To address these challenging problems, this paper first enriches the semantic information of apps by extracting descriptive text of each app from the app store and thus proposes a topic model to transform apps as well as user preferences into latent vectors. Then, a set of nearest neighbors can be constructed based on the similarity of latent vectors and it is employed for training the prediction model. Furthermore, our prediction scheme is built on the temporal sequential data and is modeled by using the chain-augmented Naive Bayes model. Experimental results with a real smartphone application log data have demonstrated that our method achieves higher recall and DCG values compared with several baseline next-app prediction methods.",
        "abstract": "Next-app prediction is the task of predicting the next app that a user will choose to use on the smartphone. It helps to establish a variety of intelligent personalized services, such as fast-launch UI app, intelligent user-phone interactions, and so on. Since app names only provide limited semantic information, the intrinsic relation among apps cannot be fully exploited. Meanwhile, next-app to be used is largely determined by a sequence of apps that a user used recently. To address these challenging problems, this paper first enriches the semantic information of apps by extracting descriptive text of each app from the app store and thus proposes a topic model to transform apps as well as user preferences into latent vectors. Then, a set of nearest neighbors can be constructed based on the similarity of latent vectors and it is employed for training the prediction model. Furthermore, our prediction scheme is built on the temporal sequential data and is modeled by using the chain-augmented Naive Bayes model. Experimental results with a real smartphone application log data have demonstrated that our method achieves higher recall and DCG values compared with several baseline next-app prediction methods.",
        "authors": "Changjian Fang, Dejun Mu, Youquan Wang, Zhiang Wu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883377",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "A new numerical technique to solve nonlinear systems of initial value problems for nonlinear first-order differential equations (ODEs) that model genetic networks in systems biology is developed. This technique is based on finding local Galerkin approximations on each sub-interval at a given time grid of points using piecewise hat functions. Comparing the numerical solution of the new method for a single nonlinear ODE with an exact solution shows that this method gives accurate solutions with relative error1.88×10−11for a time step1×10−6. This new method is compared with the adaptive Runge Kutta (ARK) method for solving systems of ODEs, and the results are comparable for a time step2×10−4. It is shown that the relative error of the Galerkin method decreases approximately linearly with the log of the number of hat functions used. Unlike the ARK method, this new method has the potential to be parallelizable and to be useful for solving biological problems involving large genetic networks. An NSF commissioned video illustrating how systems biology helps us understand that a fundamental process in cells is included.",
        "abstract": "A new numerical technique to solve nonlinear systems of initial value problems for nonlinear first-order differential equations (ODEs) that model genetic networks in systems biology is developed. This technique is based on finding local Galerkin approximations on each sub-interval at a given time grid of points using piecewise hat functions. Comparing the numerical solution of the new method for a single nonlinear ODE with an exact solution shows that this method gives accurate solutions with relative error1.88×10−11for a time step1×10−6. This new method is compared with the adaptive Runge Kutta (ARK) method for solving systems of ODEs, and the results are comparable for a time step2×10−4. It is shown that the relative error of the Galerkin method decreases approximately linearly with the log of the number of hat functions used. Unlike the ARK method, this new method has the potential to be parallelizable and to be useful for solving biological problems involving large genetic networks. An NSF commissioned video illustrating how systems biology helps us understand that a fundamental process in cells is included.",
        "authors": "Ahmad Al-Omari, Heinz-Bernd Schüttler, Jonathan Arnold, Thiab Taha",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2013",
        "doi": "https://doi.org/10.1109/ACCESS.2013.2269192",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Traditional surveillance systems for observing water levels are often complex, costly, and time-consuming. In this paper, we developed a low-cost unmanned surveillance system consisting of remote measuring stations and a monitoring center. The system uses a map-based Web service, as well as video cameras, water level analyzers, and wireless communication routers necessary to display real-time water level measurements of rivers and reservoirs on a Web platform. With the aid of a wireless communication router, the water level information is transmitted to a server connected to the Internet via a cellular network. By combining complex water level information of different river basins, the proposed system can be used to forecast and prevent flood disasters. In order to evaluate the proposed system, we conduct experiments using three feasible methods, including the difference method, dictionary learning, and deep learning. The experimental results show that the deep learning-based method performs best in terms of accuracy and stability.",
        "abstract": "Traditional surveillance systems for observing water levels are often complex, costly, and time-consuming. In this paper, we developed a low-cost unmanned surveillance system consisting of remote measuring stations and a monitoring center. The system uses a map-based Web service, as well as video cameras, water level analyzers, and wireless communication routers necessary to display real-time water level measurements of rivers and reservoirs on a Web platform. With the aid of a wireless communication router, the water level information is transmitted to a server connected to the Internet via a cellular network. By combining complex water level information of different river basins, the proposed system can be used to forecast and prevent flood disasters. In order to evaluate the proposed system, we conduct experiments using three feasible methods, including the difference method, dictionary learning, and deep learning. The experimental results show that the deep learning-based method performs best in terms of accuracy and stability.",
        "authors": "Guan Gui, Hikmet Sari, Jian Xiong, Jinqiu Pan, Wang Luo, Yue Yin",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883702",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The latest high efficiency video coding (HEVC) standard introduces a large number of inter-mode block partitioning modes. The HEVC reference test model (HM) uses partially exhaustive tree-structured mode selection, which still explores a large number of prediction unit (PU) modes for a coding unit (CU). This impacts on encoding time rise which deprives a number of electronic devices having limited processing resources to use various features of HEVC. By analyzing the homogeneity, residual, and different statistical correlation among modes, many researchers speed-up the encoding process through the number of PU mode reduction. However, these approaches could not demonstrate the similar rate-distortion (RD) performance with the HM due to their dependency on existing Lagrangian cost function (LCF) within the HEVC framework. In this paper, to avoid the complete dependency on LCF in the initial phase, we exploit visual sensitive foreground motion and spatial salient metric (FMSSM) in a block. To capture its motion and saliency features, we use the dynamic background and visual saliency modeling, respectively. According to the FMSSM values, a subset of PU modes is then explored for encoding the CU. This preprocessing phase is independent from the existing LCF. As the proposed coding technique further reduces the number of PU modes using two simple criteria (i.e., motion and saliency), it outperforms the HM in terms of encoding time reduction. As it also encodes the uncovered and static background areas using the dynamic background frame as a substituted reference frame, it does not sacrifice quality. Tested results reveal that the proposed method achieves 32% average encoding time reduction of the HM without any quality loss for a wide range of videos.",
        "abstract": "The latest high efficiency video coding (HEVC) standard introduces a large number of inter-mode block partitioning modes. The HEVC reference test model (HM) uses partially exhaustive tree-structured mode selection, which still explores a large number of prediction unit (PU) modes for a coding unit (CU). This impacts on encoding time rise which deprives a number of electronic devices having limited processing resources to use various features of HEVC. By analyzing the homogeneity, residual, and different statistical correlation among modes, many researchers speed-up the encoding process through the number of PU mode reduction. However, these approaches could not demonstrate the similar rate-distortion (RD) performance with the HM due to their dependency on existing Lagrangian cost function (LCF) within the HEVC framework. In this paper, to avoid the complete dependency on LCF in the initial phase, we exploit visual sensitive foreground motion and spatial salient metric (FMSSM) in a block. To capture its motion and saliency features, we use the dynamic background and visual saliency modeling, respectively. According to the FMSSM values, a subset of PU modes is then explored for encoding the CU. This preprocessing phase is independent from the existing LCF. As the proposed coding technique further reduces the number of PU modes using two simple criteria (i.e., motion and saliency), it outperforms the HM in terms of encoding time reduction. As it also encodes the uncovered and static background areas using the dynamic background frame as a substituted reference frame, it does not sacrifice quality. Tested results reveal that the proposed method achieves 32% average encoding time reduction of the HM without any quality loss for a wide range of videos.",
        "authors": "Manoranjan Paul, Manzur Murshed, Pallab Kanti Podder",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883967",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Person re-identification (ReID), which aims at matching individuals across non-overlapping cameras, has attracted much attention in the field of computer vision due to its research significance and potential applications. Triplet loss-based CNN models have been very successful for person ReID, which aims to optimize the feature embedding space such that the distances between samples with the same identity are much shorter than those of samples with different identities. Researchers have found that hard triplets' mining is crucial for the success of the triplet loss. In this paper, motivated by focal loss designed for the classification model, we propose the triplet focal loss for person ReID. Triplet focal loss can up-weight the hard triplets' training samples and relatively down-weight the easy triplets adaptively via simply projecting the original distance in the Euclidean space to an exponential kernel space. We conduct experiments on three largest benchmark datasets currently available for person ReID, namely, Market-1501, DukeMTMCReID, and CUHK03, and the experimental results verify that the proposed triplet focal loss can greatly outperform the traditional triplet loss and achieve competitive performances with the representative state-of-the-art methods.",
        "abstract": "Person re-identification (ReID), which aims at matching individuals across non-overlapping cameras, has attracted much attention in the field of computer vision due to its research significance and potential applications. Triplet loss-based CNN models have been very successful for person ReID, which aims to optimize the feature embedding space such that the distances between samples with the same identity are much shorter than those of samples with different identities. Researchers have found that hard triplets' mining is crucial for the success of the triplet loss. In this paper, motivated by focal loss designed for the classification model, we propose the triplet focal loss for person ReID. Triplet focal loss can up-weight the hard triplets' training samples and relatively down-weight the easy triplets adaptively via simply projecting the original distance in the Euclidean space to an exponential kernel space. We conduct experiments on three largest benchmark datasets currently available for person ReID, namely, Market-1501, DukeMTMCReID, and CUHK03, and the experimental results verify that the proposed triplet focal loss can greatly outperform the traditional triplet loss and achieve competitive performances with the representative state-of-the-art methods.",
        "authors": "Qi Zhang, Shizhou Zhang, Xing Wei, Yanning Zhang, Yong Xia",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884743",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper discusses synchronization of time-varying delayed neural networks by fixed-time control. First, several new fixed-time stability theorems of the dynamic system are discussed, and the estimation of the convergence time is also gained. Compared with some existing results, the convergence time given in this paper can be less conservative and more accurate. Second, as one of the important applications of fixed-time stability, several novel sufficient criteria are derived such that the two time-varying neural networks can be synchronized within a fixed-time. Finally, the simulation result is presented to show the effectiveness of the theoretical result.",
        "abstract": "This paper discusses synchronization of time-varying delayed neural networks by fixed-time control. First, several new fixed-time stability theorems of the dynamic system are discussed, and the estimation of the convergence time is also gained. Compared with some existing results, the convergence time given in this paper can be less conservative and more accurate. Second, as one of the important applications of fixed-time stability, several novel sufficient criteria are derived such that the two time-varying neural networks can be synchronized within a fixed-time. Finally, the simulation result is presented to show the effectiveness of the theoretical result.",
        "authors": "Chao Xu, Xiaoqun Wu, Yuhua Xu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883417",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Automatic inference of task level parallelism is fundamental for ensuring many kinds of safety and liveness properties of parallel applications. For example, two tasks running in parallel may be involved in data races when they have conflicting memory accesses, or one is affecting the termination of another by updating shared variables. In this paper, we have considered a task-graph-based actor model, used in signal processing applications (e.g., baseband processing in wireless communication, LTE uplink processing) that are deployed on many-core platforms, in which the actors, task-graphs, and tasks are the active entities running in parallel. The actors invoke task graphs, which in turn invoke tasks, and they communicate through message passing, thus creating different kinds of dependencies and parallelism in the application. We introduce a novel May Happen in Parallel (MHP) analysis for complex parallel applications based on our computational model. The MHP analysis consists of (i) data-flow analysis applicable to parallel control-flow structures inferring MHP facts representing pairs of tasks running in parallel, (ii) identification of all direct and indirect communication by generating a context-free grammar and enumerating valid strings representing parallelism and dependencies among active entities, and (iii) inferring MHP facts when multiple task-graphs communicate. Our analysis is applicable to other computational models (e.g. Cilk or X10) too. We have fully implemented our analysis and evaluated it on signal processing applications consisting of a maximum of 36.57 million lines of code representing 232 different tasks. The analysis took approximately 7 minutes to identify all communication information and 10.5 minutes to identify 12052 executable parallel task-pairs (to analyse for concurrency bugs) proving that our analysis is scalable for industrial-sized code-bases.",
        "abstract": "Automatic inference of task level parallelism is fundamental for ensuring many kinds of safety and liveness properties of parallel applications. For example, two tasks running in parallel may be involved in data races when they have conflicting memory accesses, or one is affecting the termination of another by updating shared variables. In this paper, we have considered a task-graph-based actor model, used in signal processing applications (e.g., baseband processing in wireless communication, LTE uplink processing) that are deployed on many-core platforms, in which the actors, task-graphs, and tasks are the active entities running in parallel. The actors invoke task graphs, which in turn invoke tasks, and they communicate through message passing, thus creating different kinds of dependencies and parallelism in the application. We introduce a novel May Happen in Parallel (MHP) analysis for complex parallel applications based on our computational model. The MHP analysis consists of (i) data-flow analysis applicable to parallel control-flow structures inferring MHP facts representing pairs of tasks running in parallel, (ii) identification of all direct and indirect communication by generating a context-free grammar and enumerating valid strings representing parallelism and dependencies among active entities, and (iii) inferring MHP facts when multiple task-graphs communicate. Our analysis is applicable to other computational models (e.g. Cilk or X10) too. We have fully implemented our analysis and evaluated it on signal processing applications consisting of a maximum of 36.57 million lines of code representing 232 different tasks. The analysis took approximately 7 minutes to identify all communication information and 10.5 minutes to identify 12052 executable parallel task-pairs (to analyse for concurrency bugs) proving that our analysis is scalable for industrial-sized code-bases.",
        "authors": "Abu Naser Masud, Björn Lisper, Federico Ciccozzi",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885705",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In this paper, the preparation and the measurement of a set of human breast phantoms for microwave breast imaging (MBI) as a method of tumor detection are presented. The developed artificial breast phantoms have realistic dielectric properties. Homogenous and most realistic heterogeneous breast phantoms have been fabricated based upon 3D structures filled up with different chemical mixtures that imitate the various breast tissue types (skin, healthy fat tissue, glandular tissue, and tumor tissue) regarding permittivity over ultra-wideband frequency band (3.1-10.6 GHz). The primary challenge in fabricating such phantoms is in developing suitable mixtures of materials to emulate those properties across the frequency band of interest in hyperthermia and to fabricate the phantom with realistic anatomy. Once fabricated, the dielectric properties are measured using a dielectric probe connected with a modern vector network analyzer. The measured dielectric is compared to real human breast dielectric properties, and the primary imaging results are presented. The integrated design of the homogenous and heterogeneous phantoms permits to combine the tumor and breast phantoms dynamically for creating a test platform based on MBI systems. The experimental dielectric properties of the phantoms show good agreement with this paper and theoretical results. The phantoms are constructed in such a way that the chosen materials demonstrate the properties to be stable over a long period. The experimental results demonstrate the validity of our proposed phantoms to be used in investigating as a supplement to the real human breast tissue with multiple object and comparatively high-resolution image.",
        "abstract": "In this paper, the preparation and the measurement of a set of human breast phantoms for microwave breast imaging (MBI) as a method of tumor detection are presented. The developed artificial breast phantoms have realistic dielectric properties. Homogenous and most realistic heterogeneous breast phantoms have been fabricated based upon 3D structures filled up with different chemical mixtures that imitate the various breast tissue types (skin, healthy fat tissue, glandular tissue, and tumor tissue) regarding permittivity over ultra-wideband frequency band (3.1-10.6 GHz). The primary challenge in fabricating such phantoms is in developing suitable mixtures of materials to emulate those properties across the frequency band of interest in hyperthermia and to fabricate the phantom with realistic anatomy. Once fabricated, the dielectric properties are measured using a dielectric probe connected with a modern vector network analyzer. The measured dielectric is compared to real human breast dielectric properties, and the primary imaging results are presented. The integrated design of the homogenous and heterogeneous phantoms permits to combine the tumor and breast phantoms dynamically for creating a test platform based on MBI systems. The experimental dielectric properties of the phantoms show good agreement with this paper and theoretical results. The phantoms are constructed in such a way that the chosen materials demonstrate the properties to be stable over a long period. The experimental results demonstrate the validity of our proposed phantoms to be used in investigating as a supplement to the real human breast tissue with multiple object and comparatively high-resolution image.",
        "authors": "Md Samsuzzaman, Md Tarikul Islam, Mohammad Tariqul Islam, Salehin Kibria",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885087",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "This paper proposes a new two degree-of-freedom control structure such that the robustness control of a belt-driven servomechanism is adequately addressed. In conventional design approaches, a trade-off between robust stability and robust tracking performance is unavoidable because the control engineer must take different frequency regions into consideration. In this paper, a frequency-dependent switching control structure is proposed, where the feedback connection to the external-loop controller is dynamically switched between the outputs of the controlled plant and its nominal model. The disturbance attenuation at lower frequencies by the double-loop control, as well as the robust stability at higher frequencies through the reference feedforward control, can be achieved at one fixed two degree-of-freedom control structure. The feasibility of the proposed approach is verified by theoretical analysis and experimental results.",
        "abstract": "This paper proposes a new two degree-of-freedom control structure such that the robustness control of a belt-driven servomechanism is adequately addressed. In conventional design approaches, a trade-off between robust stability and robust tracking performance is unavoidable because the control engineer must take different frequency regions into consideration. In this paper, a frequency-dependent switching control structure is proposed, where the feedback connection to the external-loop controller is dynamically switched between the outputs of the controlled plant and its nominal model. The disturbance attenuation at lower frequencies by the double-loop control, as well as the robust stability at higher frequencies through the reference feedforward control, can be achieved at one fixed two degree-of-freedom control structure. The feasibility of the proposed approach is verified by theoretical analysis and experimental results.",
        "authors": "Chun-Lin Chen, Mi-Ching Tsai",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884218",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "To guarantee high-performance tracking control of cable-driven manipulators under complex lumped uncertainties, we propose a novel practical adaptive integral terminal sliding mode (AITSM) control scheme in this paper. The proposed control scheme utilizes time-delay estimation (TDE) to estimate and compensate the system dynamics and therefore ensures an attractive model-free control structure. Meanwhile, a novel adaptive algorithm is designed to timely and appropriately update the gains for the AITSM manifold and combined adaptive reaching law (ARL). High control accuracy, fast dynamical response, and strong robustness can be effectively ensured, thanks to the proposed AITSM manifold and combined ARL. The stability of the closed-loop control system is analyzed using the Lyapunov stability theory. Comparative experiments were conducted, and the corresponding results show that the newly proposed TDEbased AITSM control scheme can provide a better and comprehensive control performance than the existing AITSM control method.",
        "abstract": "To guarantee high-performance tracking control of cable-driven manipulators under complex lumped uncertainties, we propose a novel practical adaptive integral terminal sliding mode (AITSM) control scheme in this paper. The proposed control scheme utilizes time-delay estimation (TDE) to estimate and compensate the system dynamics and therefore ensures an attractive model-free control structure. Meanwhile, a novel adaptive algorithm is designed to timely and appropriately update the gains for the AITSM manifold and combined adaptive reaching law (ARL). High control accuracy, fast dynamical response, and strong robustness can be effectively ensured, thanks to the proposed AITSM manifold and combined ARL. The stability of the closed-loop control system is analyzed using the Lyapunov stability theory. Comparative experiments were conducted, and the corresponding results show that the newly proposed TDEbased AITSM control scheme can provide a better and comprehensive control performance than the existing AITSM control method.",
        "authors": "Bai Chen, Fei Yan, Hongtao Wu, Kangwu Zhu, Yaoyao Wang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2885082",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The SIP signaling performance has a vital role for the overall QoS of SIP-based VoIP applications over MANET. The SIP end-to-end performance metrics have been defined in RFC 6076 to provide a standardized method for the performance evaluation of the SIP signaling system over different platforms. However, to our best acknowledge, the benchmarked values for these metrics have not been proposed yet. Therefore, in this paper, a novel Cross-Layer performance enhancement approach is proposed, implemented, and evaluated to improve the performance of the SIP signaling system over OLSR-based MANET by applying significant dynamic modifications for the routing parameters. The SIP performance metrics seek to accurately reflect the SIP signaling state and the required actions for the routing parameters. The implementation of the Cross-Layer OLSR approach has been successful effectively in reducing the total delays in the SIP processes, enhancing the signaling performance, and increasing the utilization level in the system bandwidth and routing processes.",
        "abstract": "The SIP signaling performance has a vital role for the overall QoS of SIP-based VoIP applications over MANET. The SIP end-to-end performance metrics have been defined in RFC 6076 to provide a standardized method for the performance evaluation of the SIP signaling system over different platforms. However, to our best acknowledge, the benchmarked values for these metrics have not been proposed yet. Therefore, in this paper, a novel Cross-Layer performance enhancement approach is proposed, implemented, and evaluated to improve the performance of the SIP signaling system over OLSR-based MANET by applying significant dynamic modifications for the routing parameters. The SIP performance metrics seek to accurately reflect the SIP signaling state and the required actions for the routing parameters. The implementation of the Cross-Layer OLSR approach has been successful effectively in reducing the total delays in the SIP processes, enhancing the signaling performance, and increasing the utilization level in the system bandwidth and routing processes.",
        "authors": "Ali A. Amer, Feda AlShahwan, Mazin Alshamrani",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2880917",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "In-service mooring chains are subjected to harsh environmental conditions on a daily basis, which increases the necessity of integrity assessment of chain links. Periodic structural health monitoring of mooring chains is mandatory and vital in order to maintain the safety of floating platforms. Application of ultrasound for in-service mooring chain inspection is still in its infancy due to the lack of accessibility, in-field operational complexity, and the geometrical features of mooring systems. With the advancement of robotic/automated systems (i.e., chain climbing robotic mechanisms), interest in in situ ultrasound inspection has increased. At present, ultrasound inspection has been confined to the weld area of the chain links. However, according to recent studies on fatigue and residual stresses, ultrasound inspection of the chain crown should be further investigated. A new application of ultrasonic phased array full matrix capture is discussed in this paper for investigation of the chain crown. Due to the complex geometry (i.e., curved and limited access) of the chain crown, a surface mapping technique has been added to the presented full matrix capture technique. The inspection method presented in this paper is suitable for chain links both in air and underwater. A continuous water supply wedge was developed in order to supply couplant for inair inspection. Development of a technique that can be adapted for robotic inspection was considered, and an automated manipulator was used to carry out inspections. The design of the inspection method and the robotic manipulator is discussed in this paper. The technique is validated with laboratory experiments.",
        "abstract": "In-service mooring chains are subjected to harsh environmental conditions on a daily basis, which increases the necessity of integrity assessment of chain links. Periodic structural health monitoring of mooring chains is mandatory and vital in order to maintain the safety of floating platforms. Application of ultrasound for in-service mooring chain inspection is still in its infancy due to the lack of accessibility, in-field operational complexity, and the geometrical features of mooring systems. With the advancement of robotic/automated systems (i.e., chain climbing robotic mechanisms), interest in in situ ultrasound inspection has increased. At present, ultrasound inspection has been confined to the weld area of the chain links. However, according to recent studies on fatigue and residual stresses, ultrasound inspection of the chain crown should be further investigated. A new application of ultrasonic phased array full matrix capture is discussed in this paper for investigation of the chain crown. Due to the complex geometry (i.e., curved and limited access) of the chain crown, a surface mapping technique has been added to the presented full matrix capture technique. The inspection method presented in this paper is suitable for chain links both in air and underwater. A continuous water supply wedge was developed in order to supply couplant for inair inspection. Development of a technique that can be adapted for robotic inspection was considered, and an automated manipulator was used to carry out inspections. The design of the inspection method and the robotic manipulator is discussed in this paper. The technique is validated with laboratory experiments.",
        "authors": "David Carswell, Mahesh Dissanayake, Mark Sutcliffe, Michael Corsar, Premesh Shehan Lowe, Tariq Pervez Sattar",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883378",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "Multiple kernel clustering (MKC) based on global structure alignment (GSA) has unified many existing MKC algorithms, and shown outstanding clustering performance. However, we observe that most of existing GSA-based MKC algorithms only maximally align global structure of data with an ideal similarity matrix, while ignoring the local geometrical structure hidden in data, which is regarded to be important in improving the clustering performance. To address this issue, we propose a global and local structure alignment framework for MKC (GLSAMKC) which well considers both the alignment between the global structure and local structure of data with the same ideal similarity matrix. To illustrate the effectiveness of the proposed framework, we instantiate two specific GLSAMKC-based algorithms by exploiting the local structure with local linear embedding and locality preserving projection, respectively. A two-step alternate iterative and convergent optimization algorithm is developed to implement the resultant optimization problem. Extensive experimental results on five benchmark data sets demonstrate the superiority of proposed algorithms compared with the many state-of-the-art MKC algorithms, indicating the effectiveness of the proposed framework.",
        "abstract": "Multiple kernel clustering (MKC) based on global structure alignment (GSA) has unified many existing MKC algorithms, and shown outstanding clustering performance. However, we observe that most of existing GSA-based MKC algorithms only maximally align global structure of data with an ideal similarity matrix, while ignoring the local geometrical structure hidden in data, which is regarded to be important in improving the clustering performance. To address this issue, we propose a global and local structure alignment framework for MKC (GLSAMKC) which well considers both the alignment between the global structure and local structure of data with the same ideal similarity matrix. To illustrate the effectiveness of the proposed framework, we instantiate two specific GLSAMKC-based algorithms by exploiting the local structure with local linear embedding and locality preserving projection, respectively. A two-step alternate iterative and convergent optimization algorithm is developed to implement the resultant optimization problem. Extensive experimental results on five benchmark data sets demonstrate the superiority of proposed algorithms compared with the many state-of-the-art MKC algorithms, indicating the effectiveness of the proposed framework.",
        "authors": "Chuanli Wang, En Zhu, Jianping Yin, Long Gao, Ning Hu, Xinwang Liu",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2884441",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "We propose a novel 3-D convolution method, cv3dconv, for extracting spatiotemporal features from videos. It reduces the number of sum-of-products operations in 3-D convolution by thousands of times by assuming the constant moving velocity of the features. We observed that a specific class of video sequences, such as video captured by an in-vehicle camera, can be well approximated with piece-wise linear movements of 2-D features in a temporal dimension. Our principal finding is that a 3-D kernel, represented by constant velocity, can be decomposed into a convolution of a 2-D-shaped kernel and a 3-D-velocity kernel, which is parameterized using only two parameters. We derived an efficient recursive algorithm for this class of 3-D convolution, which is exceptionally suited for sparse spatiotemporal data, and this parameterized decomposed representation imposes a structured regularization along a temporal direction. We experimentally verified the validity of our approximation using a controlled dataset, and we also showed the effectiveness of the cv3dconv by adopting it for deep neural networks (DNNs) in visual odometry estimation task using publicly available event-based camera dataset captured in urban road scene. Our DNN architecture improves the estimation accuracy for about 30% compared with the existing states-of-the-arts architecture designed for event data.",
        "abstract": "We propose a novel 3-D convolution method, cv3dconv, for extracting spatiotemporal features from videos. It reduces the number of sum-of-products operations in 3-D convolution by thousands of times by assuming the constant moving velocity of the features. We observed that a specific class of video sequences, such as video captured by an in-vehicle camera, can be well approximated with piece-wise linear movements of 2-D features in a temporal dimension. Our principal finding is that a 3-D kernel, represented by constant velocity, can be decomposed into a convolution of a 2-D-shaped kernel and a 3-D-velocity kernel, which is parameterized using only two parameters. We derived an efficient recursive algorithm for this class of 3-D convolution, which is exceptionally suited for sparse spatiotemporal data, and this parameterized decomposed representation imposes a structured regularization along a temporal direction. We experimentally verified the validity of our approximation using a controlled dataset, and we also showed the effectiveness of the cv3dconv by adopting it for deep neural networks (DNNs) in visual odometry estimation task using publicly available event-based camera dataset captured in urban road scene. Our DNN architecture improves the estimation accuracy for about 30% compared with the existing states-of-the-arts architecture designed for event data.",
        "authors": "Hideo Saito, Kohta Ishikawa, Yusuke Sekikawa",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883340",
        "group_name": "OpsA",
        "cluster": 2
    },
    {
        "article": "The robust and efficient detection of infrared small target is a key technique for infrared search and track systems. Several robust principal component analysis (RPCA)-based methods have been developed recently, which have achieved the state-of-the-art performance. However, there are still two drawbacks: 1) the false alarm ratio would raise under the heavy background clutters and noises and 2) these methods are usually time-consuming and not suitable for real-time processing. To solve this problem, an infrared patch-tensor (IPT) model based on weighted tensor nuclear norm is proposed in this paper. First, the infrared image is transformed into the IPT. Considering the sum of nuclear norms adopted in the IPT model is not the convex envelope of the tensor rank, and the solution is substantially suboptimal. The tensor nuclear norm is adopted to recover the underlying low-rank background tensor and sparse target tensor, and the computation complexity can be reduced dramatically with the help of the tensor Singular Value Decomposition. Moreover, to further suppress the background clutters, a weight tensor is incorporated with tensor nuclear norm to preserve the background edges better. Then, the separation between target and background is formulated as a convex weighted tensor RPCA model. Finally, the proposed model can be solved by the alternating direction method of multipliers. Extensive experiments demonstrate that the proposed model outperforms the other state-of-the-arts in terms of performance and efficiency.",
        "abstract": "The robust and efficient detection of infrared small target is a key technique for infrared search and track systems. Several robust principal component analysis (RPCA)-based methods have been developed recently, which have achieved the state-of-the-art performance. However, there are still two drawbacks: 1) the false alarm ratio would raise under the heavy background clutters and noises and 2) these methods are usually time-consuming and not suitable for real-time processing. To solve this problem, an infrared patch-tensor (IPT) model based on weighted tensor nuclear norm is proposed in this paper. First, the infrared image is transformed into the IPT. Considering the sum of nuclear norms adopted in the IPT model is not the convex envelope of the tensor rank, and the solution is substantially suboptimal. The tensor nuclear norm is adopted to recover the underlying low-rank background tensor and sparse target tensor, and the computation complexity can be reduced dramatically with the help of the tensor Singular Value Decomposition. Moreover, to further suppress the background clutters, a weight tensor is incorporated with tensor nuclear norm to preserve the background edges better. Then, the separation between target and background is formulated as a convex weighted tensor RPCA model. Finally, the proposed model can be solved by the alternating direction method of multipliers. Extensive experiments demonstrate that the proposed model outperforms the other state-of-the-arts in terms of performance and efficiency.",
        "authors": "Jungang Yang, Wei An, Yang Sun, Yunli Long, Zheran Shang",
        "journal_conference_name": "IEEE Access",
        "publisher": "IEEE",
        "year": "2018",
        "doi": "https://doi.org/10.1109/ACCESS.2018.2883727",
        "group_name": "OpsA",
        "cluster": 2
    }
]