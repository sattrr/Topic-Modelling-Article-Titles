{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(ChromeDriverManager().install())\n",
    "options = Options()\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "# options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-cache\") \n",
    "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_path = \"../../data/raw/article_links.json\"\n",
    "\n",
    "# Membaca file JSON\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Konversi JSON ke DataFrame\n",
    "df = pd.DataFrame(data[\"URL\"], columns=[\"URL\"])\n",
    "\n",
    "print(df.head())  # Cek apakah data terbaca dengan benar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# List untuk menyimpan hasil scraping\n",
    "all_titles = []\n",
    "all_years = []\n",
    "all_authors = []\n",
    "\n",
    "# Batasan jumlah halaman yang ingin di-scrape (None jika ingin sampai akhir)\n",
    "MAX_PAGES = 5  # Ubah sesuai kebutuhan, atau None untuk tanpa batas\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        base_url = row[\"URL\"]  # Pastikan kolom URL ada di dataset\n",
    "        print(f\"\\nüîç Scraping artikel dari: {base_url}\")\n",
    "\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Jika MAX_PAGES ditentukan dan batas tercapai, berhenti\n",
    "            if MAX_PAGES and page_number > MAX_PAGES:\n",
    "                print(f\"‚ö†Ô∏è Mencapai batas maksimum {MAX_PAGES} halaman. Lanjut ke URL berikutnya.\")\n",
    "                break\n",
    "\n",
    "            # Bangun URL untuk halaman tertentu\n",
    "            url = f\"{base_url}&sortType=vol-only-newest&pageNumber={page_number}\"\n",
    "            print(f\"üìÑ Membuka halaman {page_number}: {url}\")\n",
    "\n",
    "            driver.get(url)\n",
    "\n",
    "            # Tunggu elemen muncul untuk memastikan halaman telah dimuat\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"col\"))\n",
    "                )\n",
    "            except:\n",
    "                print(f\"‚ö†Ô∏è Halaman {page_number} gagal dimuat, lanjut ke URL berikutnya.\")\n",
    "                break\n",
    "\n",
    "            time.sleep(3)  # Beri jeda agar halaman termuat sempurna\n",
    "\n",
    "            # Parse halaman dengan BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            articles = soup.find_all(\"div\", class_=\"col result-item-align px-3\")\n",
    "\n",
    "            # Mengecek apakah artikel ditemukan\n",
    "            if not articles:\n",
    "                print(f\"‚ö†Ô∏è Halaman {page_number} kosong atau tidak ditemukan artikel baru. Berhenti.\")\n",
    "                break  # Jika halaman kosong, hentikan scraping URL ini\n",
    "\n",
    "            for article in articles:\n",
    "                # **Scraping Judul**\n",
    "                h2_tag = article.find(\"h2\")\n",
    "                title_tag = h2_tag.find(\"a\") if h2_tag else None\n",
    "                title = title_tag.text.strip() if title_tag else \"Judul Tidak Ditemukan\"\n",
    "                all_titles.append(title)\n",
    "\n",
    "                # **Scraping Tahun**\n",
    "                year_container = article.find(\"div\", class_=\"description text-base-md-lh\")\n",
    "                year_tag = year_container.find(\"span\") if year_container else None\n",
    "                year = year_tag.text.strip() if year_tag else \"Tahun Tidak Ditemukan\"\n",
    "                all_years.append(year)\n",
    "\n",
    "                # **Scraping Author**\n",
    "                author_spans = article.find_all(\"span\", attrs={\"_ngcontent-ng-c893371016\": True})\n",
    "                authors = \", \".join(set(span.text.strip() for span in author_spans))  # Hapus duplikasi\n",
    "                authors = authors.replace(\";\", \"\").strip(\", \")  # Bersihkan format\n",
    "                all_authors.append(authors)\n",
    "\n",
    "            print(f\"‚úÖ Berhasil scraping {len(articles)} artikel dari halaman {page_number}\")\n",
    "\n",
    "            page_number += 1  # Lanjut ke halaman berikutnya\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå Error: Kolom 'URL' tidak ditemukan. Pastikan nama kolom sesuai. Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Terjadi kesalahan saat scraping {url}: {e}\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List untuk menyimpan hasil scraping\n",
    "all_titles = []\n",
    "all_years = []\n",
    "all_authors = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        base_url = row[\"URL\"]  # Pastikan kolom URL ada di dataset\n",
    "        print(f\"\\nüîç Scraping artikel dari: {base_url}\")\n",
    "\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Bangun URL untuk halaman tertentu\n",
    "            url = f\"{base_url}&sortType=vol-only-newest&pageNumber={page_number}\"\n",
    "            print(f\"üìÑ Membuka halaman {page_number}: {url}\")\n",
    "\n",
    "            driver.get(url)\n",
    "\n",
    "            # Tunggu elemen muncul untuk memastikan halaman telah dimuat\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"col\"))\n",
    "                )\n",
    "            except:\n",
    "                print(f\"‚ö†Ô∏è Halaman {page_number} gagal dimuat, lanjut ke URL berikutnya.\")\n",
    "                break\n",
    "\n",
    "            time.sleep(3)  # Beri jeda agar halaman termuat sempurna\n",
    "\n",
    "            # Parse halaman dengan BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            articles = soup.find_all(\"div\", class_=\"col result-item-align px-3\")\n",
    "\n",
    "            # Mengecek apakah artikel ditemukan\n",
    "            if not articles or len(articles) == 0:\n",
    "                print(f\"‚ö†Ô∏è Halaman {page_number} kosong atau tidak ditemukan artikel baru. Coba cek secara manual.\")\n",
    "                continue  # Lanjutkan ke halaman berikutnya alih-alih berhenti langsung\n",
    "\n",
    "\n",
    "            for article in articles:\n",
    "                # **Scraping Judul**\n",
    "                h2_tag = article.find(\"h2\")\n",
    "                title_tag = h2_tag.find(\"a\") if h2_tag else None\n",
    "                title = title_tag.text.strip() if title_tag else \"Judul Tidak Ditemukan\"\n",
    "                all_titles.append(title)\n",
    "\n",
    "                # **Scraping Tahun**\n",
    "                year_container = article.find(\"div\", class_=\"description text-base-md-lh\")\n",
    "                year_tag = year_container.find(\"span\") if year_container else None\n",
    "                year = year_tag.text.strip() if year_tag else \"Tahun Tidak Ditemukan\"\n",
    "                all_years.append(year)\n",
    "\n",
    "                # **Scraping Author**\n",
    "                author_spans = article.find_all(\"span\", attrs={\"_ngcontent-ng-c893371016\": True})\n",
    "                authors = \", \".join(set(span.text.strip() for span in author_spans))  # Hapus duplikasi\n",
    "                authors = authors.replace(\";\", \"\").strip(\", \")  # Bersihkan format\n",
    "                all_authors.append(authors)\n",
    "\n",
    "            print(f\"‚úÖ Berhasil scraping {len(articles)} artikel dari halaman {page_number}\")\n",
    "\n",
    "            page_number += 1  # Lanjut ke halaman berikutnya\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå Error: Kolom 'URL' tidak ditemukan. Pastikan nama kolom sesuai. Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Terjadi kesalahan saat scraping {url}: {e}\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Membuat DataFrame dengan tambahan kolom author\n",
    "output_df = pd.DataFrame({\n",
    "    'Judul': all_titles,\n",
    "    'Tahun': all_years,\n",
    "    'Author': all_authors\n",
    "})\n",
    "\n",
    "# Menentukan path untuk menyimpan JSON\n",
    "output_json_path = \"../../data/raw/scraped_articles.json\"\n",
    "\n",
    "# Membuat direktori jika belum ada\n",
    "output_dir = Path(output_json_path).parent\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Menyimpan DataFrame dalam format JSON (dengan indentasi untuk keterbacaan)\n",
    "output_df.to_json(output_json_path, orient=\"records\", indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ Hasil scraping telah disimpan ke: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Membuat DataFrame dengan tambahan kolom author\n",
    "output_df = pd.DataFrame({\n",
    "    'Judul': all_titles,\n",
    "    'Tahun': all_years,\n",
    "    'Author': all_authors\n",
    "})\n",
    "\n",
    "# Menentukan path untuk menyimpan JSON\n",
    "output_json_path = \"../../data/raw/scraped_articles.json\"\n",
    "\n",
    "# Membuat direktori jika belum ada\n",
    "output_dir = Path(output_json_path).parent\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Menyimpan DataFrame dalam format JSON (dengan indentasi untuk keterbacaan)\n",
    "output_df.to_json(output_json_path, orient=\"records\", indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ Hasil scraping telah disimpan ke: {output_json_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
